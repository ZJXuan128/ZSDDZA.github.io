<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>X</title>
  
  <subtitle>x</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-04-11T16:06:46.019Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>X</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>初识反向传播</title>
    <link href="http://yoursite.com/2020/04/12/%E5%88%9D%E8%AF%86%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    <id>http://yoursite.com/2020/04/12/初识反向传播/</id>
    <published>2020-04-11T16:06:15.000Z</published>
    <updated>2020-04-11T16:06:46.019Z</updated>
    
    <content type="html"><![CDATA[<h3 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h3><p>在单层感知机模型中，对于输入与输出之间的权重调整依赖于预测产生的误差，由于不含隐藏层，误差可以直接计算得到。而对于多层网络来说，由于隐藏层的存在，输入输出之间的权重变得复杂，显然直接计算并不合理，而是需要通过从输出到输入反方向逐层计算。</p><p>由于是从输出到输入，所以我们一定需要先有一个正向传播的过程。使得样本从输入层开始，由上至下逐层经隐节点计算处理，最终样本信息被传送到输出层节点，得到预测的结果。再根据正向传播得到的结果也就是预测经行误差计算。</p><p>由于有了误差和输出，以及之前传播过来的网络，就有了反向传播的原材料，可以开始进行反向传播了。反向传播无法直接计算隐节点的预测误差，但却可以利用输出节点的预测误差来逐层估计隐节点的误差，也就是将输出节点的预测误差反方向传播到上层隐节点，逐层调整连接权重，直至输入节点和隐节点的权重全部得到调整，使得网络输出值越来越逼近实际值。</p><h3 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h3><p>每个神经元都由两个单元组成。第一个单元的功能是把每个输入信号和对应权重系数($w_{i}$)作乘积然后对这些乘积求和。第二个单元实现了一个非线性函数。这个函数也称为神经元激活函数(neuron activation function)。如下图所示，信号$e$是加法器（即第一个单元）的输出信号，而$y = f(e)$是非线性函数（即第二个单元）的输出信号。显然，$y$也同时是这整个神经元的输出信号。<br><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2dhbGF4eS5hZ2guZWR1LnBsL352bHNpL0FJL2JhY2twX3RfZW4vYmFja3Byb3BfZmlsZXMvaW1nMDFiLmdpZg" alt="在这里插入图片描述"><br>一般这个非线性激活函数采用sigmoid函数。节点的输出被限制在0~1的范围内。对于分类问题，输出节点给出的是预测类别的概率值；对于回归问题，输出节点给出的标准化处理后的预测值，只需还原处理即可。<br>使用sigmoid的原因：</p><ul><li>在模型开始训练阶段，由于连接权重的设置要求满足均值为0的均匀分布，所以初期的连接权重在0值附近，使得加法器结果也在0附近。此时sigmoid函数的斜率近似为一个常数，输入输出间呈近似线性关系，模型较为简单；</li><li>随着模型训练的进行，网络权重不断调整，节点加法器的结果逐渐偏离0值，输入输出逐渐呈现出非线性关系，模型逐渐复杂起来，并且输入的变化对输出的影响程度逐渐减小</li><li>到模型训练的后期，节点加法器结果远离0，此时输入的变化将不再引起输出的明显变动，输出基本趋于稳定。神经网络的预测误差不再随着连接权重的调整而得到明显改善，预测结果稳定，模型训练结束。</li></ul><p>可见，sigmoid激活函数较好的体现了连接权重修正过程中，模型从近似线性到非线性的渐进转变过程。</p><p>除此之外，sigmoid激活函数还具有无限次可微的特点，这使得反向传播能够采用梯度下降法来挑战连接权重。</p><h3 id="关键过程"><a href="#关键过程" class="headerlink" title="关键过程"></a>关键过程</h3><p>由于权重的不同，会导致前置神经元对后续紧邻的后置神经元传递的误差不同，或者说对后置神经元的误差贡献度不同，所以在进行反向传播的时候，不能将误差均分到前置 的神经元，而是应该根据权值对误差进行反向的传递。<br><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2dhbGF4eS5hZ2guZWR1LnBsL352bHNpL0FJL2JhY2twX3RfZW4vYmFja3Byb3BfZmlsZXMvaW1nMDkuZ2lm" alt="在这里插入图片描述"><br><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2dhbGF4eS5hZ2guZWR1LnBsL352bHNpL0FJL2JhY2twX3RfZW4vYmFja3Byb3BfZmlsZXMvaW1nMTAuZ2lm" alt="在这里插入图片描述"><br>如果前置的神经元对后置多个神经元都传递了误差，那反向传播的时候潜质的误差需要按照权重对后置的误差求和。<br><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2dhbGF4eS5hZ2guZWR1LnBsL352bHNpL0FJL2JhY2twX3RfZW4vYmFja3Byb3BfZmlsZXMvaW1nMTEuZ2lm" alt="在这里插入图片描述"><br>当每个神经元的误差信号都已经被计算出来之后，我们就可以开始修改每个神经元输入结点的权重系数了。在下面的公式中，$\frac{\mathrm{df_1(e)} }{\mathrm{d} e}$表示对应的神经元激活函数的导数。<br><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2dhbGF4eS5hZ2guZWR1LnBsL352bHNpL0FJL2JhY2twX3RfZW4vYmFja3Byb3BfZmlsZXMvaW1nMTQuZ2lm" alt="在这里插入图片描述"><br><img src="http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop_files/img17.gif" alt=""><br><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2dhbGF4eS5hZ2guZWR1LnBsL352bHNpL0FJL2JhY2twX3RfZW4vYmFja3Byb3BfZmlsZXMvaW1nMTkuZ2lm" alt="在这里插入图片描述"><br><em>计算是逐层的，这里只展示一条链上（$x_1—&gt;f_1{e}—&gt;f_4{e}—&gt;f_5{e}$）的计算，同层计算方式相同不多描述</em><br><em>具体的调整和计算方法涉及到<a href="https://zhuanlan.zhihu.com/p/36564434" target="_blank" rel="noopener">梯度下降算法</a></em><br>系数$\eta$会影响网络的训练速度。有几种技术可以用于选择系数$\eta$。第一种方法是以较大的参数值开始训练过程。在建立权重系数的同时，参数也逐渐减小。第二种方法更复杂，它以较小的参数值开始训练。在训练的过程中，参数增加，然后在最后阶段参数再次减小。以较小的参数值开始训练使得我们可以确定权重参数的符号。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;引入&quot;&gt;&lt;a href=&quot;#引入&quot; class=&quot;headerlink&quot; title=&quot;引入&quot;&gt;&lt;/a&gt;引入&lt;/h3&gt;&lt;p&gt;在单层感知机模型中，对于输入与输出之间的权重调整依赖于预测产生的误差，由于不含隐藏层，误差可以直接计算得到。而对于多层网络来说，由于隐藏层的
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>k-medoids聚类</title>
    <link href="http://yoursite.com/2020/04/05/k-medoids%E8%81%9A%E7%B1%BB/"/>
    <id>http://yoursite.com/2020/04/05/k-medoids聚类/</id>
    <published>2020-04-04T17:54:17.000Z</published>
    <updated>2020-04-04T17:54:49.679Z</updated>
    
    <content type="html"><![CDATA[<p>我们知道对于K-means算法来说，如果数据样本中出现极端的离群值，导致样本数据分布出现一定的扭曲或者说偏离，则会导致聚类效果不好，与期望的效果之间有误差存在，也就是我们所说的极端值敏感。聚类很多情况下都是通过计算点之间的欧式距离来表现亲缘远近的，所以解决在其上产生的问题就逃不过中心点。这次介绍的k-medoids算法自然在处理一些异常值方面具有得天独厚的优势。<br>就原理而言，k-means与k-medoids区别在于中心点的选择。前者通过不断产生K个新的中心点（初始为随机，后续计算均值进行）来划分群集，以达到准则函数收敛的结果，其本质就是在不断寻找符合条件的群集重心；后者，顾名思义，不选用平均值，而是选用最中心的点，即需要计算当前簇各个点到中心点的距离。这也使得我们得到一个其与k-means的不同点，或者说k-medoids的性质，它所选取的中心点都是簇中的点。<br>不过由于算法的复杂性（需要遍历簇中所有点），导致更大的计算资源倾斜。</p><hr><p>简单复习一下顺便对比一下两者算法过程：</p><p>k-means：</p><blockquote><p>1） 任意选择K个对象作为初始的簇中心；<br>2） 分别计算数据集中每个元素与所选簇的中心计算距离（一般采用欧式距离），根据最近邻原则，将元素划分到相应的簇中；<br>3） 计算每个簇中对象的平均值，更新簇的中心；<br>4） 重复上面的步骤，直至更新的簇的中心与原簇的中心的差值在预定范围内，或者达到预设的迭代次数；<br>5） 输出K个簇中心。</p></blockquote><p>k-medoids:</p><blockquote><p>1、任意选取 k 个点作为 medoids<br>2、按照与medoids最近的原则，将剩余点分配到当前最佳的medoids代表的类中<br>3、在每一类中，计算每个成员点对应的准则函数，选取准则函数最小时对应的点作为新的 medoids<br>4、重复2-3的过程，直到所有的 medoids 点不再发生变化，或已达到设定的最大迭代次数<br>其中准则函数为，一类中，某个成员点和其他成员点的距离之和</p></blockquote><hr><p>来一点python的实战<br>由于可使用的第三方库不多，参考了很多文章和博客，找到了可用的库——pyclust，它还依赖于treelib库。<br>具体代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyclust <span class="keyword">import</span> KMedoids</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="string">'''构造示例数据集（加入少量脏数据）'''</span></span><br><span class="line">data1 = np.random.normal(<span class="number">0</span>,<span class="number">0.9</span>,(<span class="number">1000</span>,<span class="number">10</span>))</span><br><span class="line">data2 = np.random.normal(<span class="number">1</span>,<span class="number">0.9</span>,(<span class="number">1000</span>,<span class="number">10</span>))</span><br><span class="line">data3 = np.random.normal(<span class="number">2</span>,<span class="number">0.9</span>,(<span class="number">1000</span>,<span class="number">10</span>))</span><br><span class="line">data4 = np.random.normal(<span class="number">3</span>,<span class="number">0.9</span>,(<span class="number">1000</span>,<span class="number">10</span>))</span><br><span class="line">data5 = np.random.normal(<span class="number">50</span>,<span class="number">0.9</span>,(<span class="number">50</span>,<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">data = np.concatenate((data1,data2,data3,data4,data5))</span><br><span class="line"></span><br><span class="line"><span class="string">'''准备可视化需要的降维数据'''</span></span><br><span class="line">data_TSNE = TSNE(learning_rate=<span class="number">100</span>).fit_transform(data)</span><br><span class="line"></span><br><span class="line"><span class="string">'''对不同的k进行试探性K-medoids聚类并可视化'''</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,<span class="number">6</span>):</span><br><span class="line">    k = KMedoids(n_clusters=i,distance=<span class="string">'euclidean'</span>,max_iter=<span class="number">1000</span>).fit_predict(data)</span><br><span class="line">    colors = ([[<span class="string">'red'</span>,<span class="string">'blue'</span>,<span class="string">'black'</span>,<span class="string">'yellow'</span>,<span class="string">'green'</span>][i] <span class="keyword">for</span> i <span class="keyword">in</span> k])</span><br><span class="line">    plt.subplot(<span class="number">219</span>+i)</span><br><span class="line">    plt.scatter(data_TSNE[:,<span class="number">0</span>],data_TSNE[:,<span class="number">1</span>],c=colors,s=<span class="number">10</span>)</span><br><span class="line">    plt.title(<span class="string">'K-medoids Resul of '</span>.format(str(i)))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>效果：<br><img src="https://img-blog.csdnimg.cn/20200405014833433.PNG" alt="在这里插入图片描述"></p><p>小结：k-medoids面对孤立的点时有更好的效果，但因此也占用了较多的计算资源，后续关于k-medoids有变种算法如：PAM算法 和 CLARA算法这里不作展开。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我们知道对于K-means算法来说，如果数据样本中出现极端的离群值，导致样本数据分布出现一定的扭曲或者说偏离，则会导致聚类效果不好，与期望的效果之间有误差存在，也就是我们所说的极端值敏感。聚类很多情况下都是通过计算点之间的欧式距离来表现亲缘远近的，所以解决在其上产生的问题就
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>梯度下降与脱离鞍点</title>
    <link href="http://yoursite.com/2020/03/29/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E8%84%B1%E7%A6%BB%E9%9E%8D%E7%82%B9/"/>
    <id>http://yoursite.com/2020/03/29/梯度下降与脱离鞍点/</id>
    <published>2020-03-28T16:48:13.000Z</published>
    <updated>2020-03-28T16:48:38.128Z</updated>
    
    <content type="html"><![CDATA[<h4 id="直奔主题"><a href="#直奔主题" class="headerlink" title="直奔主题"></a>直奔主题</h4><p>如果要优化(找到它的最小值)一个函数$f(x)$ , 通常能够用Gradient Descent (GD), 也就是梯度下降，也称最速下降，因为梯度的本意是一个向量（矢量），表示某一函数在该点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向（此梯度的方向）变化最快，变化率最大（为该梯度的模）。<br>听着很简单就是每次沿着当前位置的导数方向走一小步, 走啊走啊就能够走到一个最小值点。<br>如图：<br><img src="https://img-blog.csdnimg.cn/20200328214122985.jpg" alt="在这里插入图片描述"></p><p>对应的数学语言：$x_{t+1}=x_t+γ_t\nabla f(x_t)$<br>$x_t$为第t步的位置，$γ_t$为第t步的步长，$\nabla f(x_t)$为梯度求导得到，nabla算子也可以写作$grad()$，具体可以这样表示$\nabla=\frac{\partial }{\partial x}\pmb i+\frac{\partial }{\partial y}\pmb j$。</p><p>在机器学习的中使用, 会面临非常大的数据集。这个时候如果硬要算$f(x)$的精确导数, 往往意味着我们要花几个小时把整个数据集都扫描一遍, 然后还只能走一小步。一般GD要几千步几万步才能收敛, 所以这样就根本跑不完了。其次, 我们可能就会不小心陷入了鞍点, 或者比较差的局部最优点。<br>局部最优解与鞍点：<br><img src="https://img-blog.csdnimg.cn/20200329001122896.jpg" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329001151684.jpg" alt="在这里插入图片描述"><br>Stochastic Gradient Descent (SGD) 算法而可以将将两个问题一并解决，<br>数学语言描述的形式：$x_{t+1}=x_t+γ_tg_t$<br>$g_t$就是随机梯度，满足约束：$E(g_t)= \nabla f(x_t)$<br>就是说虽然包含一定的随机性，但是从期望上来看，它是等于正确的导数的。其实SGD需要更多步才能够收敛的。由于它对导数的要求非常低，可以包含大量的噪声（或者说扰动），只要期望正确就行，所以导数算起来非常快。拿机器学习的例子来说，比如神经网络吧，训练的时候都是每次只从百万数据点里面拿128或者256个数据点，算一个不那么准的导数，然后用SGD走一步的。想想看，这样每次算的时间就快了10000倍，就算是多走几倍的路，算算也是挺值的了。</p><p>所以它可以完美解决GD的第一个问题——算得慢．这也是当初人们使用SGD的主要目的。而且，大家并不用担心导数中包含的噪声会有什么负面影响。有大量的理论工作说明，只要噪声不离谱，其实（至少在f是凸函数的情况下），SGD都能够很好地收敛。<br>再来说一下脱离鞍点的问题，导数为零的点叫Stationary points，即驻点。<br>可以是（局部）最小值，（局部）最大值，也可以是鞍点。<br>我们可以通过计算它的Hessian矩阵H来判断。</p><ul><li>如果H是负定的，说明所有的特征值都是负的．这个时候，你无论往什么方向走，导数都会变负，也就是说函数值会下降．所以，这是（局部）最大值．</li><li>如果H是正定的，说明所有的特征值都是正的．这个时候，你无论往什么方向走，导数都会变正，也就是说函数值会上升．所以，这是（局部）最小值．</li><li>如果Ｈ既包含正的特征值，又包含负的特征值，那么这个稳定点就是一个鞍点．具体参照之前的图片．也就是说有些方向函数值会上升，有些方向函数值会下降．</li><li>虽然看起来上面已经包含了所有的情况，但是其实不是的！还有一个非常重要的情况就是H可能包含特征值为０的情况．这种情况下面，我们无法判断稳定点到底属于哪一类，往往需要参照更高维的导数才行．想想看，如果特征值是０，就说明有些方向一马平川一望无际，函数值一直不变，那我们当然不知道是怎么回事了：）<br>今天讨论的情况只包含前三种，不包含第四种．第四种被称为退化了的情况，所以我们考虑的情况就叫做非退化情况。</li></ul><p>在这种非退化的情况下面，我们考虑一个重要的类别，即strict saddle函数．这种函数有这样的特点：对于每个点x</p><ul><li>要么x的导数比较大 </li><li>要么x的Hessian矩阵包含一个负的特征值 </li><li>要么x已经离某一个（局部）最小值很近了</li></ul><p>为什么我们要x满足这三个情况的至少一个呢？因为如果x的导数大，那么沿着这个导数一定可以大大降低函数值（我们对函数有光滑性假设）<br>如果x的Hessian矩阵有一个负的特征值，那么我们通过加噪声随机扰动，跑跑就能够跑到这个方向上，沿着这个方向就能够像滑滑梯一样一路滑下去，大大降低函数值<br>如果x已经离某一个（局部）最小值很近了，那么我们就完成任务了，毕竟这个世界上没有十全十美的事情，离得近和精确跑到这个点也没什么区别．<br>所以说，如果我们考虑的函数满足这个strict saddle性质，那么SGD算法其实是不会被困在鞍点的。那么strict saddle性质是不是一个合理的性质呢？</p><p>实际上，有大量的机器学习的问题使用的函数都满足这样的性质。比如Orthogonal tensor decomposition，dictionary learning, matrix completion等等。而且，其实并不用担心最后得到的点只是一个局部最优，而不是全局最优。因为实际上人们发现大量的机器学习问题，几乎所有的局部最优是几乎一样好的，也就是说，只要找到一个局部最优点，其实就已经找到了全局最优，比如Orthogonal tensor decomposition就满足这样的性质，还有NIPS16的best student paper证明了matrix completion也满足这样的性质。</p><p>下面讨论一下证明，主要讨论一下第二篇．第一篇论文其实就是用数学的语言在说＂在鞍点加扰动，能够顺着负的特征值方向滑下去＂．第二篇非常有意思，我觉得值得介绍一下想法。</p><p>首先，算法上有了一些改动．算法不再是SGD，而是跑若干步GD，然后跑一步SGD。当然实际上大家是不会这么用的，但是理论分析么，这么考虑没问题。什么时候跑SGD呢？只有当导数比较小，而且已经很长时间没有跑过SGD的时候，才会跑一次。也就是说，只有确实陷在鞍点上了，才会随机扰动一下下。</p><p>因为鞍点有负的特征值，所以只要扰动之后在这个方向上有那么一点点分量，就能够一马平川地滑下去。除非分量非常非常小的情况下才可能会继续陷在鞍点附近。换句话说，如果加了一个随机扰动，其实大概率情况下是能够逃离鞍点的！</p><p>虽然这个想法也很直观，但是要严格地证明很不容易，因为具体函数可能是很复杂的，Hessian矩阵也在不断地变化，所以要说明＂扰动之后会陷在鞍点附近的概率是小概率＂这件事情并不容易。</p><p>作者们采取了一个很巧妙的方法：对于负特征值的那个方向，任何两个点在这两个方向上的投影的距离只要大于u/2, 那么它们中间至少有一个点能够通过多跑几步GD逃离鞍点。也就是说，会持续陷在鞍点附近的点所在的区间至多只有u那么宽！通过计算宽度，我们也就可以计算出概率的上届，说明大概率下这个SGD+GD算法能够逃离鞍点了。</p><hr><p>引自百度百科，相关知识点的简要，方便理解</p><blockquote><p>凸集：具体地说，在欧氏空间中，凸集是对于集合内的每一对点，连接该对点的直线段上的每个点也在该集合内。</p></blockquote><p>倘若出现中空或凹陷如此这般，为非凸集。</p><blockquote><p>凸函数：大陆数学界某些机构关于函数凹凸性定义和国外的定义是相反的。Convex Function在某些中国大陆的数学书中指凹函数。Concave Function指凸函数。举个例子，同济大学高等数学教材对函数的凹凸性定义与此时我们讲的相反，这里的凹凸性是指其上方图是凸集，而同济大学高等数学教材则是指其下方图是凸集，两者定义正好相反。</p><p>凸优化：研究定义于凸集中的凸函数最小化的问题。</p></blockquote><p>额外强调一点：在凸优化中局部最优值必定是全局最优值。</p><blockquote><p>Hessian Matrix：是一个多元函数的二阶偏导数构成的方阵，描述了函数的局部曲率。常用于牛顿法解决优化问题，利用它可判定多元函数的极值问题。在工程实际问题的优化设计中，所列的目标函数往往很复杂，为了使问题简化，常常将目标函数在某点邻域展开成泰勒多项式来逼近原函数，此时函数在某点泰勒展开式的矩阵形式中会涉及到。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;直奔主题&quot;&gt;&lt;a href=&quot;#直奔主题&quot; class=&quot;headerlink&quot; title=&quot;直奔主题&quot;&gt;&lt;/a&gt;直奔主题&lt;/h4&gt;&lt;p&gt;如果要优化(找到它的最小值)一个函数$f(x)$ , 通常能够用Gradient Descent (GD), 也就是梯度下降
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tensorflow1.0 多元线性回归</title>
    <link href="http://yoursite.com/2020/03/22/tensorflow1-0-%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2020/03/22/tensorflow1-0-多元线性回归/</id>
    <published>2020-03-21T16:04:12.000Z</published>
    <updated>2020-03-21T16:07:03.273Z</updated>
    
    <content type="html"><![CDATA[<p>上一篇说到了单变量的线性回归，这次接着来说多元线性回归的问题。定义如下不必多说：</p><blockquote><p>在回归分析中，如果有两个或两个以上的自变量，就称为多元回归。事实上，一种现象常常是与多个因素相联系的，由多个自变量的最优组合共同来预测或估计因变量，比只用一个自变量进行预测或估计更有效，更符合实际。因此多元线性回归比一元线性回归的实用意义更大。<br>多元线性回归与一元线性回归类似，可以用最小二乘法估计模型参数，也需对模型及模型参数进行统计检验。</p></blockquote><p>社会经济现象的变化往往受到多个因素的影响，正好可以从UCI上获取到波士顿房价的数据集，于1978年开始统计，包括506个样本，每个样本包括12个特征变量和该地区的平均房价。虽然是有些年代的数据，但不影响我们的学习使用。<br>房价（单价）显然和多个特征变量相关，不是单变量线性回归（一元线性回归）问题，选择多个特征变量来建立线性方程，这就是多变量线性回归（多元线性回归）问题。<br>数据节选如下：<br><img src="https://img-blog.csdnimg.cn/20200321231046193.PNG" alt="在这里插入图片描述"><br>各列特征数据的含义，分别为：</p><ul><li>CRIM: 城镇人均犯罪率                                                        </li><li>AGE: 1940年之前建成的自用房屋比例</li><li>ZN：住宅用地超过 25000 sq.ft. 的比例</li><li>DIS：到波士顿5个中心区域的加权距离</li><li>INDUS: 城镇非零售商用土地的比例                               </li><li>RAD: 辐射性公路的靠近指数</li><li>CHAS: 边界是河流为1，否则0                                    </li><li>TAX: 每10000美元的全值财产税率</li><li>NOX: 一氧化氮浓度                                     </li><li>PTRATIO: 城镇师生比例</li><li>RM: 住宅平均房间数</li><li>LSTAT: 人口中地位低下者的比例</li><li>MEDV: 自住房的平均房价，单位：千美元</li></ul><p>对于模型的理解需要一定的矩阵代数知识，在此不多赘述。</p><p>下面来进行基于tensorflow1.0的实战：</p><p>首先导入相关的包以及使用pandas读取csv数据。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib notebook</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据文件</span></span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">"data/boston.csv"</span>, header=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#显示数据摘要描述信息</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (df.describe())</span><br></pre></td></tr></table></figure></p><p>载入数据，归一化<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取df的值</span></span><br><span class="line">df = df.values</span><br><span class="line">print(df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把 df 转换为 np 的数组格式</span></span><br><span class="line">df = np.array(df)</span><br><span class="line">print(df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对特征数据 【0到11】列 做（0-1）归一化</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">12</span>):</span><br><span class="line">    df[:,i]=df[:,i]/(df[:,i].max()-df[:,i].min())</span><br><span class="line">    </span><br><span class="line"><span class="comment"># x_data 为 归一化后的前12列特征数据</span></span><br><span class="line">x_data = df[:,:<span class="number">12</span>] </span><br><span class="line"></span><br><span class="line"><span class="comment"># y_data 为最后1列标签数据</span></span><br><span class="line">y_data = df[:,<span class="number">12</span>]</span><br></pre></td></tr></table></figure></p><p>进行数据归一化后可以加快梯度下降求解最优解的速度，也可以提高精度。</p><p>定义训练数据占位符，定义模型结构</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>,<span class="number">12</span>], name = <span class="string">"X"</span>) <span class="comment"># 12个特征数据（12列）</span></span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="keyword">None</span>,<span class="number">1</span>], name = <span class="string">"Y"</span>) <span class="comment"># 1个标签数据（1列）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义了一个命名空间</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"Model"</span>):</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># w 初始化值为shape=(12,1)的随机数</span></span><br><span class="line">    w = tf.Variable(tf.random_normal([<span class="number">12</span>,<span class="number">1</span>], stddev=<span class="number">0.01</span>), name=<span class="string">"W"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># b 初始化值为 1.0</span></span><br><span class="line">    b = tf.Variable(<span class="number">1.0</span>, name=<span class="string">"b"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># w和x是矩阵相乘，用matmul,不能用mutiply或者*</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(x, w, b)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tf.matmul(x, w) + b</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测计算操作，前向计算节点</span></span><br><span class="line">    pred= model(x, w, b)</span><br></pre></td></tr></table></figure><p>TensorFlow中的命名空间（name_scope），Tensorflow中常有数以千计节点，在可视化过程中很难一下子全部展示出来，因此可用name_scope为变量划分范围，在可视化中，这表示在计算图中的一个层级。</p><p>设置训练超参数，定义均方差损失函数，创建优化器，初始化变量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 迭代轮次</span></span><br><span class="line">train_epochs = <span class="number">50</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 学习率</span></span><br><span class="line">learning_rate = <span class="number">0.01</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"LossFunction"</span>):</span><br><span class="line">    loss_function = tf.reduce_mean(tf.pow(y-pred, <span class="number">2</span>)) <span class="comment">#均方误差</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建优化器</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_function)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="comment"># 定义初始化变量的操作</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">sess.run(init)</span><br></pre></td></tr></table></figure><p>训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range (train_epochs):</span><br><span class="line">    loss_sum = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> xs, ys <span class="keyword">in</span> zip(x_data, y_data):   </span><br><span class="line"></span><br><span class="line">        xs = xs.reshape(<span class="number">1</span>,<span class="number">12</span>)</span><br><span class="line">        ys = ys.reshape(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        _, summary_str, loss = sess.run([optimizer,sum_loss_op,loss_function], feed_dict=&#123;x: xs, y: ys&#125;) </span><br><span class="line"></span><br><span class="line">        writer.add_summary(summary_str, epoch)</span><br><span class="line">        loss_sum = loss_sum + loss</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 打乱数据顺序，避免过拟合</span></span><br><span class="line">    xvalues, yvalues = shuffle(x_data, y_data)</span><br><span class="line">    </span><br><span class="line">    b0temp=b.eval(session=sess)</span><br><span class="line">    w0temp=w.eval(session=sess)</span><br><span class="line">    loss_average = loss_sum/len(y_data)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"epoch="</span>, epoch+<span class="number">1</span>,<span class="string">"loss="</span>, loss_average,<span class="string">"b="</span>, b0temp,<span class="string">"w="</span>, w0temp )</span><br></pre></td></tr></table></figure><p>预测：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">n=<span class="number">348</span>                     <span class="comment">#指定一条数据来看效果</span></span><br><span class="line"></span><br><span class="line">x_test = x_data[n]</span><br><span class="line"></span><br><span class="line">x_test = x_test.reshape(<span class="number">1</span>,<span class="number">12</span>)</span><br><span class="line">predict = sess.run(pred, feed_dict=&#123;x: x_test&#125;)</span><br><span class="line">print(<span class="string">"预测值：%f"</span> % predict)</span><br><span class="line"></span><br><span class="line">target = y_data[n]</span><br><span class="line">print(<span class="string">"标签值：%f"</span> % target)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;上一篇说到了单变量的线性回归，这次接着来说多元线性回归的问题。定义如下不必多说：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在回归分析中，如果有两个或两个以上的自变量，就称为多元回归。事实上，一种现象常常是与多个因素相联系的，由多个自变量的最优组合共同来预测或估计因变量，比只
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tensorflow1.0 线性回归实战</title>
    <link href="http://yoursite.com/2020/03/14/tensorflow1-0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2020/03/14/tensorflow1-0-线性回归实战/</id>
    <published>2020-03-14T15:49:56.000Z</published>
    <updated>2020-03-14T15:50:22.011Z</updated>
    
    <content type="html"><![CDATA[<p><em>一个简单的线性回归案例，是一簇点，很容易想到用一条直线去拟合它，所以我们也会希望机器能用y=w</em>x+b这条直线去对其进行拟合，也可以说是去让机器学习w和b的值。</p><h2 id="损失"><a href="#损失" class="headerlink" title="损失"></a>损失</h2><p>损失是对糟糕预测的惩罚，损失是一个数值，表示对于单个样本而言模型预测的准确程度。<br>如果模型的预测完全准确，则损失为零，否则损失会较大。<br>训练模型的目标是从所有样本中找到一组平均损失“较小”的权重（w）和偏差（b）。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>损失函数也称为代价函数是将随机事件或其有关随机变量的取值映射为非负实数以表示该随机事件的“风险”或“损失”的函数。<br>下面介绍三个比较常见的损失函数：</p><blockquote><p>L1损失<br>L1范数损失函数，也被称为最小绝对值偏差（LAD），最小绝对值误差（LAE）。总的说来，它是把目标值（Yi）与估计值（f(xi)）的绝对差值的总和（S）最小化：$S=\sum_{i=1}^n|Y_i-f(x_i)|$</p><p>L2范数损失函数，也被称为最小平方误差（LSE）。总的来说，它是把目标值（Yi）与估计值（f(xi)）的差值的平方和（S）最小化：<br>$S=\sum_{i=1}^n(Y_i-f(x_i))^2$</p><p>均方误差 (MSE)<br>均方误差 (MSE) 指的是每个样本的平均平方损失<br>$MSE=\frac{1}{N}\sum_{(x,y)\in D}(y-prediction(x))^2$</p></blockquote><h2 id="模型训练与降低损失"><a href="#模型训练与降低损失" class="headerlink" title="模型训练与降低损失"></a>模型训练与降低损失</h2><p>迭代:首先我们先对模型的中的权重w和偏差b进行猜测，然后将特征点输入，执行预测和推理（Inference），将计算出的值和该样本的标签值进行对比，计算出损失值，我们的目标是使推理的值和标签值的差距越小越好，也就是损失的值越小越好，所以需要不断对计算参数进行更新，直到损失值尽可能地最低为止。</p><p>收敛:在学习优化过程中，机器学习系统将根据所有标签去重新评估所有特征，为损失函数生成一个新值，而该值又产生新的参数值。通常，可以不断迭代，直到总体损失不再变化或至少变化极其缓慢为止。这时候，我们可以说该模型已收敛。<br><em>易知该线性回归问题产生的损失与权重的关系图为如下图</em><br><img src="https://img-blog.csdnimg.cn/20200314233427520.png" alt=""></p><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>梯度下降法的基本思想可以类比为一个下山的过程，假设一个人在山上，此时他要以最快的速度下山，就需要梯度下降来帮助自己下山。具体来说，就是以自己现在所处的位置为基准，寻找一个山势最陡峭的方向，沿着高度下降的方向走，就能以最快速度到山底。</p><p>同理，将上一节所提到的损失函数看作一座山，我们的目标就是找到这个损失函数的最小值（山底），那么我们就可以在初始点找到该点函数的梯度，沿着函数值下降的方向对参数进行更新，这就是梯度下降法。</p><h2 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h2><p>那么沿着负梯度方向进行下一步探索，前进多少才合适呢？这时我们就要引入学习率的概念了。用梯度乘以一个称为学习率（有时也称为步长）的标量，以确定下一个点的位置。例如：如果梯度大小为 2.5，学习速率为 0.01，则梯度下降法算法会选择距离前一个点 0.025 的位置作为下一个点。</p><p>所以学习率是指导我们该如何通过损失函数的梯度调整网络权重的一个参数（也成为超参数）。学习率越低，损失函数的变化速度就越慢。</p><h2 id="Tensorflow实战"><a href="#Tensorflow实战" class="headerlink" title="Tensorflow实战"></a>Tensorflow实战</h2><p>本例通过生成人工数据集。随机生成一个近似采样随机分布，使得w=2.0, b=1, 并加入一个噪声，噪声的最大振幅为0.4。</p><p>下面我们来展示具体代码，假设我们要学习的函数为线性函数 𝑦=2𝑥+1。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在Jupyter中，使用matplotlib显示图像需要设置为 inline 模式，否则不会现实图像</span></span><br><span class="line">%matplotlib inline</span><br><span class="line">​</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># 载入matplotlib</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># 载入numpy</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf <span class="comment"># 载入Tensorflow</span></span><br><span class="line">​</span><br><span class="line">np.random.seed(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p>我们需要构造满足这个函数的 𝑥 和 𝑦 同时加入一些不满足方程的噪声。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#直接采用np生成等差数列的方法，生成100个点，每个点的取值在-1~1之间</span></span><br><span class="line">x_data = np.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">100</span>) </span><br><span class="line"><span class="comment"># y = 2x +1 + 噪声， 其中，噪声的维度与x_data一致</span></span><br><span class="line">y_data = <span class="number">2</span> * x_data + <span class="number">1.0</span> + np.random.randn(*x_data.shape) * <span class="number">0.4</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#画出随机生成数据的散点图</span></span><br><span class="line">plt.scatter(x_data, y_data) </span><br><span class="line"><span class="comment"># 画出我们想要学习到的线性函数 y = 2x +1</span></span><br><span class="line">plt.plot (x_data, <span class="number">2</span> * x_data + <span class="number">1.0</span>, color = <span class="string">'red'</span>,linewidth=<span class="number">3</span>)</span><br></pre></td></tr></table></figure><p>上面的代码产生了随机的-1到1的100个点，我们使用matplotlib库将这些点和要学习得到的线性函数可视化出来。</p><p>首先我们定义训练数据的占位符，这是后面数据输入的入口，x是特征值，y是标签值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义训练数据的占位符，x是特征值，y是标签值</span></span><br><span class="line">​</span><br><span class="line">x = tf.placeholder(<span class="string">"float"</span>, name = <span class="string">"x"</span>) </span><br><span class="line">y = tf.placeholder(<span class="string">"float"</span>, name = <span class="string">"y"</span>)</span><br></pre></td></tr></table></figure><p>然后我们定义模型函数，在本例中是个简单的线性函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(x, w, b)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.multiply(x, w) + b</span><br></pre></td></tr></table></figure><p>接下来我们创建模型的变量，Tensorflow变量的声明函数是tf.Variable，tf.Variable的作用是保存和更新参数，变量的初始值可以是随机数、常数或是通过其他变量的初始值计算得到。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建线性函数的斜率，变量w</span></span><br><span class="line">w = tf.Variable(<span class="number">1.0</span>, name=<span class="string">"w0"</span>)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 构建线性函数的截距，变量b</span></span><br><span class="line">b = tf.Variable(<span class="number">0.0</span>, name=<span class="string">"b0"</span>)   </span><br><span class="line">​</span><br><span class="line"><span class="comment"># pred是预测值，前向计算</span></span><br><span class="line">pred = model(x, w, b)</span><br></pre></td></tr></table></figure><p>定义一些超参数，包括训练的轮数和学习率。其中如果学习率设置过大，可能导致参数在极值附近来回摇摆，无法保证收敛。如果学习率设置过小，虽然能保证收敛，但优化速度会大大降低，我们需要更多迭代次数才能达到较理想的优化效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 迭代次数（训练轮数）</span></span><br><span class="line">train_epochs = <span class="number">10</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># 学习率</span></span><br><span class="line">learning_rate = <span class="number">0.05</span> </span><br><span class="line">​</span><br><span class="line"><span class="comment"># 控制显示loss值的粒度</span></span><br><span class="line">display_step = <span class="number">10</span></span><br></pre></td></tr></table></figure><p>定义损失函数和优化器。损失函数用于描述预测值与真实值之间的误差，从而指导模型收敛方向。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 采用均方差作为损失函数</span></span><br><span class="line">loss_function = tf.reduce_mean(tf.square(y-pred))  </span><br><span class="line">​</span><br><span class="line"><span class="comment"># 梯度下降优化器</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_function)</span><br></pre></td></tr></table></figure><p>声明会话及初始化。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">​</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">​</span><br><span class="line">sess.run(init)</span><br></pre></td></tr></table></figure></p><p>模型训练阶段，设置迭代轮次，每次通过将样本逐个输入模型，进行梯度下降优化操作，每轮迭代后，绘制出模型曲线。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开始训练，轮数为 epoch，采用SGD随机梯度下降优化方法</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(train_epochs):</span><br><span class="line">    <span class="keyword">for</span> xs,ys <span class="keyword">in</span> zip(x_data, y_data):</span><br><span class="line">        _, loss=sess.run([optimizer,loss_function], feed_dict=&#123;x: xs, y: ys&#125;) </span><br><span class="line">                </span><br><span class="line">    b0temp=b.eval(session=sess)</span><br><span class="line">    w0temp=w.eval(session=sess)</span><br><span class="line">    plt.plot (x_data, w0temp * x_data + b0temp  )<span class="comment"># 画图</span></span><br></pre></td></tr></table></figure><p>从上图可以看出，本案例所拟合的模型较简单，训练3次之后已经接近收敛 对于复杂模型，需要更多次训练才能收敛。</p><p>预测：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x_test = <span class="number">3.21</span></span><br><span class="line">​</span><br><span class="line">predict = sess.run(pred, feed_dict=&#123;x: x_test&#125;)</span><br><span class="line">print(<span class="string">"预测值：%f"</span> % predict)</span><br><span class="line">​</span><br><span class="line">target = <span class="number">2</span> * x_test + <span class="number">1.0</span></span><br><span class="line">print(<span class="string">"目标值：%f"</span> % target)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;em&gt;一个简单的线性回归案例，是一簇点，很容易想到用一条直线去拟合它，所以我们也会希望机器能用y=w&lt;/em&gt;x+b这条直线去对其进行拟合，也可以说是去让机器学习w和b的值。&lt;/p&gt;
&lt;h2 id=&quot;损失&quot;&gt;&lt;a href=&quot;#损失&quot; class=&quot;headerlink&quot;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tensorflow1.0中conv2的细节</title>
    <link href="http://yoursite.com/2020/03/08/tensorflow1-0%E4%B8%ADconv2%E7%9A%84%E7%BB%86%E8%8A%82/"/>
    <id>http://yoursite.com/2020/03/08/tensorflow1-0中conv2的细节/</id>
    <published>2020-03-07T17:46:44.000Z</published>
    <updated>2020-03-07T17:47:25.365Z</updated>
    
    <content type="html"><![CDATA[<p>对比2.0可以知道tensorflow1.0对于网络的搭建更复杂一些，因此细节上的容易出现差错，在此总结一下使用conv2d的一些小问题。</p><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.conv2d（input, filter, strides, padding, use_cudnn_on_gpu=<span class="keyword">None</span>,</span><br><span class="line">                data_format=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure><p>先来看一下各个参数的意义。</p><ul><li><strong>input</strong>：需要做卷积的输入数据。注意：这是一个4维的张量（[batch, in_height, in_width,in_channels]）。对于图像数据来说，batch是这一批样本的个数，in_height和in_width是图像的尺寸，in_channels是图像的通道数，而且要求图像的类型为float32或float64。因此，我们在对图像进行处理的时候，首先要把图像转换成这种特定的类型。</li><li><strong>filter</strong>：卷积核。这也是一个4维的张量（[filter_height, filter_width, in_channels,out_channels]）。filter_height,和filter_width是图像的尺寸，in_channels,是输入的通道数，out_channels是输出的通道数。</li><li><strong>strides</strong>：图像每一维的步长。是一个一维向量，长度为4。</li><li><strong>padding</strong>：定义元素边框与元素内容之间的空间。这里只能选择”SAME”或”VALID”，这个值决定了不同的卷积方式。当它为”SAME”时，表示边缘填充，适用于全尺寸操作；当它为”VALID”时，表示边缘不填充。</li><li><strong>use_cudnn_on_gpu</strong>：bool类型，是否使用cudnn加速。</li><li><strong>name</strong>：该操作的名称。</li><li><p>返回值：返回一个张量（tensor），即特征图（feature map）。</p><p>需要额外注意一点就是dataformat，关系到网络的output的排列方式，以及下一层的对接工作能否正确完成,它有两个选项，<strong>NHWC</strong>以及<strong>NCHW</strong>，前者为默认值。设置为 “NHWC” 时，排列顺序为 [batch, height, width, channels]。N是说这批图片有几张，H和W描述图像size，C是通道数（黑白图C=1，RBG图C=3）。以RGB为例，直观来说如下：<br><img src="https://img-blog.csdnimg.cn/20200308010449539.png" alt="在这里插入图片描述"><br>这里以灰度计算为例，说明各自的优劣。<br>对NCHW进行计算的时候，对将分成三个独立通道分别计算，，即全红一组，全绿一组这样。而NHWC得排列方式，是单个的三个相邻通道为一组计算。两者计算成本相同。我们可以知道，这样的话NHWC的局部访问存储性能更好（每三个输入像素即可得到一个输出像素）。NCHW 则必须等所有通道输入准备好才能得到最终输出结果，需要占用较大的临时空间。简单来说，就是想得到某个或某些独立像素像素的灰度计算结果，NCHW需要将全部图片计算出来，再取出特定的计算结果，而NHWC可以直接一个像素一个像素的得到结果。</p><p>在 CNN 中常常见到 1x1 卷积（例如：用于移动和嵌入式视觉应用的 MobileNets），也是每个输入 channel 乘一个权值，然后将所有 channel 结果累加得到一个输出 channel。如果使用 NHWC 数据格式，可以将卷积计算简化为矩阵乘计算，即 1x1 卷积核实现了每个输入像素组到每个输出像素组的线性变换。</p></li></ul><p>TensorFlow 为什么选择 NHWC 格式作为默认格式？因为早期开发都是基于 CPU，使用 NHWC 比 NCHW 稍快一些（不难理解，NHWC 局部性更好，cache 利用率高）。而NCHW 则是 Nvidia cuDNN 默认格式，使用 GPU 加速时用 NCHW 格式速度会更快（个别情况例外）。<br>所以设计网络的时候，需要根据具体的实践环境进行切换。</p><p><em>再回过头来看看conv2d的使用例子</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># 实践基于1.0</span></span><br><span class="line"> <span class="comment"># 2.0想实现请使用这段替换第一行：</span></span><br><span class="line"> <span class="comment"># import tensorflow.compat.v1 as tf</span></span><br><span class="line"> <span class="comment"># tf.disable_eager_execution()</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">input_data = tf.Varible(np.random.rand(<span class="number">10</span>,<span class="number">9</span>,<span class="number">9</span>,<span class="number">4</span>),dtype=np.float32)</span><br><span class="line">filter_data = tf.Varible(np.random.rand(<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">2</span>),dtype=np.float32)</span><br><span class="line">y = tf.nn.conv2d(input_data,filter_data,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">'VALID'</span>)</span><br><span class="line"></span><br><span class="line">print(input_data)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><blockquote><p>Tensor(“Variable/read:0”,shape=(10,9,9,4),dtype=float32)<br>Tensor(“Conv2D:0”,shape=(10,7,7,2),dtype=float32)</p></blockquote><p>导入所需要的库，然后我们定义需要做卷积的输入以及卷积核，这里的步长为1，padding为”VALID”。<br>我们可以看到，原本输入的shape是（10，9，9，4），由于padding为”VALID”,不对图像的边缘进行填充，所以在进行卷积之后，图像的尺寸发生了改变。<br>如果将padding改为”SAME”，图像的尺寸不变。</p><hr><p>2.0中keras的封装十分完善，对于使用来说比较友好，但是我们也更应该关注被封装一场隐藏起来的环节究竟有哪些细节在发生，在学习1.0的过程中我们会有不少收获。顺便<a href="https://minghuiwu.gitbook.io/tfbook/" target="_blank" rel="noopener">这里有一本1.0的开源书籍，对初学者十分友好，大家可以去看看</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;对比2.0可以知道tensorflow1.0对于网络的搭建更复杂一些，因此细节上的容易出现差错，在此总结一下使用conv2d的一些小问题。&lt;/p&gt;
&lt;hr&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gut
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>keras.layers--核心网络层摘要</title>
    <link href="http://yoursite.com/2020/02/29/keras-layers-%E6%A0%B8%E5%BF%83%E7%BD%91%E7%BB%9C%E5%B1%82%E6%91%98%E8%A6%81/"/>
    <id>http://yoursite.com/2020/02/29/keras-layers-核心网络层摘要/</id>
    <published>2020-02-29T15:20:58.000Z</published>
    <updated>2020-02-29T15:21:35.958Z</updated>
    
    <content type="html"><![CDATA[<p>完成一定实践后仔细阅读keras文档，做了关于核心网络层的一些摘要，主要汇总一些常用的网络层及其使用指南，大都在实践中使用过。</p><hr><p><strong>Dense</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Dense(units, activation=<span class="keyword">None</span>, use_bias=<span class="keyword">True</span>,</span><br><span class="line">                 kernel_initializer=<span class="string">'glorot_uniform'</span>, </span><br><span class="line">                 bias_initializer=<span class="string">'zeros'</span>, kernel_regularizer=<span class="keyword">None</span>, </span><br><span class="line">                 bias_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>, </span><br><span class="line">                 kernel_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure><ul><li>简要</li></ul><blockquote><p>全连接层。</p><p><em>output = activation(dot(input, kernel) + bias)</em>  其中 <em>activation</em> 是按逐个元素计算的激活函数，<em>kernel</em> 是由网络层创建的权值矩阵，以及 <em>bias</em> 是其创建的偏置向量 (只在 <em>use_bias</em><br>为 <em>True</em> 时才有用)。</p><p>如果该层的输入的秩大于2，那么它首先被展平然后 再计算与 kernel 的点乘。</p></blockquote><ul><li>重要参数</li></ul><blockquote><p>units: 正整数，输出空间维度。<br>activation: 激活函数。若不指定，则不使用激活函数 (即「线性」激活: a(x)=x。<br>use_bias: 布尔值，该层是否使用偏置向量。</p></blockquote><ul><li>输入输出</li></ul><blockquote><p>输入尺寸：<br>nD 张量，尺寸: (batch_size, …, input_dim)。 最常见的情况是一个尺寸为 (batch_size, input_dim) 的 2D 输入。<br>输出尺寸：<br>nD 张量，尺寸: (batch_size, …, units)。 例如，对于尺寸为 (batch_size, input_dim) 的 2D 输入， 输出的尺寸为 (batch_size, units)。</p></blockquote><hr><p><strong>Activation</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Activation(activation)</span><br></pre></td></tr></table></figure><ul><li><p>摘要</p><blockquote><p>激活函数可单独作为一层，也可以嵌入在Dense层中，发挥作用。</p></blockquote></li><li><p>输入输出</p><blockquote><p>输入尺寸：<br>任意尺寸。 当使用此层作为模型中的第一层时， 使用参数 input_shape （整数元组，不包括样本数的轴）。<br>输出尺寸：<br>与输入相同。</p></blockquote></li></ul><hr><p><strong>Dropout</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Dropout(rate, noise_shape=<span class="keyword">None</span>, seed=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure><ul><li>摘要<blockquote><p>Dropout 包括在训练中每次更新时， 将输入单元的按比率随机设置为 0， 这有助于防止过拟合。随机舍去一定的神经元，来降低过拟合程度。</p></blockquote></li><li>重要参数<blockquote><p>rate: 在 0 和 1 之间浮动。需要丢弃的输入比例。<br>noise_shape: 1D 整数张量， 表示将与输入相乘的二进制 dropout 掩层的形状。 例如，如果你的输入尺寸为 (batch_size, timesteps, features)，然后 你希望 dropout 掩层在所有时间步都是一样的， 你可以使用 noise_shape=(batch_size, 1, features)。<br>seed: 一个作为随机种子的 Python 整数。</p></blockquote></li></ul><hr><p><strong>Flatten</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Flatten(data_format=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure><ul><li>摘要<blockquote><p>将输入展平。不影响批量大小。</p></blockquote></li><li>重要参数<blockquote><p>data_format：一个字符串，其值为 channels_last（默认值）或者 channels_first。它表明输入的维度的顺序。此参数的目的是当模型从一种数据格式切换到另一种数据格式时保留权重顺序。channels_last 对应着尺寸为 (batch, …, channels) 的输入，而 channels_first 对应着尺寸为 (batch, channels, …) 的输入。默认为 image_data_format 的值，你可以在 Keras 的配置文件 ~/.keras/keras.json 中找到它。如果你从未设置过它，那么它将是 channels_last</p></blockquote></li></ul><hr><p><strong>Lambda</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Lambda(function, output_shape=<span class="keyword">None</span>, mask=<span class="keyword">None</span>, arguments=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure></p><ul><li>摘要<blockquote><p>将任意表达式封装为 Layer 对象。</p></blockquote></li><li>重要参数<blockquote><p>function: 需要封装的函数。 将输入张量作为第一个参数。<br>output_shape: 预期的函数输出尺寸。 只在使用 Theano 时有意义。 可以是元组或者函数。 如果是元组，它只指定第一个维度； 样本维度假设与输入相同： output_shape = (input_shape[0], ) + output_shape 或者，输入是 None 且样本维度也是 None： output_shape = (None, ) + output_shape 如果是函数，它指定整个尺寸为输入尺寸的一个函数： output_shape = f(input_shape)<br>arguments: 可选的需要传递给函数的关键字参数。</p></blockquote></li><li>输入输出<blockquote><p>输入尺寸：<br>任意。当使用此层作为模型中的第一层时， 使用参数 input_shape （整数元组，不包括样本数的轴）。<br>输出尺寸：<br>由 output_shape 参数指定 (或者在使用 TensorFlow 时，自动推理得到)。</p></blockquote></li></ul><hr><p><strong>Conv2D</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Conv2D(filters, kernel_size, strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">                    padding=<span class="string">'valid'</span>, data_format=<span class="keyword">None</span>,</span><br><span class="line">                    dilation_rate=(<span class="number">1</span>, <span class="number">1</span>), activation=<span class="keyword">None</span>,</span><br><span class="line">                    use_bias=<span class="keyword">True</span>, kernel_initializer=<span class="string">'glorot_uniform'</span>,</span><br><span class="line">                    bias_initializer=<span class="string">'zeros'</span>, kernel_regularizer=<span class="keyword">None</span>,</span><br><span class="line">                    bias_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>,</span><br><span class="line">                    kernel_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure><ul><li>摘要<blockquote><p>2D 卷积层 (例如对图像的空间卷积)。<br>该层创建了一个卷积核， 该卷积核对层输入进行卷积， 以生成输出张量。 如果 use_bias 为 True， 则会创建一个偏置向量并将其添加到输出中。 最后，如果 activation 不是 None，它也会应用于输出。<br>当使用该层作为模型第一层时，需要提供 input_shape 参数 （整数元组，不包含样本表示的轴），例如， input_shape=(128, 128, 3) 表示 128x128 RGB 图像， 在 data_format=”channels_last” 时。</p></blockquote></li><li><p>重要参数</p><blockquote><p>filters: 整数，输出空间的维度 （即卷积中滤波器的输出数量）。<br>kernel_size: 一个整数，或者 2 个整数表示的元组或列表， 指明 2D 卷积窗口的宽度和高度。 可以是一个整数，为所有空间维度指定相同的值。<br>strides: 一个整数，或者 2 个整数表示的元组或列表， 指明卷积沿宽度和高度方向的步长。 可以是一个整数，为所有空间维度指定相同的值。 指定任何 stride 值 != 1 与指定 dilation_rate 值 != 1 两者不兼容。<br>padding: “valid” 或 “same” (大小写敏感)。</p></blockquote></li><li><p>输入输出</p><blockquote><p>输入尺寸：<br>如果 data_format=’channels_first’， 输入 4D 张量，尺寸为 (samples, channels, rows, cols)。<br>如果 data_format=’channels_last’， 输入 4D 张量，尺寸为 (samples, rows, cols, channels)。<br>输出尺寸：<br>如果 data_format=’channels_first’， 输出 4D 张量，尺寸为 (samples, filters, new_rows, new_cols)。<br>如果 data_format=’channels_last’， 输出 4D 张量，尺寸为 (samples, new_rows, new_cols, filters)。</p></blockquote></li></ul><hr><p><strong>MaxPooling2D</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">                          strides=<span class="keyword">None</span>, padding=<span class="string">'valid'</span>,</span><br><span class="line">                          data_format=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure><ul><li>摘要<blockquote><p>对于空间数据的最大池化。</p></blockquote></li><li>重要参数<blockquote><p>pool_size: 整数，或者 2 个整数表示的元组， 沿（垂直，水平）方向缩小比例的因数。 （2，2）会把输入张量的两个维度都缩小一半。 如果只使用一个整数，那么两个维度都会使用同样的窗口长度。</p></blockquote></li></ul><p><strong>*AveragePooling2D</strong>与之相似不过完成的是平均池化操作。*</p><hr><p><strong>SimpleRNN</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.SimpleRNN(units, activation=<span class="string">'tanh'</span>, use_bias=<span class="keyword">True</span>,</span><br><span class="line">                       kernel_initializer=<span class="string">'glorot_uniform'</span>,</span><br><span class="line">                       recurrent_initializer=<span class="string">'orthogonal'</span>, </span><br><span class="line">                       bias_initializer=<span class="string">'zeros'</span>, kernel_regularizer=<span class="keyword">None</span>, </span><br><span class="line">                       recurrent_regularizer=<span class="keyword">None</span>, bias_regularizer=<span class="keyword">None</span>,</span><br><span class="line">                       activity_regularizer=<span class="keyword">None</span>, kernel_constraint=<span class="keyword">None</span>, </span><br><span class="line">                       recurrent_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>, </span><br><span class="line">                       dropout=<span class="number">0.0</span>, recurrent_dropout=<span class="number">0.0</span>, </span><br><span class="line">                       return_sequences=<span class="keyword">False</span>, return_state=<span class="keyword">False</span>, </span><br><span class="line">                       go_backwards=<span class="keyword">False</span>, stateful=<span class="keyword">False</span>, unroll=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><ul><li>摘要</li></ul><blockquote><p>全连接的 RNN，其输出将被反馈到输入。</p><ul><li>重要参数<br>input_dim: 输入的维度（整数）。 将此层用作模型中的第一层时，此参数（或者，关键字参数 input_shape）是必需的。</li><li>输入输出<br>输入尺寸：<br>3D 张量，尺寸为 (batch_size, timesteps, input_dim)。<br>输出尺寸：</li><li>如果 return_state：返回张量列表。 第一个张量为输出。剩余的张量为最后的状态， 每个张量的尺寸为 (batch_size, units)。</li><li>如果 return_sequences：返回 3D 张量， 尺寸为 (batch_size, timesteps, units)。</li><li>否则，返回尺寸为 (batch_size, units) 的 2D 张量。</li></ul><hr><p><strong>LSTM</strong></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.LSTM(units, activation=<span class="string">'tanh'</span>, </span><br><span class="line">                  recurrent_activation=<span class="string">'hard_sigmoid'</span>, use_bias=<span class="keyword">True</span>, </span><br><span class="line">                  kernel_initializer=<span class="string">'glorot_uniform'</span>, </span><br><span class="line">                  recurrent_initializer=<span class="string">'orthogonal'</span>, </span><br><span class="line">                  bias_initializer=<span class="string">'zeros'</span>, unit_forget_bias=<span class="keyword">True</span>, </span><br><span class="line">                  kernel_regularizer=<span class="keyword">None</span>, recurrent_regularizer=<span class="keyword">None</span>, </span><br><span class="line">                  bias_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>, </span><br><span class="line">                  kernel_constraint=<span class="keyword">None</span>, recurrent_constraint=<span class="keyword">None</span>, </span><br><span class="line">                  bias_constraint=<span class="keyword">None</span>, dropout=<span class="number">0.0</span>, recurrent_dropout=<span class="number">0.0</span>, </span><br><span class="line">                  implementation=<span class="number">1</span>, return_sequences=<span class="keyword">False</span>, </span><br><span class="line">                  return_state=<span class="keyword">False</span>, go_backwards=<span class="keyword">False</span>, stateful=<span class="keyword">False</span>, unroll=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><ul><li>摘要<blockquote><p>长短期记忆网络层</p></blockquote></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;完成一定实践后仔细阅读keras文档，做了关于核心网络层的一些摘要，主要汇总一些常用的网络层及其使用指南，大都在实践中使用过。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Dense&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>卷积知识小结</title>
    <link href="http://yoursite.com/2020/02/19/%E5%8D%B7%E7%A7%AF%E7%9F%A5%E8%AF%86%E5%B0%8F%E7%BB%93/"/>
    <id>http://yoursite.com/2020/02/19/卷积知识小结/</id>
    <published>2020-02-19T15:59:04.000Z</published>
    <updated>2020-02-19T15:59:26.260Z</updated>
    
    <content type="html"><![CDATA[<p>简单接触了一些卷积神经网络的知识，在此小结一下。由于它的一般应用于图像分类领域，所以在此不便详细图文并茂的解释，只是浅谈一些自己积累到的小知识点。关于卷积神经网络的入门级知识，我推荐<a href="https://zhuanlan.zhihu.com/p/27908027" target="_blank" rel="noopener">前往这里了解</a>。</p><hr><p>我们知道cnn其实就是在做一个特征提取器的工作。计算过程就是一个累乘累加的过程。将卷积核的</p><h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><ul><li>卷积神经网络：（卷积层+（可选）池化层）<em>N+全连接层</em>M</li><li>全卷积神经网络：（卷积层+（可选）池化层）<em>N+反卷积层</em>M</li></ul><p>由于卷积层和池化层一般情况下会使输出的尺寸不断变小提取出抽象的特征，由此可以用来处理分类问题。而全卷积神经网络将最后的全连接层全部换成反卷积层，使得我们可以得到和输入图片尺寸相同的输出，从而完成图片的物体分割工作。</p><h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>卷积解决了以下问题：</p><p>比如在图像分类问题中，网络由全连接层组成就会导致<strong>参数过多</strong>。会浪费我们的计算资源，并且能承载很大信息量的神经网络会轻易模拟出数据中的规律，而产生过拟合，降低模型的泛化能力。</p><h3 id="卷积核"><a href="#卷积核" class="headerlink" title="卷积核"></a>卷积核</h3><p>卷积如何解决问题涉及到卷积核的一部分内容，首先不得不提到我们必须了解的卷积神经网络的两个重要性质：</p><ul><li><p>[ ] 局部链接</p><p>输出单元单元通过卷积核与原图进行连接操作，而卷积核的大小一般不会太大所以减少了连接数目，一定程度上缓解了问题。</p></li><li>[ ] 参数共享</li></ul><p>卷积核一次只能和原图相同尺寸的区域产生来连接，而其余的地方采用滑动窗口的方式依次连接，也就是说卷积核前后是不变的，只是按规定的步长进行移动来生成最终的feature map。</p><p>经过以上两部分的操作可以大大降低参数过多的问题。</p><p>我们使用卷积其实是提取到特征，那这两种操作虽然解决了一些问题，但是否会对我们的特征提取产生影响呢？继续看图像处理的例子，比如人脸识别中脸颊部分的像素相近，嘴唇部分的像素相近，也就是说这样的图片是有信息冗余的，由此可见图像是有一定的区域性，所以在经过局部连接之后，仍然能够保留提取特征的能力。而特征应该还与其所在的位置无关，也就是说，假如人脸在图片的右下角或左上角，对于脸部的特征不论出现在图片的什么位置，都应该被识别出来，而参数共享就恰好对应了这一特点，使得无论这一特征出现在什么位置都会得到好的匹配结果。只能匹配到固定位置的特征毫无疑问就是一种过拟合的表现，而恰恰参数共享避免了这种情况。</p><h3 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h3><p>卷积后的数据量减小了，但仍然过于庞大，池化操作就是在减少数据量。池化分为最大值池化和平均池化。最常用的是最大值池化，我们主要介绍这种。最大池化保留了每一个小块内的最大值，所以它相当于保留了这一块最佳匹配结果。这也就意味着它不会具体关注窗口内到底是哪一个地方匹配了，而只关注是不是有某个地方匹配上了。这也就能够看出，CNN能够发现图像中是否具有某种特征，而不用在意到底在哪里具有这种特征（似乎和参数共享有类似的作用）。</p><h3 id="非线性激活"><a href="#非线性激活" class="headerlink" title="非线性激活"></a>非线性激活</h3><p>激活函数比如Relu，它的公式：$f(x)=max(0,x)$即，保留大于等于0的值，其余所有小于0的数值直接改写为0。卷积后的图中的值，越小则越不相关，我们进行特征提取时，为了使得数据更少，操作更方便，就直接舍弃掉那些不相关联的数据（直接取零）。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;简单接触了一些卷积神经网络的知识，在此小结一下。由于它的一般应用于图像分类领域，所以在此不便详细图文并茂的解释，只是浅谈一些自己积累到的小知识点。关于卷积神经网络的入门级知识，我推荐&lt;a href=&quot;https://zhuanlan.zhihu.com/p/27908027
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>HDF5--python 使用简介</title>
    <link href="http://yoursite.com/2020/02/12/HDF5-python-%E4%BD%BF%E7%94%A8%E7%AE%80%E4%BB%8B/"/>
    <id>http://yoursite.com/2020/02/12/HDF5-python-使用简介/</id>
    <published>2020-02-12T15:52:23.000Z</published>
    <updated>2020-02-12T15:56:28.834Z</updated>
    
    <content type="html"><![CDATA[<p><em>引：在使用TensorFlow，学习回调函数时，使用了ModelCheckpoint()，产生了.h5的文件。为了了解回调函数产生的信息，需要了解HDF5的相关内容，以及在python中的使用与相关问题解决。</em><br>文中一些叙述为了方便初次接触者理解，表述并不严谨，仅供简单参考。</p><hr><h2 id="初次见面"><a href="#初次见面" class="headerlink" title="初次见面"></a>初次见面</h2><p>HDF5（Hierarchical Data Formal）是用于存储大规模数值数据的较为理想的存储格式，文件后缀名为h5，存储读取速度非常快，且可在文件内部按照明确的层次存储数据，同一个HDF5可以看做一个高度整合的文件夹，其内部可存放不同类型的数据。</p><p>在Python中操纵HDF5文件的方式主要有两种</p><ul><li>是利用<strong>pandas</strong>中内建的一系列HDF5文件操作相关的方法，来完成相关操作。</li><li>二是利用<strong>h5py</strong>模块来完成Python原生数据结构与HDF5格式的转化</li></ul><p>本篇主要介绍hdf5的基础内容和对应模块使用的快速入门。</p><h2 id="初遇时的差池"><a href="#初遇时的差池" class="headerlink" title="初遇时的差池"></a>初遇时的差池</h2><p><del>一段小插曲</del><br>HDF是HDF(Hierarchical Data File)是美国国家高级计算应用中心(National Center for Supercomputing Application,NCSA)为了满足各种领域研究需求而研制的一种能高效存储和分发科学数据的新型数据格式 。阅读的文档中提到了到国家卫星气象中心（NSMC）曾经发布过一份《HDF5.0 使用简介》,抱着些许迷信权威的心态阅读后发现它涉及的信息就我目前来说价值不大，其中主要有讲HDF5文件的组织，API，创建，数据集数据空间，组群，属性等等，内容大而全，但似乎这篇教材发布的相对较早，所以产生了一定的局限性并且内部的相关URL都失效了，它本身也在国家卫星气象中心的官网上没有什么存在的痕迹。相关的API只涉及了C和FORTRAN的外壳包装函数。对于像我这样在使用python且第一次接触HDF5的使用者并不友好，索性只读了开头的基础内容并建立了更详细的认知后就放弃了继续阅读的打算。</p><h2 id="相识"><a href="#相识" class="headerlink" title="相识"></a>相识</h2><p>首先从hdf5文件讲起。</p><p>HDF5文件具有两类存储对象，dataset和group。dataset是类似于数组的数据集，而group是类似文件夹一样的容器，存放dataset和其他group。</p><p>HDF本意即是<strong>层次数据格式</strong>，所以就其存储结构来说是类似与POSIX风格的。其实现的方式就是group。每层都用’/‘分隔。我们创建的file object其实也可以看作一个group，是一个root group，其余的groups可以称为subgroups。</p><p>dataset与numpy中的array相似，比如都具有shape、dtype、以及一些切片操作等。虽然与Numpy的数组在接口上很相近，但是支持更多对外透明的存储特征，如数据压缩，误差检测，分块传输。</p><p>HDF5的一个很好的features就是可以在数据旁边存储元数据<sup><a href="#fn_1" id="reffn_1">1</a></sup>。所有的group和dataset都支持叫做<strong>属性</strong>的数据形式。</p><h4 id="h5py"><a href="#h5py" class="headerlink" title="h5py"></a>h5py</h4><p>想到python一定有对应的文件解析库，于是我开始了h5py的“快速”入门。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">import h5py</span><br><span class="line"># 读取</span><br><span class="line">file = h5py.File(&apos;test.hdf5&apos;, &apos;r&apos;)# .hdf5与.h5意义相同</span><br><span class="line"># 一下也可以完成读取</span><br><span class="line">with h5py.File(&quot;mytestfile.hdf5&quot;, &quot;w&quot;) as f:</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"># 我们需要注意的是group（包括File对象）与python中的字典相似</span><br><span class="line"># 通过该方法可以获得对应group下的subgroups或datasets，返回包含字符串的列表</span><br><span class="line">f.keys()</span><br><span class="line"></span><br><span class="line"># 此时我们假设存在一个名为DataSet的dataset对象</span><br><span class="line"># 利用对应键来索引值的方法可以获取该对象</span><br><span class="line">dest = f[&apos;DataSet&apos;]</span><br><span class="line"></span><br><span class="line"># dataset对象满足我们平时使用的numpy数组的一些操作，如下：</span><br><span class="line">dest.shape</span><br><span class="line">dest.dtype</span><br><span class="line">dest[:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 创建HDF5文件</span><br><span class="line">f = h5py.File(&apos;test.h5&apos;, &apos;w&apos;)</span><br><span class="line"></span><br><span class="line"># 使用create_dataset创建给定形状和数据类型的空dataset</span><br><span class="line">dataset = f.create_dataset(&apos;DS&apos;,(100,), dtype=&apos;i&apos;)</span><br><span class="line"># 也可以使用numpy中数组来初始化</span><br><span class="line">array = np.arange(100)</span><br><span class="line">dataset = f.create_dataset(&apos;init&apos;, data=array)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 分块存储</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">在缺省设置下，HDF5数据集在内存中是连续布局的，也就是按照传统的C序。</span><br><span class="line">Dataset也可以在HDF5的分块存储布局下创建。</span><br><span class="line">也就是dataset被分为大小相同的若干块随意地分布在磁盘上，并使用B树建立索引。</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"># 为了进行分块存储，将关键字设为一个元组来指示块的形状。</span><br><span class="line">dset = f.create_dataset(&quot;chunked&quot;, (1000, 1000), chunks=(100, 100))</span><br><span class="line"># 也可以自动分块，不必指定块的形状。</span><br><span class="line">dset = f.create_dataset(&quot;autochunk&quot;, (1000, 1000), chunks=True)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 分层结构</span><br><span class="line"># 遍历subgroups</span><br><span class="line">for name in f:</span><br><span class="line">    print(name)</span><br><span class="line"># 递归遍历所有subgroups</span><br><span class="line">def print_name(name):</span><br><span class="line">    print(name)</span><br><span class="line"></span><br><span class="line">f.visit(print_name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 属性通过attrs成员访问，类似于python中词典格式。</span><br><span class="line"> dataset.attrs[&apos;bias&apos;] = 60</span><br><span class="line"> &apos;bias&apos; in dataset.attrs</span><br></pre></td></tr></table></figure><p>一些其他的特性</p><ol><li>滤波器组<br>HDF5的滤波器组能够对分块数组进行变换。最常用的变换是高保真压缩。使用一个特定的压缩滤波器创建dataset之后，读写都可以向平常一样，不必添加额外的步骤。<br>用关键词compression来指定压缩滤波器，而滤波器的可选参数使用关键词compression_opt来指定：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dset = f.create_dataset(&quot;zipped&quot;, (100, 100), compression=&quot;gzip&quot;)</span><br></pre></td></tr></table></figure><ol><li>HDF5文件的限制</li></ol><ul><li>HDF5文件本身大小没有限制，但是HDF5的一个dataset最高允许32个维，每个维度最多可有2^64个值，每个值大小理论上可以任意大</li><li>目前一个chunk允许的最大容量为2^32-1 byte (4GB). 大小固定的dataset的块的大小不能超过dataset的大小。</li></ul><p><a href="http://docs.h5py.org/en/latest/index.html" title="HDF5 for Python -- h5py 2.10.0 doc" target="_blank" rel="noopener">更多信息</a></p><h4 id="pandas"><a href="#pandas" class="headerlink" title="pandas"></a>pandas</h4><p><strong> 写出 </strong></p><p>　　pandas中的HDFStore()用于生成管理HDF5文件IO操作的对象，其主要参数如下：</p><p>　　path：字符型输入，用于指定h5文件的名称（不在当前工作目录时需要带上完整路径信息）</p><p>　　mode：用于指定IO操作的模式，与Python内建的open()中的参数一致，默认为’a’，即当指定文件已存在时不影响原有数据写入，指定文件不存在时则新建文件；’r’，只读模式；’w’，创建新文件（会覆盖同名旧文件）；’r+’，与’a’作用相似，但要求文件必须已经存在；</p><p>　　complevel：int型，用于控制h5文件的压缩水平，取值范围在0-9之间，越大则文件的压缩程度越大，占用的空间越小，但相对应的在读取文件时需要付出更多解压缩的时间成本，默认为0，代表不压缩</p><p>　　下面我们创建一个HDF5 IO对象store：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">store = pd.HDFStore(<span class="string">'demo.h5'</span>)</span><br><span class="line"><span class="string">'''查看store类型'''</span></span><br><span class="line">print(store)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">pandas</span>.<span class="title">io</span>.<span class="title">pytables</span>.<span class="title">HDFStore</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">File</span> <span class="title">path</span>:</span> demo.h5</span><br></pre></td></tr></table></figure><p>可以看到store对象属于pandas的io类，通过上面的语句我们已经成功的初始化名为demo.h5的的文件，本地也相应的出现了对应文件。</p><p>接下来我们创建pandas中不同的两种对象，并将它们共同保存到store中，首先创建series对象：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建一个series对象</span></span><br><span class="line">s = pd.Series(np.random.randn(<span class="number">5</span>), index=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>, <span class="string">'e'</span>])</span><br><span class="line"><span class="comment">#创建一个dataframe对象</span></span><br><span class="line">df = pd.DataFrame(np.random.randn(<span class="number">8</span>, <span class="number">3</span>),</span><br><span class="line">                 columns=[<span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'C'</span>])</span><br></pre></td></tr></table></figure><p>第一种方式利用键值对将不同的数据存入store对象中，这里为了代码简洁使用了元组赋值法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">store[<span class="string">'s'</span>],store[<span class="string">'df'</span>] = s,df</span><br></pre></td></tr></table></figure><p>第二种方式利用store对象的put()方法，其主要参数如下：</p><p>　　key：指定h5文件中待写入数据的key</p><p>　　value：指定与key对应的待写入的数据</p><p>　　format：字符型输入，用于指定写出的模式，’fixed’对应的模式速度快，但是不支持追加也不支持检索；’table’对应的模式以表格的模式写出，速度稍慢，但是支持直接通过store对象进行追加和表格查询操作</p><p>使用put()方法将数据存入store对象中：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">store.put(key=<span class="string">'s'</span>,value=s);store.put(key=<span class="string">'df'</span>,value=df)</span><br></pre></td></tr></table></figure></p><p>既然是键值对的格式，那么可以查看store的items属性（注意这里store对象只有items和keys属性，没有values属性）：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">store.items</span><br></pre></td></tr></table></figure></p><p>调用store对象中的数据直接用对应的键名来索引即可：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">store[<span class="string">'df'</span>]</span><br></pre></td></tr></table></figure></p><p>删除store对象中指定数据的方法有两种，一是使用remove()方法，传入要删除数据对应的键：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">store.remove(<span class="string">'s'</span>)</span><br><span class="line">print(store.keys())</span><br></pre></td></tr></table></figure><p>　　二是使用Python中的关键词del来删除指定数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span> store[<span class="string">'s'</span>]</span><br><span class="line">print(store.keys())</span><br></pre></td></tr></table></figure><p>这时若想将当前的store对象持久化到本地，只需要利用close()方法关闭store对象即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">store.close()</span><br><span class="line"><span class="string">'''查看store连接状况，False则代表已关闭'''</span></span><br><span class="line">store.is_open</span><br><span class="line"><span class="comment"># 这时本地的h5文件也相应的存储进store对象关闭前包含的文件</span></span><br></pre></td></tr></table></figure><p>　除了通过定义一个确切的store对象的方式，还可以从pandas中的数据结构直接导出到本地h5文件中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建新的数据框</span></span><br><span class="line">df_ = pd.DataFrame(np.random.randn(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line"><span class="comment">#导出到已存在的h5文件中，这里需要指定key</span></span><br><span class="line">df_.to_hdf(path_or_buf=<span class="string">'demo.h5'</span>,key=<span class="string">'df_'</span>)</span><br><span class="line"><span class="comment">#创建于本地demo.h5进行IO连接的store对象</span></span><br><span class="line">store = pd.HDFStore(<span class="string">'demo.h5'</span>)</span><br><span class="line"><span class="comment">#查看指定h5对象中的所有键</span></span><br><span class="line">print(store.keys())</span><br></pre></td></tr></table></figure><p><strong>读入</strong><br>在pandas中读入HDF5文件的方式主要有两种，一是通过上一节中类似的方式创建与本地h5文件连接的IO对象，接着使用键索引或者store对象的get()方法传入要提取数据的key来读入指定数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">store = pd.HDFStore(<span class="string">'demo.h5'</span>)</span><br><span class="line"><span class="string">'''方式1'''</span></span><br><span class="line">df1 = store[<span class="string">'df'</span>]</span><br><span class="line"><span class="string">'''方式2'''</span></span><br><span class="line">df2 = store.get(<span class="string">'df'</span>)</span><br><span class="line">df1 == df2</span><br></pre></td></tr></table></figure><p>可以看出这两种方式都能顺利读取键对应的数据。</p><p>　　第二种读入h5格式文件中数据的方法是pandas中的read_hdf()，其主要参数如下：</p><p>　　path_or_buf：传入指定h5文件的名称</p><p>　　key：要提取数据的键</p><p>　　需要注意的是利用read_hdf()读取h5文件时对应文件不可以同时存在其他未关闭的IO对象，否则会报错，如下例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(store.is_open)</span><br><span class="line">df = pd.read_hdf(<span class="string">'demo.h5'</span>,key=<span class="string">'df'</span>)</span><br></pre></td></tr></table></figure><p>　把IO对象关闭后再次提取：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">store.close()</span><br><span class="line">print(store.is_open)</span><br><span class="line">df = pd.read_hdf(<span class="string">'demo.h5'</span>,key=<span class="string">'df'</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure><hr><p>参考：<br><a href="https://blog.csdn.net/yudf2010/article/details/50353292" target="_blank" rel="noopener">https://blog.csdn.net/yudf2010/article/details/50353292</a><br><a href="https://segmentfault.com/a/119000001667088" target="_blank" rel="noopener">https://segmentfault.com/a/119000001667088</a><br><a href="https://www.cnblogs.com/feffery/p/11135082.html" target="_blank" rel="noopener">https://www.cnblogs.com/feffery/p/11135082.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;em&gt;引：在使用TensorFlow，学习回调函数时，使用了ModelCheckpoint()，产生了.h5的文件。为了了解回调函数产生的信息，需要了解HDF5的相关内容，以及在python中的使用与相关问题解决。&lt;/em&gt;&lt;br&gt;文中一些叙述为了方便初次接触者理解，表述
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>激活函数</title>
    <link href="http://yoursite.com/2020/02/06/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    <id>http://yoursite.com/2020/02/06/激活函数/</id>
    <published>2020-02-05T18:32:07.000Z</published>
    <updated>2020-02-06T04:38:03.183Z</updated>
    
    <content type="html"><![CDATA[<p>借此机会把深度学习中常用的激活函数做一些总结。</p><h3 id="激活函数概念"><a href="#激活函数概念" class="headerlink" title="激活函数概念"></a>激活函数概念</h3><p>所谓激活函数（Activation Function），就是在人工神经网络的神经元上运行的函数，负责将神经元的输入映射到输出端。</p><h3 id="什么是激活函数"><a href="#什么是激活函数" class="headerlink" title="什么是激活函数"></a>什么是激活函数</h3><p>激活函数（Activation functions）对于人工神经网络模型去学习、理解非常复杂和非线性的函数来说具有十分重要的作用。它们将非线性特性引入到我们的网络中。神经元中，输入的 inputs 通过加权，求和后，还被作用了一个函数，这个函数就是激活函数。引入激活函数是为了增加神经网络模型的非线性。没有激活函数的每层都相当于矩阵相乘。就算你叠加了若干层之后，无非还是个矩阵相乘罢了。</p><h3 id="为什么使用激活函数"><a href="#为什么使用激活函数" class="headerlink" title="为什么使用激活函数"></a>为什么使用激活函数</h3><p>如果不用激活函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合，这种情况就是最原始的感知机（Perceptron）。<br>如果使用的话，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。</p><h3 id="个人认知与理解"><a href="#个人认知与理解" class="headerlink" title="个人认知与理解"></a>个人认知与理解</h3><p>我觉得机器学习的传统算法与深度学习算法的比较重要的区别是：</p><ol><li>从广义上来讲，机器学习的传统算法一般只能使用一种目标模型函数，比如逻辑回归使用logistic函数、只能解决单维度问题；而深度学习可以在不同神经层使用不同或者多种激活函数、因此拥有多种或者不同函数的特性，所以解决问题具有多维度、线性、非线性等处理能力</li><li>深度学习的激活函数使得深度学习算法既能解决简单的线性问题、也能处理复杂的非线性问题</li><li>数据中的特征往往具有不同的特性、特征与不同模型之间也有较大的辨识差异，机器学习的传统算法的单一模型可能只能对部分特征产生重要作用，而深度学习的多种激活函数则比较全面、多维度对特征进行学习</li></ol><h3 id="常用的激活函数"><a href="#常用的激活函数" class="headerlink" title="常用的激活函数"></a>常用的激活函数</h3><ol><li>sigmoid 函数</li><li>tanh 函数</li><li>relu 函数</li><li>leaky relu 函数</li><li>elu 函数</li><li>softmax 函数</li></ol><h3 id="饱和激活函数与非饱和激活函数"><a href="#饱和激活函数与非饱和激活函数" class="headerlink" title="饱和激活函数与非饱和激活函数"></a>饱和激活函数与非饱和激活函数</h3><ol><li>饱和函数是指当自变量 x 达到某个值(或者说趋于无穷小、无穷大)的时候，因变量 y 就不再发生变化，而是趋于某一个固定的值</li></ol><ul><li>sigmoid 函数就是一个饱和激活函数，当自变量 z 趋于无穷小时，因变量 y 趋于 0；当自变量 z 趋于无穷大时，因变量 y 趋于 1</li><li>tanh 函数就是一个饱和激活函数，当自变量 z 趋于无穷小时，因变量 y 趋于 -1；当自变量 z 趋于无穷大时，因变量 y 趋于 1</li></ul><ol><li>饱和函数是指当自变量 x 达到某个值(或者说趋于无穷小、无穷大)的时候，因变量 y 就依然发生变化，并不是趋于某一个固定的值</li></ol><ul><li>relu 函数就是一个非饱和激活函数，当自变量 z 小于 0 时，因变量 y 等于 0；但当自变量 z 大于 0 时，因变量 y 是一个 z 的变化值</li><li>relu 的变种激活函数</li></ul><ol><li>非饱和激活函数的优势</li></ol><ul><li>首先，“非饱和激活函数”能解决所谓的“梯度消失”问题</li><li>其次，它能加快收敛速度<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">-10</span>, <span class="number">10</span>, <span class="number">100</span>)</span><br></pre></td></tr></table></figure></li></ul><h3 id="1-sigmoid激活函数-饱和函数"><a href="#1-sigmoid激活函数-饱和函数" class="headerlink" title="1.sigmoid激活函数(饱和函数)"></a>1.sigmoid激活函数(饱和函数)</h3><p>sigmoid函数计算公式<br>$<br>\sigma(z) =\frac{1}{1+e^{-z}},e\approx 2.7183<br>$<br>sigmoid函数的导数公式<br>$<br>{\sigma(z)}’ =\sigma(z)(1-\sigma(z)),e\approx 2.7183<br>$<br>sigmoid函数评价</p><ul><li>优点：sigmoid函数有效地将实数域的线性问题映射到[0,1]区间的类别概率问题，实现分类；</li><li>缺点：然而在深度学习算法中使用sigmoid函数有时候在反向求导传播时会导致梯度消失的现象：<br>当z很大时，$\sigma(z)$趋近于1，当z很小时，$\sigma(z)$趋近于0<br>其导数${\sigma(z)}’ =\sigma(z)(1-\sigma(z))$在z很大或很小时都会趋近于0，造成梯度消失的现象<h1 id="手写计算函数"><a href="#手写计算函数" class="headerlink" title="手写计算函数"></a>手写计算函数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z, mode=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> mode:  <span class="comment"># 手写公式</span></span><br><span class="line">        L = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(z)):</span><br><span class="line">            sigmoid_value = <span class="number">1</span>/(<span class="number">1</span>+(<span class="number">2.7183</span>)**(-z[i]))</span><br><span class="line">            L.append(sigmoid_value)</span><br><span class="line">        <span class="keyword">return</span> L</span><br><span class="line">    <span class="keyword">else</span>:  <span class="comment"># 使用numpy</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br><span class="line">plt.plot(x, sigmoid(z=x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用 Tensorflow 的 sigmoid 函数</span></span><br><span class="line">y_sigmoid = tf.nn.sigmoid(x)</span><br><span class="line">plt.plot(x, y_sigmoid)</span><br></pre></td></tr></table></figure></li></ul><h3 id="2-tanh激活函数-饱和函数"><a href="#2-tanh激活函数-饱和函数" class="headerlink" title="2.tanh激活函数(饱和函数)"></a>2.tanh激活函数(饱和函数)</h3><p>tanh函数计算公式<br>$<br>tanh(z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}},e\approx 2.7183<br>$<br>tanh函数的导数公式<br>$<br>{tanh(z)}’=1-(tanh(z))^{2},e\approx 2.7183<br>$<br>tanh函数评价</p><ul><li>优点：sigmoid函数有效地将实数域的线性问题映射到[-1,1]区间的类别概率问题，实现分类；</li><li>缺点：然而在深度学习算法中使用tanh函数有时候在反向求导传播时会导致梯度消失的现象：<br>当z很大时，tanh(z)\tanh(z)tanh(z)趋近于1，当z很小时，tanh(z)tanh(z)tanh(z)趋近于-1<br>其导数 ${tanh(z)}’=1-(tanh(z))^{2}$在z很大或很小时都会趋近于0，造成梯度消失的现象<br>tanh与sigmoid的关系式：<br>$tanh(z)=2sigmoid(2z)-1$<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tanh</span><span class="params">(z, mode=True)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> mode:  <span class="comment"># 手写公式</span></span><br><span class="line">        L = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(z)):</span><br><span class="line">            exp1 = <span class="number">2.7183</span>**(z[i])</span><br><span class="line">            exp2 = <span class="number">2.7183</span>**(-z[i])</span><br><span class="line">            tanh_value = (exp1-exp2)/(exp1+exp2)</span><br><span class="line">            L.append(tanh_value)</span><br><span class="line">        <span class="keyword">return</span> L</span><br><span class="line">    <span class="keyword">else</span>:  <span class="comment"># 使用numpy</span></span><br><span class="line">        <span class="keyword">return</span> (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))</span><br><span class="line">plt.plot(x,tanh(z=x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用 Tensorflow 的 tanh 函数</span></span><br><span class="line">y_tanh = tf.nn.tanh(x)</span><br><span class="line">plt.plot(x, y_tanh)</span><br></pre></td></tr></table></figure></li></ul><h3 id="3-relu激活函数-非饱和函数"><a href="#3-relu激活函数-非饱和函数" class="headerlink" title="3.relu激活函数(非饱和函数)"></a>3.relu激活函数(非饱和函数)</h3><p>修正线性单元（Rectified linear unit，ReLU）</p><p>relu函数计算公式<br>$relu(z)=max(0,z)$<br>relu函数的导数公式<br>$<br>{relu(z)}’=\left\{\begin{matrix}1 &amp; z&gt; 0\ 0 &amp; z\leq 0\end{matrix}\right.<br>$<br>relu函数评价</p><ul><li>优点：<br>relu函数的非饱和性可以有效地解决梯度消失的问题，提供相对较宽的激活边界<br>relu函数是阈值函数，运算简单且快速，只需通过阈值判断就可以得到激活值，而sigmoid、tanh函数都需要计算指数，运算复杂<br>ReLU的单侧抑制(负梯度都为 0)提供了网络的稀疏表达能力</li><li>缺点：<br>relu(z)=max(0,z)relu(z)=max(0,z)relu(z)=max(0,z)会导致负梯度的神经元产生不可逆的死亡，也称为死亡神经元，由于负梯度值都为 0，在往后的运算中都将以 0 传播<br>如果学习率（Learning Rate）设置较大，会导致超过一定比例的神经元不可逆死亡，进而参数梯度无法更新，整个训练过程失败<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(x)</span>:</span>  <span class="comment"># 手写公式</span></span><br><span class="line">    relu_list = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x)):</span><br><span class="line">        relu_list.append(max(<span class="number">0</span>, x[i]))</span><br><span class="line">    <span class="keyword">return</span> relu_list</span><br><span class="line">plt.ylim(<span class="number">-1</span>,<span class="number">6</span>)</span><br><span class="line">plt.plot(x, relu(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用 Tensorflow 的 relu 函数</span></span><br><span class="line">y_relu = tf.nn.relu(x)</span><br><span class="line">plt.ylim(<span class="number">-1</span>,<span class="number">6</span>)</span><br><span class="line">plt.plot(x, y_relu)</span><br></pre></td></tr></table></figure></li></ul><h3 id="4-leaky-relu"><a href="#4-leaky-relu" class="headerlink" title="4.leaky relu"></a>4.leaky relu</h3><p>带泄露修正线性单元（Leaky ReLU）函数是经典（以及广泛使用的）的ReLu激活函数的变体，该函数输出对负值输入有很小的坡度，它旨在解决负梯度神经元死亡的问题。由于负梯度神经元导数总是不为零，这能减少静默神经元的出现，允许基于梯度的学习（虽然会很慢），解决了Relu函数进入负区间后神经元不学习的问题。</p><p>leaky relu 函数计算公式<br>$Lrelu(z)=max(0.1z,z),a&lt;1(a=0.1)$,其中a可以自定义</p><p>leaky relu 函数的导数公式<br>$<br>{Lrelu(z)}’=\left\{\begin{matrix}1 &amp; z&gt;0\ 0.1 &amp;z\leq 0 \end{matrix}\right.<br>$<br>leaky relu 函数评价</p><ul><li>优点：<br>旨在解决 relu 负梯度神经元死亡的问题，函数输出对负值输入有很小的坡度<br>解决了Relu函数进入负区间后神经元不学习的问题</li><li>缺点：<br>负梯度的 a 难以寻求合适的系数值，a 通常小于 1 且通常使用 a = 0.1<br>负梯度的 a 的选择和确定需要一定的经验或者做实验验证，相对麻烦一些<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">leaky_rule</span><span class="params">(z)</span>:</span>  <span class="comment"># 手写公式</span></span><br><span class="line">    L = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(z)):</span><br><span class="line">        lrule_value = max(<span class="number">0.1</span>*z[i], z[i])</span><br><span class="line">        L.append(lrule_value)</span><br><span class="line">    <span class="keyword">return</span> L</span><br><span class="line">plt.plot(x, leaky_rule(z=x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用 Tensorflow 的 leaky_relu 函数</span></span><br><span class="line">y_lrelu = tf.nn.leaky_relu(x)</span><br><span class="line">plt.plot(x, y_lrelu)</span><br></pre></td></tr></table></figure></li></ul><h3 id="5-elu"><a href="#5-elu" class="headerlink" title="5.elu"></a>5.elu</h3><p>指数线性单元（Exponential Linear Units）</p><p>elu 函数计算公式<br>$elu(z)={za(ez−1)z≥0z&lt;0$,tensorflow中,a默认为1</p><p>elu 函数的导数公式<br>$<br>{elu(z)}’=\left\{\begin{matrix}1 &amp; z\leq 0\ ae^{z} &amp; z&lt;0\end{matrix}\right.<br>$<br>elu 函数评价</p><p>融合了sigmoid和ReLU，左侧具有软饱和性，右侧无饱和性。<br>右侧线性部分使得ELU能够缓解梯度消失，而左侧软饱能够让ELU对输入变化或噪声更鲁棒。<br>ELU的输出均值接近于零，所以收敛速度更快。<br>在 ImageNet上，不加 Batch Normalization 30 层以上的 ReLU 网络会无法收敛，PReLU网络在MSRA的Fan-in （caffe ）初始化下会发散，而 ELU 网络在Fan-in/Fan-out下都能收敛。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">elu</span><span class="params">(z, a=<span class="number">0.1</span>)</span>:</span>  <span class="comment"># 手写公式</span></span><br><span class="line">    L = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(z)):</span><br><span class="line">        <span class="keyword">if</span> z[i] &gt;= <span class="number">0</span>:</span><br><span class="line">            elu_value = z[i]</span><br><span class="line">            L.append(elu_value)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            elu_value = a*(<span class="number">2.7183</span>**(z[i]) - <span class="number">1</span>)</span><br><span class="line">            L.append(elu_value)</span><br><span class="line">    <span class="keyword">return</span> L</span><br><span class="line">plt.plot(x, elu(z=x, a=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用 Tensorflow 的 elu 函数</span></span><br><span class="line">y_elu = tf.nn.elu(x)</span><br><span class="line">plt.plot(x, y_elu)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;借此机会把深度学习中常用的激活函数做一些总结。&lt;/p&gt;
&lt;h3 id=&quot;激活函数概念&quot;&gt;&lt;a href=&quot;#激活函数概念&quot; class=&quot;headerlink&quot; title=&quot;激活函数概念&quot;&gt;&lt;/a&gt;激活函数概念&lt;/h3&gt;&lt;p&gt;所谓激活函数（Activation Funct
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tensorflow基础</title>
    <link href="http://yoursite.com/2020/02/03/tensorflow%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2020/02/03/tensorflow基础/</id>
    <published>2020-02-03T04:11:06.000Z</published>
    <updated>2020-02-06T04:11:56.931Z</updated>
    
    <content type="html"><![CDATA[<h2 id="第一部分：Tensorflow基础"><a href="#第一部分：Tensorflow基础" class="headerlink" title="第一部分：Tensorflow基础"></a>第一部分：Tensorflow基础</h2><h3 id="Tensors张量"><a href="#Tensors张量" class="headerlink" title="Tensors张量"></a>Tensors张量</h3><h4 id="常量constant"><a href="#常量constant" class="headerlink" title="常量constant:"></a>常量constant:</h4><p>‘’’python<br>x = tf.constant([[4,2],[9,5]])<br>print(x)</p><p>tf.Tensor(<br>[[4 2]<br> [9 5]], shape=(2, 2), dtype=int32)<br>‘’’</p><p>可以通过“.numpy()”来得到numpy array类型</p><p>‘’’python<br>x.numpy()</p><p>array([[4, 2],<br>       [9, 5]], dtype=int32)<br>‘’</p><p>像numpy一样，有shape和dtype属性</p><p>‘’’python<br>print(‘shape:’,x.shape)<br>print(‘dx.dtype)</p><p>(2, 2)</p><p><dtype: 'int32'=""><br>‘’’</dtype:></p><p>常用的产生常量的方法是tf.ones和tf.zeros就像numpy的np.ones``np.zeros</p><p>‘’’python<br>print(tf.ones(shape=(2,3)))<br>print(tf.zeros(shape=(3,2)))</p><p>tf.Tensor(<br>[[1. 1. 1.]<br> [1. 1. 1.]], shape=(2, 3), dtype=float32)<br>tf.Tensor(<br>[[0. 0.]<br> [0. 0.]<br> [0. 0.]], shape=(3, 2), dtype=float32)<br>‘’’</p><h3 id="随机数常量random-constant正态分布"><a href="#随机数常量random-constant正态分布" class="headerlink" title="随机数常量random constant正态分布"></a>随机数常量random constant正态分布</h3><p>‘’’python<br>tf.random.normal(shape=(2,2),mean=0,stddev=1.0)</p><tf.tensor: id="12," shape="(2," 2),="" dtype="float32," numpy="array([[-0.05229542," 0.64488363],="" [="" 0.37966082,="" 1.0098479="" ]],=""><h1 id="整数均匀分布"><a href="#整数均匀分布" class="headerlink" title="整数均匀分布"></a>整数均匀分布</h1><p>tf.random.uniform(shape=(2,2),minval=0,maxval=10,dtype=tf.int32)</p><p><tf.tensor: id="16," shape="(2," 2),="" dtype="int32," numpy="array([[6," 3],="" [8,="" 7]],=""><br>‘’’</tf.tensor:></p><h3 id="Variables变量"><a href="#Variables变量" class="headerlink" title="Variables变量"></a>Variables变量</h3><p>变量是一种特别的张量，用来存储可变数值，需要用一些值来初始化<br>‘’’pyhon<br>initial_value = tf.random.normal(shape=(2,2))<br>a = tf.Variable(initial_value)<br>print(a)</p><p><tf.variable 'variable:0'="" shape="(2," 2)="" dtype="float32," numpy="array([[" 0.07630513,="" -0.39769924],="" [-0.9712114="" ,="" -0.62955064]],=""><br>‘’’</tf.variable></p><p>可以通过assign(value)来赋值“=”，或assign_add(value)“+=”，或assign_sub(value)“-=”<br>‘’’python<br>new_value = tf.random.normal(shape=(2, 2))<br>a.assign(new_value)<br>for i in range(2):<br>    for j in range(2):<br>        assert a[i, j] == new_value[i, j]</p><p>added_value = tf.random.normal(shape=(2,2))<br>a.assign_add(added_value)<br>for i in range(2):<br>    for j in range(2):<br>        assert a[i,j] == new_value[i,j]+added_value[i,j]<br>‘’’</p><h3 id="Tensorflow数学运算"><a href="#Tensorflow数学运算" class="headerlink" title="Tensorflow数学运算"></a>Tensorflow数学运算</h3><p>可以像numpy那样做作运算，Tensorflow的不同时这些运算可以放到GPU或TPU上执行<br>‘’’python<br>a = tf.random.normal(shape=(2,2))<br>b = tf.random.normal(shape=(2,2))<br>c = a+b<br>d = tf.square(c)<br>e = tf.exp(c)<br>print(a)<br>print(b)<br>print(c)<br>print(d)<br>print(e)</p><p>tf.Tensor(<br>[[ 1.6862711 -1.4246397]<br> [-1.0287055 -1.3188182]], shape=(2, 2), dtype=float32)<br>tf.Tensor(<br>[[ 1.4519434  0.7635907]<br> [ 1.1213834 -1.4559215]], shape=(2, 2), dtype=float32)<br>tf.Tensor(<br>[[ 3.1382146  -0.661049  ]<br> [ 0.09267795 -2.7747397 ]], shape=(2, 2), dtype=float32)<br>tf.Tensor(<br>[[9.8483906e+00 4.3698579e-01]<br> [8.5892025e-03 7.6991806e+00]], shape=(2, 2), dtype=float32)<br>tf.Tensor(<br>[[23.062654    0.51630944]<br> [ 1.0971084   0.0623657 ]], shape=(2, 2), dtype=float32)<br>‘’’</p><h2 id="GradientTape计算梯度"><a href="#GradientTape计算梯度" class="headerlink" title="GradientTape计算梯度"></a>GradientTape计算梯度</h2><p>和numpy的另一个不同是，可以自动跟踪任何变量的梯度。<br>打开一个GradientTape,然后通过tape.watch()来跟踪变量<br>‘’’python<br>a = tf.random.normal(shape=(2,2))<br>b = tf.random.normal(shape=(2,2))<br>with tf.GradientTape() as tape:<br>    tape.watch(a)#开始记录所有有关a参与过的运算<br>    c = tf.sqrt(tf.square(a)+tf.square(b)) #变量a做一些运算</p><pre><code>#计算c对于a的梯度dc_da = tape.gradient(c,a)print(dc_da)</code></pre><p>tf.Tensor(<br>[[-0.53557533  0.87920487]<br> [ 0.24663754  0.4680054 ]], shape=(2, 2), dtype=float32)<br>‘’’</p><p>对于所有变量，默认状态下会跟踪计算并用来求梯度，所以不用使用tape.watch()<br>‘’’python<br>a = tf.Variable(a)<br>with tf.GradientTape() as tape:<br>    c = tf.sqrt(tf.square(a)+tf.square(b))<br>    dc_da = tape.gradient(c,a)<br>    print(dc_da)</p><p>tf.Tensor(<br>[[-0.53557533  0.87920487]<br> [ 0.24663754  0.4680054 ]], shape=(2, 2), dtype=float32)<br>‘’’<br>可以通过多开几个GradientTape来求高阶导数：<br>‘’’python<br>with tf.GradientTape() as outer_tape:<br>    with tf.GradientTape() as tape:<br>        c = tf.sqrt(tf.square(a)+tf.square(b))<br>        dc_da = tape.gradient(c,a)<br>    d2c_d2a = outer_tape.gradient(dc_da,a)<br>    print(d2c_d2a)</p><p>tf.Tensor(<br>[[0.54411626 0.33872807]<br> [1.5284648  0.5024241 ]], shape=(2, 2), dtype=float32)<br>‘’’</p></tf.tensor:>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;第一部分：Tensorflow基础&quot;&gt;&lt;a href=&quot;#第一部分：Tensorflow基础&quot; class=&quot;headerlink&quot; title=&quot;第一部分：Tensorflow基础&quot;&gt;&lt;/a&gt;第一部分：Tensorflow基础&lt;/h2&gt;&lt;h3 id=&quot;Tenso
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>二分法</title>
    <link href="http://yoursite.com/2020/01/01/%E4%BA%8C%E5%88%86%E6%B3%95/"/>
    <id>http://yoursite.com/2020/01/01/二分法/</id>
    <published>2019-12-31T16:12:16.000Z</published>
    <updated>2019-12-31T16:16:14.325Z</updated>
    
    <content type="html"><![CDATA[<p><em>2019年终记事：一年的经历那么多，回忆起来却短的不可想象。2019年的最后一篇博客，回想第一篇到现在，虽然一路上走走停停，但学习的脚步还在前进。短短二十年放弃的东西比一声叹息多多了，回头看来人生得要有一件能坚持下去的东西……新年的钟声带不走2019的遗憾，希望它能带来2020的希望。</em></p><hr><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><blockquote><p>在计算机科学中，二分搜索，也称为半间隔搜索，对数搜索，是一种搜索算法，用于查找排序数组中目标值的位置。二分搜索将目标值与数组的中间元素进行比较。<br>如果它们不相等，则消除目标不能位于其中的那一半，并在剩余的一半上继续搜索，再次将中间元素与目标值进行比较，并重复此过程直到找到目标值。<br>如果搜索以其余一半为空结束，则目标不在数组中。<br>在最坏的情况下，二分搜索会以对数时间运行，进行$O(\log_{}n)$比较，其中${n}$是数组中元素的数量，${O}$是Big O表示法， 而${\log}$是对数。 除了小数组以外，二分搜索比线性搜索快。 但是，必须首先对数组进行排序才能应用二进制搜索。有专门为快速搜索而设计的专用数据结构，例如哈希表，可以比二进制搜索更有效地进行搜索。 但是，二分搜索可用于解决更广泛的问题，例如，即使数组中不存在目标，也要在数组中找到相对于目标而言第二小的元素。<br>二进制搜索树和B树数据结构基于二分搜索。<br>——wiki</p></blockquote><p>在leetcode刷题时我们总能遇到可以利用二分搜索解决的问题，但往往我们写得出来的二分搜索代码并不能work或者潜藏有bug。造成这种现象的原因是纷杂的情况导致算法的细节处理不同而容易忽视边界的细节问题。我们需要一个简洁明了的思路来将问题一般化。矛盾的特殊性应该包含在矛盾的一般性当中，用高度抽象化的过程对具体问题降维打击。</p><p>编写博客时翻阅了不少leetcode中的二分题解，在此借鉴并总结一下。在完成的过程中翻阅了《计算机程序设计艺术》的相关章节，受益颇多，一部分内容会穿插在文中讲述。</p><h2 id="入题"><a href="#入题" class="headerlink" title="入题"></a>入题</h2><p><strong>一些背景——</strong></p><p>《计算机程序设计艺术》的作者 Donald Knuth：</p><blockquote><p>Although the basic idea of binary search is comparatively straightforward, the details can be surprisingly tricky …</p></blockquote><p>译：“虽然二分搜索的基本思想很直白，但细节出奇的难以应对…”</p><p>他在《计算机程序设计艺术（第三卷）》中也指出了（大意）:也许是第一部出版的关于非数值程序设计方法的书（1946）中，首先提出了二分查找。再到后来的许多人对二分算法的改进直至60年代往后，所有工作才算是基本完成。</p><p><strong>一些待解决的问题——</strong></p><ol><li>中位数索引值的获取</li><li>循环执行条件的设置与返回值的选择</li><li>分支语句的选择</li></ol><p><strong>关于模板——</strong></p><p>这里借助Thomas H. Cormen在《算法基础—打开算法之门》中的一段伪代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">程序   BINARY-SEARCH(A,n,x)</span><br><span class="line"></span><br><span class="line">输入：</span><br><span class="line">·A：一个数组；</span><br><span class="line">·n：要查找的数组A中元素的个数；</span><br><span class="line">·x：要查找的值；</span><br><span class="line"></span><br><span class="line">输出：要么是满足A[i]=x的索引i，要么是一个特殊值 NOT-FOUND（可取相对数组A的任何无效索引值，例如0或任意负整数）。</span><br><span class="line"></span><br><span class="line">//左右索性记为p,r；中间位置记为q</span><br><span class="line"></span><br><span class="line">1. 将p赋值为1，将r赋值为n。</span><br><span class="line">2. 只要p≤r，执行如下操作：</span><br><span class="line">        A. 将q赋值为⌊(p+r)/2⌋。</span><br><span class="line">        B. 如果A[q]=x，那么返回q。</span><br><span class="line">        C. 否则（A[q]≠x），如果A[q]&gt;x，那么将r赋值为q-1。</span><br><span class="line">        D. 否则（A[q]&lt;x），那么将p赋值为q+1。</span><br><span class="line">3. 返回 NOT-FOUND。</span><br></pre></td></tr></table></figure></p><p>这段算法描述是我们常规的理解和处理方法，这次打算讨论的是在它的基础上进行一定的改变，并简单谈谈优劣。</p><p><strong>基本思想——</strong></p><p>二分的基本思想其实就是每次可以将当前的数中将近一般的不满足要求的数全部除掉，所以大O时间复杂度是$O(\log_{2}n)$,可以达到对数级的复杂度。</p><p>为了真正了解二分搜索算法中所发生的事情，最好把它想象成一棵二叉搜索树。</p><p>关于平均比较次数暂且不论。</p><h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><p><strong>1. 中位数索引值</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> mid=(left+right)/<span class="number">2</span>;</span><br></pre></td></tr></table></figure><p>上面的代码是我们经常写的，但它确实存在有bug，在于当left与right都很大的时候，left+right很有可能超过int类型能表示的最大值（32位机下为$2^{31}-1$，即2147483647），此时会产生整形溢出产生负值，为了避免此问题的发生一般会写作下面的形式：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> mid=left+(right-left)/<span class="number">2</span>;</span><br></pre></td></tr></table></figure></p><p>可以有效避免绝大多数会产生溢出的情况。但它并不是最完美的写法。我们知道对于二进制计算机来说，除法的计算并不容易完成，二进制的位运算应该是最简单快捷的。当面对需要以二作为除数的时候，选择&gt;&gt;1，会带来更好的效果。但仅仅只是修改这一点，mid的计算仍旧存在冗余的部分。</p><p>我们从原码的角度思考，其实不难发现，因为第一位用来表示正负符号的缘故，当发4个字节下可以表示的最大正整数比int最大值两倍还多。当发生整形溢出时，实际存储的二进制只不过因为最高位从0变为1，导致整形正数被认为是整形负数，无法正确表示我们目标的数值，但其实内部存储的数据如果按无符号整形（即最高位具有二进制权值，而非表示符号）来输出其实是我们需要的值，此时再对这个数进行<font color="#FF0000">无符号的&gt;&gt;1</font>操作得到我们需要的除二结果。</p><p>关于无符号右移，在java中使用&gt;&gt;&gt;，所以java代码如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> mid = (right + left) &gt;&gt;&gt; <span class="number">1</span> ;</span><br></pre></td></tr></table></figure></p><p>而在c语言中，仅有的&gt;&gt;运算符却是在有符号位时产生的是有符号的右移，所以运算前需要将运算数值强制类型转换为无符号型，再右移。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> mid=((<span class="keyword">unsigned</span> <span class="keyword">int</span>)(left+right))&gt;&gt;<span class="number">1</span>;</span><br></pre></td></tr></table></figure></p><p>不进行强制类型转换的话，会把溢出产生的负数值除二。具体大家看下面的例子：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> a=<span class="number">2147483647</span>,b=<span class="number">2147483645</span>;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"%d"</span>,(a+b)&gt;&gt;<span class="number">1</span>);<span class="comment">//结果输出-2</span></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"%u"</span>,((<span class="keyword">unsigned</span> <span class="keyword">int</span>)(a+b))&gt;&gt;<span class="number">1</span>);<span class="comment">//结果输出2147483646</span></span><br></pre></td></tr></table></figure></p><p><strong>2. 算法设计</strong></p><p>这里将之前提出的关于“循环执行条件的设置与返回值的选择”以及“分支选择”问题归结在一起，我们其实是从算法设计的角度来考虑的。</p><p>首先如上面的伪代码，我们循环的条件设置的为left≤right，出循环时的情况是left已经越过right，表示全部的元素都被搜索完。我们的修改是将循环条件改为left &lt; right,此时出循环的情况为left与right相等，表示整个数组只有现在唯一一个元素没有被搜索，也就是下标为left（right）的元素。根据不同的情形，对最后一个元素进行判断。比如说，我们知道这个数组一定会有我们搜索的值时，此时不用判断就可以知道当其他元素都被排除时，最后这个就是我们要找的元素。如果我们不确定是否一定存在，那么在循环外部额外判断此时最后的元素是否满足条件即可。</p><p>问题的关键在于这样的设计意义是什么，因为当我们用之前的方法，我们在每一次都需要判断是否中值是我们需要的值，但是根据统计规律要查找的数据一般情况下出现在中间的情况并不多。而且出循环时表示没有找到，举个例子：某些情况下我们会想把没有找到的元素添加进我们所查找的表中时，我们想先返回插入的位置，此时我们需要考虑返回left或者right。而每次遇到不同的问题我们需要返回的值都需要根据不同的情况选择，这样的设计并不完美。我们对于任意情况下都可以统一处理是理想状态，这样这段代码的复用性也会变高。</p><p>关于死循环，死循环容易发生在只有 2 个元素时，我需要慎重选择中位数，一定要确保：</p><ol><li>如果分支的逻辑，在选择左边界的时候，不能排除中位数，那么中位数就选“右中位数”，只有这样区间才会收缩，否则进入死循环。</li><li>同理，如果分支的逻辑，在选择右边界的时候，不能排除中位数，那么中位数就选“左中位数”，只有这样区间才会收缩，否则进入死循环。</li></ol><p>在区间中的元素只剩下 2 个时候，例如：left = 3，right = 4。此时左中位数就是左边界，如果你的逻辑执行到 left = mid 这个分支，且你选择的中位数是左中位数，此时左边界就不会得到更新，区间就不会再收缩（理解这句话是关键），从而进入死循环；<br>为了避免出现死循环，你需要选择中位数是右中位数，当逻辑执行到 left = mid 这个分支的时候，因为你选择了右中位数，让逻辑可以转而执行到 right = mid - 1 让区间收缩，最终成为 1 个数，退出 while 循环。</p><h1 id="总结与延申"><a href="#总结与延申" class="headerlink" title="总结与延申"></a>总结与延申</h1><p>其实Donald Knuth在著作中也指出了</p><blockquote><p>H.Bottenbruch迈出了二分算法的第二步，介绍了有趣的变形，避免了在最后结束之前单作一次相等判断：在步骤B2中利用$i⬅⌈(l+u)/2⌉$代替$⌊(l+u)/2⌋$，每当 $K≥K_i$时置$l⬅i$；然后在每一步中$u-l$都减值。最后，当$l=u$时，我们有$K_l≤K≤K_{l+1}$，而且再做一次比较即可判断这个查找是否成功（假定开始时$K≥K_l$）。这个思想子啊许多计算机上稍微加速了内循环，而且同样的原理可以在这一节讨论的所有算法使用，但由于此前推导出的一次成功的平均查找次数于一次不成功的平均查找次数之间的关系一次成功的查找平均将要求大约再做一次迭代。由于内循环仅执行$lgN$次，因此在这一次额外的迭代与一次更快的循环之间的折衷并不节省时间，除非N非常大。在含有重复码值时，获取该算法给定码值最右出现，这一性质有时很重要。<br><em>大师的思想与见识比我们深远的多，多多读书无疑是一场对于世界和自我的再发掘。</em></p></blockquote><p>在上述描述中，我们不难看出来其实整个算法的核心就是在靠左右边界夹逼，而我们做出的改变其实再更深刻的讨论中，在不涉及大数量的情况下，对时间的影响并不明显。但是这种算法仍然有可用性，就如我在之前分析的一样。</p><p>最后的一点延申，我们的二分使用了left mid right三个指针，而仅使用如下两个量，记录当前位置i,和它的变化速度δ；每次不相等的比较之后，我们可以置i⬅1±δ和δ⬅δ/2（近似地）。这是可以的，但要对细节极端小心才行，简化的解决方法注定引起失误！这个算法实现后，产生的一颗“均匀的”二分查找树，我们可以观察它得到更好的算法。</p><p>而且另外一点我们在整个过程的观察中，其实可以发现斐波那契数列可以起到和2的乘方相似的作用。斐波那契搜索就是在二分查找的基础上根据斐波那契数列进行分割的。在斐波那契数列找一个等于略大于查找表中元素个数的数F[n]，将原查找表扩展为长度为F[n]<br>(如果要补充元素，则补充重复最后一个元素，直到满足F[n]个元素)，完成后进行斐波那契分割，即F[n]个元素分割为前半部分F[n-1]个元素，后半部分F[n-2]个元素，找出要查找的元素在那一部分并递归，直到找到。斐波那契查找，是区间中单峰函数的搜索技术。乍看之下，似乎有些神秘，就如Donald Knuth所说，如果我们简单地把程序拿出来并试图看看它在干什么活，它似乎在变魔术，但只要把查找树画出来，神秘感就消失了。<br>这次地介绍就到此结束了。</p><p><em><font color="#FF0000" size="0">关于平均比较数和均匀二分查找等详细内容，本文暂时未涉及，后续可能补充，有兴趣的读者可以自行了解。</font></em></p><hr><p align="right"><font face="Segoe Script">Attempt the end;and never stand to doubt;<br>Nothing's so hard,but search will find it out.</font></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;em&gt;2019年终记事：一年的经历那么多，回忆起来却短的不可想象。2019年的最后一篇博客，回想第一篇到现在，虽然一路上走走停停，但学习的脚步还在前进。短短二十年放弃的东西比一声叹息多多了，回头看来人生得要有一件能坚持下去的东西……新年的钟声带不走2019的遗憾，希望它能
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Excel模块简析</title>
    <link href="http://yoursite.com/2019/06/02/Excel%E6%A8%A1%E5%9D%97%E7%AE%80%E6%9E%90/"/>
    <id>http://yoursite.com/2019/06/02/Excel模块简析/</id>
    <published>2019-06-02T09:30:47.000Z</published>
    <updated>2019-08-07T07:08:09.971Z</updated>
    
    <content type="html"><![CDATA[<hr><p>Python中处理Excel文件主要依靠xlrd xlwt和pandas库的模块进行数据处理。这里主要介绍xlrd和xlwt扩展包。</p><hr><p>xlrd 是一个库, 用于读取 Excel 文件中的数据和格式信息, 无论它们是. xls 还是. xlsx 文件。<br>对于包含文本的数据处理我们需要注意的是扩展包所支持的编码方案，此包将所有文本字符串显示为 Python unicode 对象。<br>我们通常会接触到的Exel文件中日期的存储方式与一般数据有格式上的区别。但是在Excel文件中，日期不存储为单独的数据类型;它们存储为浮点数。这个浮点数代表从1990年1月0日，或者说1899年12月31日开始经过的日期数加上一个24小时的小数部分。（注：在Mac上日期是从1994年1月1日开始）这对我们处理这些数据产生了预期之外的困难，原因在于我们不容易分辨一个浮点数是否表示日期数据。对于本模块，它通过检查已应用于每个数字单元格的格式来帮助我们，它可以检测到单元格数据的格式，从而判定单元格中的值是否是日期。但我们处理这些数据时，仍要格式化进行。往往我们的处理方式如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> worksheet.cell_type(row_index, col_index) == <span class="number">3</span>:</span><br><span class="line"></span><br><span class="line">date_cell = xldate_as_tuple(worksheet.cell_value(row_index,col_index),workbook.datemode)</span><br><span class="line">date_cell = date(*date_cell[<span class="number">0</span>:<span class="number">3</span>]).strftime(<span class="string">'%m/%d/%Y'</span>)</span><br></pre></td></tr></table></figure></p><p>简单解释一下，在xlrd的文档中我们可以知道，日期型数据的单元格类型为3。这里使用了worksheet对象的cell_value函数和行列索引来获得单元格中的值。这个值作为xldate_as_tuple()函数的第一个参数，参数workbook.datemode，可以使函数确定日期是基于1990年还是1994年，返回一个元组。strftime将data对象转化为格式化字符串。<br>基础部分我就直接列在下方：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xlrd.open_workbook(filename=<span class="keyword">None</span>, logfile=&lt;_io.TextIOWrapper name=<span class="string">'&lt;stdout&gt;'</span> mode=<span class="string">'w'</span> encoding=<span class="string">'UTF-8'</span>&gt;, verbosity=<span class="number">0</span>, use_mmap=<span class="number">1</span>, file_contents=<span class="keyword">None</span>, encoding_override=<span class="keyword">None</span>, formatting_info=<span class="keyword">False</span>, on_demand=<span class="keyword">False</span>, ragged_rows=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure></p><p><em>filename– 要打开的电子表格文件的路径。</em>logfile – 一个打开的文件, 将消息和诊断写入其中。<br><em>verbosity – 增加写入日志文件的跟踪材料的数量。</em>use_mmap – 是否使用 mmap 模块是试探性确定的。使用此参数覆盖结果。<br>目前的试探: mmap (如果存在) 将使用它。<br>file_contents – 一个字符串或者一个mmap.mmap对象或者其他类似行为的对象。 如果file_contents提供, filename将不会被使用, except (possibly)但 (可能) 在消息中除外。<br>encoding_override – 用于克服旧版本文件中丢失或错误的代码页信息。<br>formatting_info –默认值为False, 它节省内存。 在这种情况下, “空白” 单元格 (具有自己的格式信息但没有数据) 通过忽略文件的BLANK和MULBLANK记录被视为空。 这将切断空单元格或空白单元格行的任何底部或右侧 “边距”。<br>当为True,格式信息将从电子表格文件中读取。这将提供所有单元格, 包括空单元格和空白单元格。格式信息可用于每个单元格。<br>请注意, 当与 xlsx 文件一起使用时，这将引发注意实现错误,。<br>on_demand –控制工作表最初是否全部加载, 或者在调用方要求时加载。<br>ragged_rows –<br>默认值False表示所有行都用空单元格填充, 以便所有行的大小与 ncols中的大小相同。<br>True表示行的末尾没有空单元格。如果行的大小千差万别, 这可能会节省大量内存。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;p&gt;Python中处理Excel文件主要依靠xlrd xlwt和pandas库的模块进行数据处理。这里主要介绍xlrd和xlwt扩展包。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;xlrd 是一个库, 用于读取 Excel 文件中的数据和格式信息, 无论它们是. xls 还是. xls
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>sqlite3-SQLite数据库模块简单认识</title>
    <link href="http://yoursite.com/2019/05/26/sqlite3-SQLite%E6%95%B0%E6%8D%AE%E5%BA%93%E6%A8%A1%E5%9D%97%E7%AE%80%E5%8D%95%E8%AE%A4%E8%AF%86/"/>
    <id>http://yoursite.com/2019/05/26/sqlite3-SQLite数据库模块简单认识/</id>
    <published>2019-05-26T11:35:07.000Z</published>
    <updated>2019-08-07T07:08:47.163Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>SQLite 是一个C语言库，它可以提供一种轻量级的基于磁盘的数据库，这种数据库不需要独立的服务器进程，也允许需要使用一种非标准的 SQL 查询语言来访问它。一些应用程序可以使用 SQLite 作为内部数据存储。可以用它来创建一个应用程序原型，然后再迁移到更大的数据库，比如 PostgreSQL 或 Oracle。</em></strong><br>要使用这个模块，必须先创建一个 Connection 对象，它代表数据库。下面例子中，数据将存储在 example.db 文件中：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import sqlite3</span><br><span class="line">conn = sqlite3.connect(&apos;example.db&apos;)</span><br></pre></td></tr></table></figure></p><p>你也可以使用 :memory: 来创建一个内存中的数据库</p><p>当有了 Connection 对象后，你可以创建一个 Cursor 游标对象，然后调用它的 execute() 方法来执行 SQL 语句：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">c = conn.cursor()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建表</span></span><br><span class="line">c.execute(<span class="string">'''CREATE TABLE stocks</span></span><br><span class="line"><span class="string">             (date text, trans text, symbol text, qty real, price real)'''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 插入一行数据</span></span><br><span class="line">c.execute(<span class="string">"INSERT INTO stocks VALUES ('2006-01-05','BUY','RHAT',100,35.14)"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存（提交）更改</span></span><br><span class="line">conn.commit()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们也可以在完成任务后，关闭连接</span></span><br><span class="line"><span class="comment"># 只需要保证在关闭之前任何更改都已提交</span></span><br><span class="line">conn.close()</span><br></pre></td></tr></table></figure></p><p>这些数据被持久化保存了，而且可以在之后的会话中使用它们：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sqlite3</span><br><span class="line">conn = sqlite3.connect(<span class="string">'example.db'</span>)</span><br><span class="line">c = conn.cursor()</span><br></pre></td></tr></table></figure></p><p>通常你的 SQL 操作需要使用一些 Python 变量的值。你不应该使用 Python 的字符串操作来创建你的查询语句，因为那样做不安全；它会使你的程序容易受到 SQL 注入攻击（在 <a href="https://xkcd.com/327/" target="_blank" rel="noopener">https://xkcd.com/327/</a> 上有一个搞笑的例子，看看有什么后果）</p><p>推荐另外一种方法：使用 DB-API 的参数替换。在你的 SQL 语句中，使用 ? 占位符来代替值，然后把对应的值组成的元组做为 execute() 方法的第二个参数。（其他数据库可能会使用不同的占位符，比如 %s 或者 :1）例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># Never do this -- insecure!</span><br><span class="line">symbol = &apos;RHAT&apos;</span><br><span class="line">c.execute(&quot;SELECT * FROM stocks WHERE symbol = &apos;%s&apos;&quot; % symbol)</span><br><span class="line"></span><br><span class="line"># Do this instead</span><br><span class="line">t = (&apos;RHAT&apos;,)</span><br><span class="line">c.execute(&apos;SELECT * FROM stocks WHERE symbol=?&apos;, t)</span><br><span class="line">print(c.fetchone())</span><br><span class="line"></span><br><span class="line"># Larger example that inserts many records at a time</span><br><span class="line">purchases = [(&apos;2006-03-28&apos;, &apos;BUY&apos;, &apos;IBM&apos;, 1000, 45.00),</span><br><span class="line">             (&apos;2006-04-05&apos;, &apos;BUY&apos;, &apos;MSFT&apos;, 1000, 72.00),</span><br><span class="line">             (&apos;2006-04-06&apos;, &apos;SELL&apos;, &apos;IBM&apos;, 500, 53.00),</span><br><span class="line">            ]</span><br><span class="line">c.executemany(&apos;INSERT INTO stocks VALUES (?,?,?,?,?)&apos;, purchases)</span><br></pre></td></tr></table></figure></p><p>要在执行 SELECT 语句后获取数据，你可以把游标作为 iterator，然后调用它的 fetchone() 方法来获取一条匹配的行，也可以调用 fetchall() 来得到包含多个匹配行的列表。</p><p>下面是一个使用迭代器形式的例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; for row in c.execute(&apos;SELECT * FROM stocks ORDER BY price&apos;):</span><br><span class="line">        print(row)</span><br><span class="line"></span><br><span class="line">(&apos;2006-01-05&apos;, &apos;BUY&apos;, &apos;RHAT&apos;, 100, 35.14)</span><br><span class="line">(&apos;2006-03-28&apos;, &apos;BUY&apos;, &apos;IBM&apos;, 1000, 45.0)</span><br><span class="line">(&apos;2006-04-06&apos;, &apos;SELL&apos;, &apos;IBM&apos;, 500, 53.0)</span><br><span class="line">(&apos;2006-04-05&apos;, &apos;BUY&apos;, &apos;MSFT&apos;, 1000, 72.0)</span><br></pre></td></tr></table></figure></p><hr><p>摘自Python官方文档</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;&lt;em&gt;SQLite 是一个C语言库，它可以提供一种轻量级的基于磁盘的数据库，这种数据库不需要独立的服务器进程，也允许需要使用一种非标准的 SQL 查询语言来访问它。一些应用程序可以使用 SQLite 作为内部数据存储。可以用它来创建一个应用程序原型，然后再
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>浮点的限制与争议</title>
    <link href="http://yoursite.com/2019/05/19/%E6%B5%AE%E7%82%B9%E7%9A%84%E9%99%90%E5%88%B6%E4%B8%8E%E4%BA%89%E8%AE%AE/"/>
    <id>http://yoursite.com/2019/05/19/浮点的限制与争议/</id>
    <published>2019-05-19T04:22:38.000Z</published>
    <updated>2019-05-19T05:21:25.407Z</updated>
    
    <content type="html"><![CDATA[<p>浮点算术在计算机领域的广泛应用不必多说。应该大多数人都有这样的经历，总是在不经意间会遇到浮点运算的一些不满足预期的奇怪输出。这里主要来探讨一下个中奥秘。<br>首先，我们都明白，大多数十进制小数都不能精确地表示为二进制小数，这导致在大多数情况下，你输入的十进制浮点数都只能近似地以二进制浮点数形式储存在计算机中。用十进制来理解这个问题显得更加容易一些。考虑分数$1/3$。我们可以得到它在十进制下的一个近似值$0.3$，或者更接近$0.33$，或者更接近的$0.333……$无论你写下多少，永远只能更接近。同样，我们用$2$作为基数，$0.1$是永远也无法精确表示为二进制的小数。在二进制下，$1/10$是一个无限循环小数$0.0001100110011001100110011001100110011001100110011…$在任何一个位置停下都仅仅只是近似值。因此，在今天的大部分架构上，浮点数都只能近似地使用二进制小数表示，对应分数的分子使用每8字节的前53位表示，分母则表示为$2$的幂次(IEEE754)。在$1/10$这个例子中，相应的二进制分数是 $3602879701896397/2^{55}$ ，它很接近$1/10$，但并不是$1/10$。往往我们能观察到的输出结果精度达不到近似值有差异的地方。记住了，即使输出的结果看起来好像就是$1/10$的精确值，实际储存的值只是最接近$1/10$的计算机可表示的二进制分数。有趣的是，有许多不同的十进制数共享相同的最接近的近似二进制小数。例如，$0.1$、$0.10000000000000001$、$0.1000000000000000055511151231257827021181583404541015625$全都近似于$3602879701896397/2^{55}$。由于所有这些十进制值都具有相同的近似值，因此可以显示其中任何一个。<br>出现浮点运算不符合预期值的情况，其本质不是语言的限制，也不是代码的问题，你在所有支持在硬件中浮点运算的语言都会发现相同的情况。<br>有时我们想要更美观的输出，我们可以对小数进行格式化输出，但其实这并不影响浮点数在计算机里的存储值，只是见舍入后的结果进行显示而已。我们用Python举例。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; format(math.pi,&apos;.12g&apos;)</span><br><span class="line">&apos;3.14159265359&apos;</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; format(math.pi,&apos;.2f&apos;)</span><br><span class="line">&apos;3.14&apos;</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; format(math.pi)</span><br><span class="line">&apos;3.141592653589793&apos;</span><br></pre></td></tr></table></figure></p><p>我们看到的输出结果只是表象而已，不理解内部的运算会在应用过程中产生很多问题。比如下面的例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; .1+.1+.1==.3</span><br><span class="line">Flase</span><br></pre></td></tr></table></figure></p><p>我们无法将$2$为基数的$0.1$精确到十进制的$1/10$，对应的$0.3$同样无法精确。哪怕我们使用Python的$round()$函数预先舍入也无济于事。不过预后舍入就可以达到我们想要的效果。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; round(.1,1)+round(.1,1)+round(.1,1)==round(.3,1)</span><br><span class="line">False</span><br><span class="line">&gt;&gt;&gt;round(.1+.1+.1,10)==round(.3,10)</span><br><span class="line">True</span><br></pre></td></tr></table></figure></p><p>浮点数运算会造成许多这样的“意外”。不过大多数情况下一点误差是可以容忍的。需要注意的是，每次浮点运算都可能导致新的舍入错误。<br>对于需要精确十进制表示的使用场景，请尝试使用$decimal$模块，该模块实现了适合会计应用和高精度应用的十进制运算。<br>另一种形式的精确运算由$fractions$模块提供支持，该模块实现了基于有理数的算术运算（因此可以精确表示像$1/3$这样的数值）。<br>如果你是浮点运算的重度用户，你应该看一下数值运算Python包NumPy以及由SciPy项目所提供的许多其它数学和统计运算包。参见<a href="https://scipy.org" target="_blank" rel="noopener">https://scipy.org</a>。<br>Python也提供了一些工具，可以在你真的想要知道一个浮点数精确值的少数情况下提供帮助。例如float.as_integer_ratio()方法会将浮点数表示为一个分数:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x=3.14159</span><br><span class="line">&gt;&gt;&gt; x.as_integer_ratio()</span><br><span class="line">(3537115888337719, 1125899906842624)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; x==3537115888337719/1125899906842624</span><br><span class="line">True</span><br></pre></td></tr></table></figure></p><p>这样的表示法是精确的，可以跨版本移植，和别的标准相同的语言（Java、C99）交换数据。</p><hr><p>这里引出问题所在和基本的解决措施。下一篇将深入本质探讨此问题，包括IEEE754的相关具体细节。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;浮点算术在计算机领域的广泛应用不必多说。应该大多数人都有这样的经历，总是在不经意间会遇到浮点运算的一些不满足预期的奇怪输出。这里主要来探讨一下个中奥秘。&lt;br&gt;首先，我们都明白，大多数十进制小数都不能精确地表示为二进制小数，这导致在大多数情况下，你输入的十进制浮点数都只能近
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>算法题：爬楼梯</title>
    <link href="http://yoursite.com/2019/05/10/%E7%AE%97%E6%B3%95%E9%A2%98%EF%BC%9A%E7%88%AC%E6%A5%BC%E6%A2%AF/"/>
    <id>http://yoursite.com/2019/05/10/算法题：爬楼梯/</id>
    <published>2019-05-09T18:29:07.000Z</published>
    <updated>2019-08-07T10:00:04.697Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>题目：假设你正在爬楼梯，需要n阶你才能到达楼顶。每次你可以爬1阶或2阶台阶。你有多少种不同的方法可以爬到楼顶呢？</em></strong></p><hr><p>这里将对此问题写出多个解法，包括一般容易实现的递归，以及它的一步一步优化。</p><hr><h2 id="贪心算法"><a href="#贪心算法" class="headerlink" title="贪心算法"></a>贪心算法</h2><p>这是我看到题目后最先想到的也是十分麻烦的方法，但我觉得有必要拿出来看看。思路是，如果爬楼梯每次都选择同一种，即要么全走两阶大致（n的奇偶决定，暂且定为偶数便于理解）一共n/2次到达楼顶，要么全走一阶一共n次就可以到达楼顶。选取其中一种状况进行分析即可，这两种都是终极状态，我需要做的是对两者的相互演化过程做出模拟即可。<br>举个例子，这里优先考虑把选择次数降到最低，每次均走2阶。然后把一个两阶拆成两个1阶，每次拆分选择次数就增加1，再对这一共n/2+1次选择进行排列组合得到一定的方法数，即绝大多数都是走2阶，选择一下在哪几次走一阶，不同的选择是不同的方法。然后再将一个两阶拆分，继续上述操作，直至把所有两阶全部拆分为1阶。这时统计所有方法数即可。这里的代码是上述例子的实现，因为涉及到排列组合，组合数用阶乘计算方便直接，所以又定义了两个子函数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">int PermuteCombine(int WayNum,int StairsNum)//全部先尽可能多的转化为一次2个台阶的方法，，再将一次2个台阶拆解为两个一次台阶，在在总次数中排列组合。 </span><br><span class="line">&#123;</span><br><span class="line">int i;</span><br><span class="line">if(StairsNum%2==0)//总阶数进行奇偶判断 ，处理大同小异，均从顶向下、以迭代的方法将问题 扩大为一个规模稍大的问题 </span><br><span class="line">for(i=0;i&lt;=StairsNum/2;i++)</span><br><span class="line">WayNum+=C(StairsNum/2+i,StairsNum/2-i);</span><br><span class="line">else</span><br><span class="line">for(i=0;i&lt;(StairsNum+1)/2;i++)</span><br><span class="line">WayNum+=C((StairsNum+1)/2+i,(StairsNum-1)/2-i); </span><br><span class="line">return WayNum;</span><br><span class="line">&#125;</span><br><span class="line">int factorial(int n)//阶乘函数 n！ </span><br><span class="line">&#123;</span><br><span class="line">if(n==1)return 1;//递归出口 </span><br><span class="line">return n*factorial(n-1);</span><br><span class="line">&#125;</span><br><span class="line">int C(int n,int m)//利用数学定义计算组合数 ,n个不同元素中取出m个元素的组合数。用符号 C(n,m) 表示</span><br><span class="line">&#123;</span><br><span class="line">if(m==0||n==m)//特殊情况处理 </span><br><span class="line">return 1;</span><br><span class="line">else</span><br><span class="line">return (factorial(n)/(factorial(n-m)*factorial(m)));//计算公式C(n,m)=n!/[m!*(n-m)!]将结果返回 </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>不难看出，这里的处理其实是不断地将问题规模扩大，而如果先假定全走1阶，则整个程序体现出来，应该是越来越多的两个一组的1阶被一个二阶取代，问题规模不断变小。其实本质是一样的。</p><blockquote><p>贪心算法（英语：greedy algorithm），又称贪婪算法，是一种在每一步选择中都采取在当前状态下最好或最优（即最有利）的选择，从而希望导致结果是最好或最优的算法。不从整体最优上加以考虑，它所做出的是在某种意义上的局部最优解。</p></blockquote><p>这里的方法其实也不是特别符合贪婪的定义，但从某种处理方式上来看具有一定的相似性。贪心算法简单来说两个基本要素，一是贪心选择，而是最优子结构。每做一次贪心选择就将所求问题简化为一个规模更小的子问题。先全走1阶，后一步一步合并成2阶，就是这样一个过程。当一个问题的最优解包含其子问题的最优解时，称此问题具有最优子结构性质。这里与动态规划十分接近，可以推测此题应该也会有动态规划的解法。<br>    至于贪心的局限在于它在达成整体最优的路上是不可回退的。即在当前选择时，是无法返回上一次选择的状态。</p><h2 id="树形递归模拟爬楼梯"><a href="#树形递归模拟爬楼梯" class="headerlink" title="树形递归模拟爬楼梯"></a>树形递归模拟爬楼梯</h2><p>上一个算法不得不说写的是又烂又长，看的让人头大。这里要介绍的方法理解起来倒是简单多了。看到小标题，相信大家心里已经知道是什么样的了，不多说了，上代码。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">int recursion(int NowNum,int StairsNum)//从0阶台阶开始模拟爬楼梯，构建一个递归树 ，每一层都有两个分支选择，+1或+2。 </span><br><span class="line">&#123;//两个递归出口 </span><br><span class="line">if(NowNum&gt;StairsNum)//上一阶加2后超出总阶数 则此方法无效不计数 </span><br><span class="line">return 0;</span><br><span class="line">if(NowNum==StairsNum)//刚好满足总阶数   则此方法（在递归树中的分支路径）有效记一次 </span><br><span class="line">return 1;</span><br><span class="line">return recursion(NowNum+1,StairsNum)+recursion(NowNum+2,StairsNum);// 将当前方法（路径）继续深化到下一层，直到返回 。上一层方法是下层方法的总和，每层都有两个分支。 </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>树形递归，时间复杂度$O(2^n)$，空间树大小$2^n$。</p><h2 id="记忆化递归"><a href="#记忆化递归" class="headerlink" title="记忆化递归"></a>记忆化递归</h2><p>上一个方法倒是简单易懂，但不可避免的浪费了许多空间和时间。其实如果你仔细画过或者模拟过上面的递归树，不难发现一件事，我们有很多重复计算。没有模拟过也不要紧，我们现在大致演示一下。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">                   0</span><br><span class="line">  _________|__________</span><br><span class="line"> |                    |</span><br><span class="line"> 1       2</span><br><span class="line">    _____|_____          _____|_____</span><br><span class="line">   |           |        |           |</span><br><span class="line">   2           3        3           4</span><br><span class="line"> __|__       __|__    __|__       __|__</span><br><span class="line">|     |     |     |  |     |     |     |</span><br><span class="line">3     4     4     5  4     5     5     6</span><br><span class="line">......</span><br></pre></td></tr></table></figure></p><p>仔细观察会发现在这个树中，有许多相似的结构分支，尤其是从零开始的分支2，直接包含在分支1中，也就是说，我们深度优先遍历递归树的时候，其实有多于一半的时间和大量空间的浪费都是不必要的。我们只需要用一个数组记录每次的值，当需要递归去计算某个分支时，如果已经计算过则直接从数组中获取值即可，不必再次遍历子树。代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">//在主函数中要先定义记忆数组，并初始化零。</span><br><span class="line">int memo[StairsNum+1],i;</span><br><span class="line">for(i=0;i&lt;StairsNum+1;i++)//变量定义数组长度后无法直接初始化，定义完后用循环初始化数组。 </span><br><span class="line">memo[i]=0;</span><br><span class="line"></span><br><span class="line">int MemoRecur(int NowNum,int StairsNum,int memo[])// 之前的递归过程有大量的重复计算，改进后把每一步结果都储存下来，以便需要的时候直接使用就不用再次进入递归计算，减少时间，定义数组会占用n大小的空间 </span><br><span class="line">&#123;</span><br><span class="line">if(NowNum&gt;StairsNum)</span><br><span class="line">return 0;</span><br><span class="line">if(NowNum==StairsNum)</span><br><span class="line">return 1;</span><br><span class="line">if(memo[NowNum]&gt;0)</span><br><span class="line">return memo[NowNum];</span><br><span class="line">memo[NowNum]=MemoRecur(NowNum+1,StairsNum,memo)+MemoRecur(NowNum+2,StairsNum,memo);</span><br><span class="line">return memo[NowNum]; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>典型的浪费空间换取时间的策略。毫无疑问我们将上一次的算法进行了优化。</p><h2 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h2><p>之前的猜测可以证实了，这里确实可以用动态规划，尤其是在上一个优化中，我们发现记忆数组其实帮我完成了一件事——抽象描述就是：使得我们可以在选择时回退到之前选择过的情形而直接获取答案。至于这里使用动态规划的考虑是这样的：如第i阶可由(i-1)阶爬1阶或(i-2)阶爬两阶得到，规划到最初，第三阶由第二阶爬1阶或第1阶爬两阶得到，第四阶是三阶爬1或2阶爬2…类推 。代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">int DynamicProgram(int StairsNum)</span><br><span class="line">&#123;</span><br><span class="line">int dp[StairsNum+1],i;</span><br><span class="line">dp[1]=1,dp[2]=2;</span><br><span class="line">for(i=3;i&lt;StairsNum+1;i++)</span><br><span class="line">dp[i]=dp[i-1]+dp[i-2];</span><br><span class="line">return dp[StairsNum];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>一层循环就结束整个问题，相应，降低了时间，但是用了数组就浪费了空间。</p><h2 id="斐波那契数列"><a href="#斐波那契数列" class="headerlink" title="斐波那契数列"></a>斐波那契数列</h2><p>其实大家应该在之前的某种方法就已经的发现，这道题在数学上就是求斐波那契数列的第N项的值。那么如何求这个数列就变得重要了。也许有人会用递归算，这不免和之前的我们优化过的算法陷入同样的情况，子树会重复，再一步一步优化就到了用一遍循环就算出值。其实对这个数列有所了解的同学应该知道，这个数列是有通项公式的。如下<br>$a_n=\frac{1}{\sqrt 5}[(\frac{1+\sqrt 5}{2})^n-(\frac{1-\sqrt 5}{2})^n]$<br>我们将值代入即可求出对于的通项值，代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">int Fib(int StairsNum)//直接使用公式,空间O(1),由于使用POW函数，时间变为O(log n) </span><br><span class="line">&#123;</span><br><span class="line">double s5=sqrt(5);</span><br><span class="line">return(int)(1/s5*(pow((1+s5)/2,StairsNum+1)-pow((1-s5)/2,StairsNum+1)));//可能由于计算精度问题，极少部分答案与准确值相差1， </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>由于计算次方我们调用了pow()函数，需要导入math.h，无形中我们在计算公式时，还是会消耗一定时间。这时我们的空间复杂度$O(1)$，调用函数缘故，时间复杂度：$O(\log_{}n)$</p><h2 id="矩阵快速幂"><a href="#矩阵快速幂" class="headerlink" title="矩阵快速幂"></a>矩阵快速幂</h2><p>借助公式的方法其实不是那么完美，因为有时会有情况限制我们对于函数的使用，那有没有类似的方法可以快速计算出斐波那契数列呢？这里介绍矩阵快速幂。要能使用这方法，我们需要先知道如何用矩阵表示斐波那契数列。<br>我们引入矩阵乘法进行推导：<br>$\left[<br> \begin{matrix}<br>   F_n\\<br>   F_{n-1}<br>  \end{matrix}<br>\right]=<br>\left[<br> \begin{matrix}<br>   F_n+F_{n-2}\\<br>   F_{n-1}<br>  \end{matrix}<br>\right]=<br>\left[<br> \begin{matrix}<br>   1&amp;1\\<br>   1&amp;0<br>  \end{matrix}<br>\right]\cdot<br>\left[<br> \begin{matrix}<br>   F_{n-1}\\<br>   F_{n-2}<br>  \end{matrix}<br>\right]$<br>然后化简：<br>$\left[<br> \begin{matrix}<br>   F_n\\<br>   F_{n-1}<br>  \end{matrix}<br>\right]=<br>\left[<br> \begin{matrix}<br>   1&amp;1\\<br>   1&amp;0<br>  \end{matrix}<br>\right]^{n-1}\cdot<br>\left[<br> \begin{matrix}<br>   1\\<br>   0<br>  \end{matrix}<br>\right]$<br>此时我们知道，想要得出含有斐波那契数列的矩阵，只需要对一个矩阵求幂即可，那么如何能快速求出高次幂变成我们关注的重点。<br>首先由整数快速幂引入。当我们想求$x^8$时，我不必将x自乘8次，而是可以转化为求$x^2$三次，主要是这种结合思想。那用怎样的策略才能把高次拆分成计算最快的低次乘积？我们用$x^{25}$举例子。<br>25的二进制是11001,则$x^{25}=x^{16}\cdot x^8\cdot x^1$，具体代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">int QuickPow(int x,int N)</span><br><span class="line">&#123;</span><br><span class="line">    int res = x;</span><br><span class="line">    int ans = 1;</span><br><span class="line">    while(N)</span><br><span class="line">    &#123;</span><br><span class="line">        if(N&amp;1)//位与运算判断N最后一位是否为1</span><br><span class="line">        &#123;</span><br><span class="line">            ans = ans * res;</span><br><span class="line">        &#125;</span><br><span class="line">        res = res*res;</span><br><span class="line">        N = N&gt;&gt;1;//位右移，判断下一位</span><br><span class="line">    &#125;</span><br><span class="line">    return ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看出res与二进制位权值对应，而ans则负责将我们二进制位上存在1 的进行记录。当N=25时，最终res变成了$x^{32}$，ans变成了$x^{25}$。简单来说就是res一直跑，当遇到我们需要的ans时，在前一次的ans上再乘当前的res。快速幂的处理方法就是这样，这种情形下，我们只使用常量级空间$O(1)$，时间复杂度到了与使用函数一样的$O(\log_{}n)$。我们现在只需要把这种方法放到矩阵中去即可。代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">int QuickPower(int N)</span><br><span class="line">&#123;</span><br><span class="line">int n=2,i,j;</span><br><span class="line">N-=1;//</span><br><span class="line">int ans[2][2],res[2][2],tep[2][2];</span><br><span class="line">for(i=0;i&lt;n;i++)</span><br><span class="line">for(j=0;j&lt;n;j++)//初始化ans矩阵，变成单位矩阵E，进行矩阵乘法的基石 </span><br><span class="line">&#123;</span><br><span class="line">if(i==j)</span><br><span class="line">ans[i][j]=1;</span><br><span class="line">else</span><br><span class="line">ans[i][j]=0;</span><br><span class="line">&#125;</span><br><span class="line">for(i=0;i&lt;n;i++)//初始化res矩阵 </span><br><span class="line">for(j=0;j&lt;n;j++)</span><br><span class="line">&#123;</span><br><span class="line">if(i&amp;&amp;j)</span><br><span class="line">res[i][j]=0;</span><br><span class="line">else</span><br><span class="line">res[i][j]=1;</span><br><span class="line">&#125;</span><br><span class="line">while(N)//快速幂部分 </span><br><span class="line">&#123;</span><br><span class="line">if(N&amp;1)</span><br><span class="line">&#123;//ans=ans*res</span><br><span class="line">for(i=0;i&lt;n;i++)</span><br><span class="line">for(j=0;j&lt;n;j++)</span><br><span class="line">&#123;</span><br><span class="line">tep[i][j]=ans[0][j]*res[i][0]+ans[j][1]*res[1][i];</span><br><span class="line">&#125;</span><br><span class="line">for(i=0;i&lt;n;i++)</span><br><span class="line">for(j=0;j&lt;n;j++)</span><br><span class="line">ans[i][j]=tep[i][j];</span><br><span class="line">&#125;//res=res*res</span><br><span class="line">for(i=0;i&lt;n;i++)</span><br><span class="line">for(j=0;j&lt;n;j++)</span><br><span class="line">&#123;</span><br><span class="line">tep[i][j]=res[0][j]*res[i][0]+res[j][1]*res[1][i];</span><br><span class="line">&#125;</span><br><span class="line">for(i=0;i&lt;n;i++)</span><br><span class="line">for(j=0;j&lt;n;j++)</span><br><span class="line">res[i][j]=tep[i][j];</span><br><span class="line">N&gt;&gt;=1;</span><br><span class="line">&#125;</span><br><span class="line">return ans[0][0];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><hr><p>至此我要介绍的全部方法都结束了。其实，如果认真看下来，可能都有点忘了我们最初只是为了算出爬楼梯的方法数。我之所以要写这么多方法是因为我觉得这些方法远比一道题的最优解重要。因为有些方法可能在这道题里并不是最优的解法，但在别的题里或许是最优的解。我们应该能理解并使用这些方法，哪怕是最开始的贪婪算法，只要掌握了思想，其实在许多地方是有用武之地的。</p><p>总结：爬楼梯问题在本质上来说就是将整体最优解转化为子问题最优解，可选方案包括动态规划与贪心算法，<br>具体到本问题中斐波那契数列的应用以及简化，都是动态规划在此情形下的数学优化，而使用排列组合方法则是贪心算法的体现，如果将问题的特征更一般化，通用的解决方案还得从本质的动态规划和贪心入手，而动态规划相比于贪心则更容易实现，且贪心的每一个操作都会对结果产生直接影响，而动态规划可以把之前的数据保存下来可以回退，数据更立体，可操作性更强。</p><p><em>匆忙总结，如有错误,海涵见谅。</em>😋</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;&lt;em&gt;题目：假设你正在爬楼梯，需要n阶你才能到达楼顶。每次你可以爬1阶或2阶台阶。你有多少种不同的方法可以爬到楼顶呢？&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;这里将对此问题写出多个解法，包括一般容易实现的递归，以及它的一步一步优化。&lt;/p&gt;

      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>异常处理</title>
    <link href="http://yoursite.com/2019/05/07/%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/"/>
    <id>http://yoursite.com/2019/05/07/异常处理/</id>
    <published>2019-05-07T04:13:15.000Z</published>
    <updated>2019-08-07T07:10:40.269Z</updated>
    
    <content type="html"><![CDATA[<p><em>引言：每当人们接触事物时不免产生对其基本概念的哲学思考。也许对人来讲，一句简单的“我是谁？”就足足困扰了哲学家千年，但面对计算机，有些问题却显得容易回答得多了。曾听闻一个观点，研究自然科学是相当困难的，因为对象是自然界，是通过既有的方法与认知对现象进行解读，是在未知中探索；而对于学习一些人们既定的规则也好，生产工具也罢，相对是容易的，因为这些东西都是人类创造产生的，为了提升生产效率等各方面原因，人不必为难自己，一切一定以最易接受和使用为前提。如今想来有些道理，我们有意图所以我们创造，创造工具，构建规则，便利自己。<br>（这篇文章以异常处理为主线讲述，异常这个概念的产生，由何而来，产生的意义，对于它的存在进行探讨。以广义入手，实践落实在 Python3 的应用上 ）</em></p><h2 id="异常是什么？"><a href="#异常是什么？" class="headerlink" title="异常是什么？"></a>异常是什么？</h2><ul><li><strong>标准概念</strong>：异常指的是在程序运行过程中发生的异常事件，通常是由外部问题（如硬件错误、输入错误）所导致的。在Python、Java等面向对象的编程语言中异常属于对象。</li><li><strong>个人理解</strong>：一般来讲异常是个事件，在程序<font color="#DC143C">运行</font>时大多不会被处理，而影响了程序的正常执行。对于程序来讲，其语句或表达式在语法上是正确的，但在尝试执行时，这个事件（异常）会引发错误。这里先入为主地讲，一些面向对象的编程语言将此事件引发的错误抽象成一个对象，亦命名为“异常”，所有异常对象组成一个类。在 Python 中, 所有异常都必须是从 BaseException 派生的类的实例。<blockquote><p><em>更多异常在Python中的细节，建议阅读<a href="https://docs.python.org/3/tutorial/errors.html" target="_blank" rel="noopener">官方文档</a>。需要注意的是，在Python中标准的异常类型是内置的标识符（而不是保留关键字）。<br>（注：大致浏览完文档后，会发现之前先入为主的观点是有些不全面的，为了符合人对于异常的第一印象和某种潜在认知，此前我隐式地将异常与错误等同，其实在Python中，异常不仅仅是以Error作为尾缀的，即有些异常在某种层面上来讲并不是一个错误，而更像是之前讲的，导致程序无法正常执行的事件。）</em></p></blockquote></li></ul><p><strong>说了这么多，异常存在的意义是什么呢？</strong><br>其实是把”程序无法正常进行“这件事更抽象化的集中起来，归为一类。面向对象编程具有更高的整合能力，一个类是具有相同的方法和属性的，这样使我们对于这类事件有了更统一化且流程化的处理方式。这样的处理方式我们称为“异常处理”。</p><h2 id="异常处理是什么？"><a href="#异常处理是什么？" class="headerlink" title="异常处理是什么？"></a>异常处理是什么？</h2><p>上文似乎已经回答了这个问题，但不妨碍我们再深入探究一下。惯例先来一下官方一点的定义<br><em>Wiki：异常处理，是编程语言或计算机硬件里的一种机制，用于处理软件或信息系统中出现的异常状况（即超出程序正常执行流程的某些特殊条件）。</em><br>追根溯源一下，其实在本文讨论的异常处理这种方式出现前，人们对于软硬件不能正常操作时，系统设置了错误代码，即 error code ,对于不同的错误代码有不同的原因导致，操作者可以对其进行识别并找到原因。这样说不够直观，给个例子，比如网页的错误代码  404 ，不用多说相信很多人都见过类似的错误代码。error code 算是异常处理某种意义上的前生，异常处理是代替日渐衰落的error code方法的新法，提供error code 所未能具体的优势。<br><strong>接下来讨论对于编程来讲，这样的处理方式与传统的返回标志配合选择结构有什么区别和优势呢？</strong><br>拿大家应该都熟悉的C语言举例吧，早期我们对于c语言的异常处理机制，通常是我们认为的对返回结果加一些标志来进行判定，如发生错误返回什么标志，正常情况下我们又是返回什么标记，对于不同的标志我们借助选择结构去进行不同的处理，但这些都不是语言本身所赋予我们的，这种机制的问题在于，C语言的异常处理机制全是我们人为的定义，过度的自由往往会导致难以驾驭的混乱产生，如果没有体制化的规则或规范产生，就会造成业务逻辑的主线受到异常处理的牵制，或者说是我们难免会将注意力转移，并且造成业务逻辑与异常处理之间有很大程度上的缠绕。而对于Python来说，它有内部有已经构建好的异常类，我们使用expection的机制，对这些使程序无法正常运行的事件经行捕获和处理，更加方便，使我们有更多的精力专注于业务逻辑，而不必要使主线与异常处理的方式过度耦合而导致难以维护，降低整个系统或程序的灵活性。<br>处理本身的讨论价值已经所剩无多，接下来的应该是这个过程的实现，也就是异常机制。<br><strong>机制问题：</strong></p><ul><li>程序执行方面，大部分常见的程序设计语言，都是沿着函数调用栈的函数调用逆向搜索，直到遇到异常处理代码为止。一般在这个异常处理代码的搜索过程中逐级完成栈卷回。</li><li>语法方面，多数语言的异常机制的语法是类似的：用throw或<font color="#1E90FF">raise</font>抛出一个异常对象（Java或C++等）；异常处理代码的作用范围用标记子句（<font color="#1E90FF">try</font>或begin开始的语言作用域）标示其起始，以第一个异常处理子句（catch,<font color="#1E90FF">expect</font>, rescue等）标示其结束；可连续出现若干个异常处理子句，每个处理特定类型的异常。某些语言允许<font color="#1E90FF">else</font>子句，用于无异常出现的情况。更多见的是<font color="#1E90FF">finally</font>, ensure子句，无论是否出现异常它都将执行，用于释放异常处理所需的一些资源。（蓝色为Python所采用的关键字）</li></ul><p><strong>大致介绍一下Python中关键字的原理</strong><br>由 <font color="#FF8C00">try</font> 讲起，作为处理机制的起始关键词，try的工作原理是如下（引用自<a href="https://docs.python.org/3/tutorial/errors.html" target="_blank" rel="noopener">官方文档</a>）</p><ul><li>首先，执行 try 子句 （try 和 except 关键字之间的（多行）语句）。</li><li>如果没有异常发生，则跳过 except 子句 并完成 try 语句的执行。</li><li>如果在执行try 子句时发生了异常，则跳过该子句中剩下的部分。然后，如果异常的类型和 except 关键字后面的异常匹配，则执行 except 子句 ，然后继续执行 try 语句之后的代码。</li><li>如果发生的异常和 except 子句中指定的异常不匹配，则将其传递到外部的 try 语句中；如果没有找到处理程序，则它是一个 未处理异常，执行将停止并显示如上所示的消息。<blockquote><p>try会控制一个子句，这个子句通常是我们想要测试是否会出现异常的代码段或者会经常出现过多与不可知问题的代码段，try界定了这样的一个范围。</p></blockquote></li></ul><p>与try搭配的就是 <font color="#FF8C00">expect</font> ，那么工作原理：一个 try 语句可能有多个 except 子句，以指定不同异常的处理程序。 最多会执行一个处理程序。 处理程序只处理相应的 try 子句中发生的异常，而不处理同一 try 语句内其他处理程序中的异常。基类的expect语句兼容派生类，反之不然。往往最后的 except 子句可以省略异常名，以用作通配符。</p><blockquote><p>expect的作用浅显易懂，与try搭配起来使用倒也有点 switch-case 的感觉，不过语法更简单，应用更灵活。</p></blockquote><p>主体讲完了，接下来说说可选子句，用做扩充。<br>首先<font color="#FF8C00">else</font>子句，在使用时必须放在所有的 except 子句后面。对于在try 子句不引发异常时必须执行的代码来说很有用。</p><blockquote><p>有时如果不给予通配符，而try语句中的代码出现未能捕获的异常，else的存在就降低了这种情况的出现。</p></blockquote><p><em>至于异常的 <font color="#FF8C00">args</font> 和 <font color="#FF8C00">__str__()</font> ，这里不多赘述。</em><br>抛出异常 <font color="#FF8C00">raise</font> ，有唯一指定参数——异常的实例或类。</p><blockquote><p>问题来了，为什么需要抛出异常呢？有时是对异常进行一定处理后再次扔出，一般的考虑是只想确定是否会抛出异常，或者抛出后还有处理。然而有时候还会和用户自定义异常结合使用，这时候就凸显出其功能。</p></blockquote><p><font color="#FF8C00">用户自定义异常</font>。还记得之前提到过在Python中所有的异常都必须是从 BaseException 派生的类的实例，而这里要强调的是<font color="#1E90FF#DC143C">用户自定义的异常并不是从这个基类直接派生的，而是从 Exception 直接继承或间接继承的，尽管 Exception 是从 BaseException 继承来的。</font><br>最后也就是<font color="#FF8C00">finally</font>，它可以让操作者定义一个清洗行为，finally 子句 总会在离开 try 语句前被执行，无论是否发生了异常。当在 try 子句中发生了异常且尚未被 except 子句处理（或者它发生在 except 或 else 子句中）时，它将在 finally 子句执行后被重新抛出。 当 try 语句的任何其他子句通过 break, continue 或 return 语句离开时，finally 也会在“离开之前”被执行。</p><blockquote><p>在实际应用程序中，finally子句对于释放外部资源（例如文件或者网络连接）非常有用，无论是否成功使用资源。</p></blockquote><p>短暂的介绍只是为了简单回顾和把各个关键词的价值所在指出。</p><h2 id="接下来介绍两种异常处理的基本模型。"><a href="#接下来介绍两种异常处理的基本模型。" class="headerlink" title="接下来介绍两种异常处理的基本模型。"></a>接下来介绍两种异常处理的基本模型。</h2><p><em>第一种为中止模型，另一种为继续模型，或称恢复模型。实际使用方面我们对中止模型用的比较多，这个模型比较实用，而继续模型则不是那么的应用普遍。</em></p><ul><li>中止模型 ：<br>假设错误非常严重，已至你无法在回到错误发生的地方，也就是说，这段程序经过判断认为，他已经没有办法挽回，于是就抛出异常。</li><li><p>继续模型：<br>这种模型的主旨是恢复当前的运行环境，然后希望能够重新回到错误的发生地，并希望第二次的尝试能够获得成功，这种模型通常为操作系统所应用。</p><blockquote><p>虽然恢复模型开始显得很吸引人，并且人们使用的操作系统也支持恢复模型的异常处理,但程序员们最终还是转向了使用类似”终止模型”的代码。因为：处理程序必须关注异常抛出的地点，这势必要包含依赖于抛出位置的非通用性代码。这增加了代码编写和维护的困难，对于异常可能会从许多地方抛出的大型程序来说，更是如此。</p></blockquote><p>完</p></li></ul><hr><p>本文的理想受众不是那些对异常处理毫无认知的人群，而是有初步了解且至少做过一些实践哪怕几行的小程序的这类人，用于更深入的思考和理解一些本质问题，而不是受限于语法句式的细节问题，因而未曾加入代码片段。如需大量代码段，推荐阅读<a href="https://docs.python.org/3/tutorial/errors.html" target="_blank" rel="noopener">官方文档</a>。相比于语法规范和语言特性，本文更集中于思考”定义、概念、存在“的价值，加入了不少主观的感受和理解，如有异议、疑惑欢迎讨论，有错误也欢迎斧正。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;em&gt;引言：每当人们接触事物时不免产生对其基本概念的哲学思考。也许对人来讲，一句简单的“我是谁？”就足足困扰了哲学家千年，但面对计算机，有些问题却显得容易回答得多了。曾听闻一个观点，研究自然科学是相当困难的，因为对象是自然界，是通过既有的方法与认知对现象进行解读，是在未知
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>博客创建问题及解决摘要</title>
    <link href="http://yoursite.com/2019/01/02/%E5%8D%9A%E5%AE%A2%E5%88%9B%E5%BB%BA%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%91%98%E8%A6%81/"/>
    <id>http://yoursite.com/2019/01/02/博客创建问题及解决摘要/</id>
    <published>2019-01-02T14:56:41.000Z</published>
    <updated>2020-02-12T08:44:44.295Z</updated>
    
    <content type="html"><![CDATA[<p>以下为本博客创建时所遇到的一些疑难问题与所关注过的注意事项，对其有详细解答。中途还会介绍涉及到的部分零碎知识。如有描述不准，望海涵斧正。<a href="https://www.cnblogs.com/fengxiongZz/p/7707219.html" target="_blank" rel="noopener">(参考的安装教程)</a></p><h1 id="一、Node-js-amp-Github环境搭建"><a href="#一、Node-js-amp-Github环境搭建" class="headerlink" title="一、Node.js &amp; Github环境搭建"></a>一、Node.js &amp; Github环境搭建</h1><h5 id="①、下载安装时避免将两个环境置入同一个文件夹"><a href="#①、下载安装时避免将两个环境置入同一个文件夹" class="headerlink" title="①、下载安装时避免将两个环境置入同一个文件夹"></a>①、下载安装时避免将两个环境置入同一个文件夹</h5><h5 id="②、创建Github项目时以“账户名-github-io”为格式"><a href="#②、创建Github项目时以“账户名-github-io”为格式" class="headerlink" title="②、创建Github项目时以“账户名.github.io”为格式"></a>②、创建Github项目时以“账户名.github.io”为格式</h5><h2 id="———————-进入正题———————"><a href="#———————-进入正题———————" class="headerlink" title="———————-进入正题———————-"></a>———————-<em>进入正题</em>———————-</h2><p><del>这里出现问题比较多，需要重点关注</del></p><h1 id="二、安装hexo"><a href="#二、安装hexo" class="headerlink" title="二、安装hexo"></a>二、<strong>安装hexo</strong></h1><h3 id="1、使用npm安装Github"><a href="#1、使用npm安装Github" class="headerlink" title="1、使用npm安装Github"></a>1、使用npm安装Github</h3><p><strong><em>问题Ⅰ：</em></strong>首次用npm安装Github时，一段进度条会一直无法“前进”，安装进度停滞不前。<br><strong>剖析：</strong>首先，简单介绍一下npm。</p><blockquote><p>npm是随同NodeJS一起安装的包(package)管理工具，能解决NodeJS代码部署上的很多问题。<br>它由三个独立的部分组成：</p><ul><li>网站</li><li>注册表(registry)</li><li>命令行工具(CLI)<br>由于众所周知的原因，国内访问外国网站时一直不怎么流畅，就此问题而言，比起搭梯子，一个淘宝镜像来的更方便快捷。:D<br>那么接下来就是讲一下镜像了。<br>镜像是冗余的一种类型，一个磁盘上的数据在另一个磁盘上存在一个完全相同的副本即为镜像。<br>根据这个描述，可以知道这个淘宝NPM镜像（可以从网络轻松获得，复制停滞进度条前的代码百度即可）是用来替代官方版本的完整镜像，可以完美解决此问题。<br>附上镜像：<code>npm config set registry https://registry.npm.taobao.org</code>  （退出原有的控制台，再次进入安装hexo的文件夹中，直接在命令行中输入即可）<h3 id="2、关联hexo和Github-page"><a href="#2、关联hexo和Github-page" class="headerlink" title="2、关联hexo和Github page"></a>2、关联hexo和Github page</h3><strong><em>问题Ⅱ：</em></strong><font color="#9932CC"> .ssh </font>文件夹及涉及问题<br><strong>具体描述及解决措施：</strong></li><li>①描述：在Git Bash的命令框中输入<code>cd ~/.ssh</code>时，反馈没有此文件夹。<br>解决方案：正常现象，可以继续教程的下一步，系统会自动创建.ssh文件夹。<br>即输入<code>ssh-keygen -t rsa -C “your email address”</code> <font color="#DC143C">(所有指令中注意空格不可略、不可多)</font><br>然后就没什么多说的了，回车就完事了。这时<font color="#9932CC"> .ssh </font>文件夹中会出现两个文件，一个是公钥一个是私钥。这就牵扯到密码学的RSA加密算法<del>在前面输入指令中出现过</del>。简单来说公钥是可公开信息，而私钥是你自己保管。通过一系列算法保证信息安全性。因为对极大整数做因数分解的难度决定了RSA算法的可靠性（不是简单入门的素数判断哦）穷举可破解，但时间得不偿失😊。</li><li>②描述：在hexo文件夹中，找到_config.yml文件并修改时需要注意的事项。<br>  第一这类文件对格式要求即为严格，<font color="#DC143C"> 每个冒号后必须要使用一个空格 </font>，对它的修改在后来配置自己博客时经常出现，所以需要牢记。不然本地检测会报错。<br><img src="..\images\error.jpg" alt="错误实例" title="从朋友那爬了一张图"><br>  第二修改时会有一项<code>repository:</code>     注意使用的是github上项目的SSH不是HTTP！不是HTTP！<del>没有第三遍</del></li></ul></blockquote><p>至此基本总结结束，后续也许会修改</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;以下为本博客创建时所遇到的一些疑难问题与所关注过的注意事项，对其有详细解答。中途还会介绍涉及到的部分零碎知识。如有描述不准，望海涵斧正。&lt;a href=&quot;https://www.cnblogs.com/fengxiongZz/p/7707219.html&quot; target=&quot;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>First post</title>
    <link href="http://yoursite.com/2018/12/30/First%20post/"/>
    <id>http://yoursite.com/2018/12/30/First post/</id>
    <published>2018-12-30T05:32:01.000Z</published>
    <updated>2019-08-07T12:15:47.182Z</updated>
    
    <content type="html"><![CDATA[<p> I know it seems like the world is crumbling out there,but it is actually a great time in your life to get a little crazy. Follow your curiosity,and be ambitious about it. Don’t give up your dreams. The world needs you all!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt; I know it seems like the world is crumbling out there,but it is actually a great time in your life to get a little crazy. Follow your cu
      
    
    </summary>
    
    
  </entry>
  
</feed>
