<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">


























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=6.6.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.6.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.6.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.6.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.6.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '6.6.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta property="og:type" content="website">
<meta property="og:title" content="X">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="X">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="X">



  <link rel="alternate" href="/atom.xml" title="X" type="application/atom+xml">




  <link rel="canonical" href="http://yoursite.com/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>X</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <a href="https://github.com/ZSDDZA" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">X</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">x</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-schedule">

    
    
    
      
    

    

    <a href="/schedule/" rel="section"><i class="menu-item-icon fa fa-fw fa-calendar"></i> <br>日程表</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-sitemap">

    
    
    
      
    

    

    <a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br>站点地图</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/05/10/FM算法简述/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/05/10/FM算法简述/" class="post-title-link" itemprop="url">FM算法简述</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-05-10 01:25:50 / 修改时间：01:47:03" itemprop="dateCreated datePublished" datetime="2020-05-10T01:25:50+08:00">2020-05-10</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h5 id="诞生的契机"><a href="#诞生的契机" class="headerlink" title="诞生的契机"></a>诞生的契机</h5><p>故事从LR模型说起，传统的LR模型每个特征都是相互独立的，但是我们要处理的情况往往没有这么理想，当需要考虑到特征之间的关系时，得要通过人工的方式对这些特征进行组合。除此之外，非线性SVM可以对特征进行kernel映射，但是在特征高度稀疏的情况下，学习效果并不好。其他的可以学习到特征之间关系的算法都受限于输入和使用场景，因此FM（Factorization Machine）诞生了。</p>
<p>来一个贴合的小栗子，比如要根据用户的各种行为特征来预测对于某部电影的喜爱程度（评分）。如下图：<br><img src="https://img-blog.csdnimg.cn/20200509233915479.png" alt="图片来自https://www.cnblogs.com/AndyJee/p/7879765.html"><br>User、Move、Last Move rated这三项都经过one-hot编码，所以本身是高度稀疏的。并且仔细查看这几项特征可以发现，其中有可能存在相关联的特征，比如当前当前打分的电影和上一步看过的电影，以及这两部提到的电影在图中黄框都有对应的评分。这样的例子屡见不鲜，有两个突出特点：特征高度稀疏且特征之间可能存在关联，这正是FM的使用范畴。</p>
<h5 id="算法模型"><a href="#算法模型" class="headerlink" title="算法模型"></a>算法模型</h5><script type="math/tex; mode=display">y(x):=w_0+\sum_{i=1}^{n}{w_ix_i}+\sum_{i=1}^{n}{\sum_{j=1+1}^{n}{<v_i,v_j>x_ix_j}}</script><script type="math/tex; mode=display">w_0\in{\mathbb{R}},\textbf{w}\in{\mathbb{R^n}},\pmb{V}\in{\mathbb{R}^{n\times{k}}}</script><script type="math/tex; mode=display"><v_i,v_j>:=\sum_{f=1}^{k}v_{i,f}\cdot{v_{j,f}}</script><p>仔细观察定义式，其实就是我们熟知的线性模型和一个交叉组合特征。</p>
<p>为什么要通过向量v的学习方式而不是简单的wij参数呢？<br>这是因为在稀疏条件下，这样的表示方法打破了特征的独立性，能够更好地挖掘特征之间的相关性。以上述电影为例，我们要估计用户A和电影ST的关系w(A&amp;ST)以更好地预测y，如果是简单地考虑特征之间的共现情况来估计w(A&amp;ST)，从已有的训练样本来看，这两者并没有共现，因此学习出来的w(A&amp;ST)=0。而实际上，A和ST应该是存在某种联系的，从用户角度来看，A和B都看过SW，而B还看过ST，说明A也可能喜欢ST，说明A很有可能也喜欢ST。而通过向量v来表示用户和电影，任意两两之间的交互都会影响v的更新，从前面举的例子就可以看过，A和B看过SW，这样的交互关系就会导致v(ST)的学习更新，因此通过向量v的学习方式能够更好的挖掘特征间的相互关系，尤其在稀疏条件下。</p>
<h5 id="关于复杂度"><a href="#关于复杂度" class="headerlink" title="关于复杂度"></a>关于复杂度</h5><p>直观来看上述定义式的计算时间复杂度显然是达到了<script type="math/tex">O(kn^2)</script>级别，这里比较tricky地方就是可以通过数学的方式化简至<script type="math/tex">O(kn)</script>步骤如下</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\quad\sum_{i=1}^{n}{\sum_{j=i+1}^{n}{<v_i,v_j>x_ix_j}}
\\&=\frac{1}{2}\sum_{i=1}^{n}{\sum_{j=1}^{n}{<v_i,v_j>x_ix_j}}-\frac{1}{2}\sum_{i=1}^{n}<v_i,v_i>x_ix_i
\\&=\frac{1}{2}(\sum_{i=1}^{n}{\sum_{j=1}^{n}{\sum_{f=1}^{k}{v_{i,f}v_{j,f}x_ix_j}}}-\sum_{i=1}^{n}{\sum_{f=1}^{n}{v_{i,f}v_{i,f}x_ix_i}})
\\&=\frac{1}{2}\sum_{f=1}^{k}{((\sum_{i=1}^{n}{v_{i,f}x_i})(\sum_{j=1}^{n}v_{j,f}x_j)-\sum_{i=1}^{n}{v_{i,f}^2x_i^2})}
\\&=\frac{1}{2}\sum_{f=1}^{k}{((\sum_{i=1}^{n}{v_{i,f}x_i})^2-\sum_{i=1}^{n}{v_{i,f}^2x_i^2})}
\end{aligned}</script><h5 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h5><p>  FM可以应用于很多预测任务，比如回归、分类、排序等等。</p>
<ul>
<li>回归Regression：y^(x)直接作为预测值，损失函数可以采用least square error；</li>
<li>二值分类Binary Classification：y^(x)需转化为二值标签，如0,1。损失函数可以采用hinge loss或logit<br>loss；</li>
<li>排序Rank：x可能需要转化为pair-wise的形式如(X^a,X^b)，损失函数可以采用pairwise loss</li>
</ul>
<p>参考：<br><a href="https://www.cnblogs.com/AndyJee/p/7879765.html" target="_blank" rel="noopener">https://www.cnblogs.com/AndyJee/p/7879765.html</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/05/03/深度学习中的Embeding/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/05/03/深度学习中的Embeding/" class="post-title-link" itemprop="url">py深度学习中的Embeding</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-05-03 01:40:36 / 修改时间：01:41:38" itemprop="dateCreated datePublished" datetime="2020-05-03T01:40:36+08:00">2020-05-03</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在许多深度学习实战中对于Embeding的介绍比较含糊，比如 Keras中文文档中对嵌入层 Embedding的介绍除了一句 “嵌入层将正整数（下标）转换为具有固定大小的向量”之外就没有了。下面选择重点详细说一下。<br>单词嵌入是使用密集的矢量表示来表示单词和文档的一类方法。词嵌入是对传统的词袋模型编码方案的改进，传统方法使用大而稀疏的矢量来表示每个单词或者在矢量内对每个单词进行评分以表示整个词汇表，这些表示是稀疏的，因为每个词汇的表示是巨大的，给定的词或文档主要由零值组成的大向量表示。这里指的实际就是：使用One-hot 方法编码的向量会很高维也很稀疏。假设我们在做自然语言处理（NLP）中遇到了一个包含2000个词的字典，当使用One-hot编码时，每一个词会被一个包含2000个整数的向量来表示，其中1999个数字是0，要是我的字典再大一点的话这种方法的计算效率岂不是大打折扣？<br>在嵌入中，单词由密集向量表示，其中向量表示将单词投影到连续向量空间中。向量空间中的单词的位置是从文本中学习的，并且基于在使用单词时围绕单词的单词。学习到的向量空间中的单词的位置被称为它的嵌入：Embedding。<br>还需要注意得点就是：训练神经网络的过程中，每个嵌入的向量都会得到更新。我们可以发现在多维空间中词与词之间有多少相似性，这使我们能可视化的了解词语之间的关系，不仅仅是词语，任何能通过嵌入层 Embedding 转换成向量的内容都可以这样做。<br><img src="https://img-blog.csdnimg.cn/20200503011440947.png" alt="在这里插入图片描述"><br>为了直观的理解embeding，借用网上找到的这个例子：</p>
<blockquote>
<p>“deep learning is very deep”</p>
</blockquote>
<p>使用嵌入层embedding 的第一步是通过索引对该句子进行编码，这里我们给每一个不同的句子分配一个索引，上面的句子就会变成这样：</p>
<blockquote>
<p>1 2 3 4 1</p>
</blockquote>
<p>接下来会创建嵌入矩阵，我们要决定每一个索引需要分配多少个‘潜在因子’，这大体上意味着我们想要多长的向量，通常使用的情况是长度分配为32和50。在这篇博客中，为了保持文章可读性这里为每个索引指定6个潜在因子。嵌入矩阵就会变成这样：<br><img src="https://img-blog.csdnimg.cn/20200503010703759.png" alt="在这里插入图片描述"><br>这样，我们就可以使用嵌入矩阵来而不是庞大的one-hot编码向量来保持每个向量更小。简而言之，嵌入层embedding在这里做的就是把单词“deep”用向量[.32, .02, .48, .21, .56, .15]来表达。然而并不是每一个单词都会被一个向量来代替，而是被替换为用于查找嵌入矩阵中向量的索引。其次这种方法面对大数据时也可有效计算。由于在深度神经网络的训练过程中嵌入向量也会被更新，我们就可以探索在高维空间中哪些词语之间具有彼此相似性，再通过使用t-SNE 这样的降维技术就可以将这些相似性可视化。</p>
<h5 id="keras"><a href="#keras" class="headerlink" title="keras"></a>keras</h5><p>嵌入层embedding用在网络的开始层将你的输入转换成向量，所以当使用 Embedding前应首先判断你的数据是否有必要转换成向量。如果你有categorical数据或者数据仅仅包含整数（像一个字典一样具有固定的数量）你可以尝试下Embedding 层。<br>如果你的数据是多维的你可以对每个输入共享嵌入层或尝试单独的嵌入层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers.embeddings <span class="keyword">import</span> Embedding</span><br><span class="line"></span><br><span class="line">Embedding(input_dim, output_dim, embeddings_initializer=<span class="string">'uniform'</span>, embeddings_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>, embeddings_constraint=<span class="keyword">None</span>, mask_zero=<span class="keyword">False</span>, input_length=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>Keras提供了一个嵌入层，适用于文本数据的神经网络。</p>
<p>它要求输入数据是整数编码的，所以每个字都用一个唯一的整数表示。这个数据准备步骤可以使用Keras提供的Tokenizer API来执行。</p>
<p>嵌入层用随机权重进行初始化，并将学习训练数据集中所有单词的嵌入。</p>
<p>它是一个灵活的图层，可以以多种方式使用，例如：</p>
<ul>
<li>它可以单独使用来学习一个单词嵌入，以后可以保存并在另一个模型中使用。</li>
<li>它可以用作深度学习模型的一部分，其中嵌入与模型本身一起学习。</li>
<li>它可以用来加载预先训练的词嵌入模型，这是一种迁移学习。<br>定义一个小问题，我们有10个文本文档，每个文档都有一个学生提交的工作评论。每个文本文档被分类为正的“1”或负的“0”。这是一个简单的情感分析问题。</li>
</ul>
<p>首先，我们将定义文档及其类别标签。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># define documents 定义文档</span></span><br><span class="line">docs = [<span class="string">'Well done!'</span>,</span><br><span class="line">        <span class="string">'Good work'</span>,</span><br><span class="line">        <span class="string">'Great effort'</span>,</span><br><span class="line">        <span class="string">'nice work'</span>,</span><br><span class="line">        <span class="string">'Excellent!'</span>,</span><br><span class="line">        <span class="string">'Weak'</span>,</span><br><span class="line">        <span class="string">'Poor effort!'</span>,</span><br><span class="line">        <span class="string">'not good'</span>,</span><br><span class="line">        <span class="string">'poor work'</span>,</span><br><span class="line">        <span class="string">'Could have done better.'</span>]</span><br><span class="line"><span class="comment"># define class labels 定义分类标签</span></span><br><span class="line">labels = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>接下来，我们来整数编码每个文件。这意味着把输入，嵌入层将具有整数序列。我们可以尝试其他更复杂的bag of word 模型比如计数或TF-IDF。</p>
<p>Keras提供one_hot()函数来创建每个单词的散列作为一个有效的整数编码。我们用估计50的词汇表大小，这大大减少了hash函数的冲突概率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># integer encode the documents 独热编码</span></span><br><span class="line">vocab_size = <span class="number">50</span></span><br><span class="line">encoded_docs = [one_hot(d, vocab_size) <span class="keyword">for</span> d <span class="keyword">in</span> docs]</span><br><span class="line">print(encoded_docs)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">6</span>, <span class="number">16</span>], [<span class="number">42</span>, <span class="number">24</span>], [<span class="number">2</span>, <span class="number">17</span>], [<span class="number">42</span>, <span class="number">24</span>], [<span class="number">18</span>], [<span class="number">17</span>], [<span class="number">22</span>, <span class="number">17</span>], [<span class="number">27</span>, <span class="number">42</span>], [<span class="number">22</span>, <span class="number">24</span>], [<span class="number">49</span>, <span class="number">46</span>, <span class="number">16</span>, <span class="number">34</span>]]</span><br></pre></td></tr></table></figure>
<p>这样以后序列具有不同的长度，但是Keras更喜欢输入矢量化和所有输入具有相同的长度。我们将填充所有输入序列的长度为4，同样，我们可以使用内置的Keras函数（在这种情况下为pad_sequences()函数）执行此操作,</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pad documents to a max length of 4 words 将不足长度的用0填充为长度4</span></span><br><span class="line">max_length = <span class="number">4</span></span><br><span class="line">padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding=<span class="string">'post'</span>)</span><br><span class="line">print(padded_docs)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[[ <span class="number">6</span> <span class="number">16</span>  <span class="number">0</span>  <span class="number">0</span>]</span><br><span class="line"> [<span class="number">42</span> <span class="number">24</span>  <span class="number">0</span>  <span class="number">0</span>]</span><br><span class="line"> [ <span class="number">2</span> <span class="number">17</span>  <span class="number">0</span>  <span class="number">0</span>]</span><br><span class="line"> [<span class="number">42</span> <span class="number">24</span>  <span class="number">0</span>  <span class="number">0</span>]</span><br><span class="line"> [<span class="number">18</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>]</span><br><span class="line"> [<span class="number">17</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>]</span><br><span class="line"> [<span class="number">22</span> <span class="number">17</span>  <span class="number">0</span>  <span class="number">0</span>]</span><br><span class="line"> [<span class="number">27</span> <span class="number">42</span>  <span class="number">0</span>  <span class="number">0</span>]</span><br><span class="line"> [<span class="number">22</span> <span class="number">24</span>  <span class="number">0</span>  <span class="number">0</span>]</span><br><span class="line"> [<span class="number">49</span> <span class="number">46</span> <span class="number">16</span> <span class="number">34</span>]]</span><br></pre></td></tr></table></figure>
<p>我们现在准备将我们的嵌入层定义为我们的神经网络模型的一部分。</p>
<p>嵌入的词汇量为50，输入长度为4，我们将选择一个8维的嵌入空间。</p>
<p>该模型是一个简单的二元分类模型。重要的是，嵌入层的输出将是每个8维的4个矢量，每个单词一个。我们将其平铺到一个32个元素的向量上以传递到密集输出层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># define the model 定义模型</span></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Embedding(vocab_size, <span class="number">8</span>, input_length=max_length))</span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"><span class="comment"># compile the model 编译</span></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'acc'</span>])</span><br><span class="line"><span class="comment"># summarize the model 打印模型信息</span></span><br><span class="line">print(model.summary())</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param <span class="comment">#</span></span><br><span class="line">=================================================================</span><br><span class="line">embedding_1 (Embedding)      (<span class="keyword">None</span>, <span class="number">4</span>, <span class="number">8</span>)              <span class="number">400</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">flatten_1 (Flatten)          (<span class="keyword">None</span>, <span class="number">32</span>)                <span class="number">0</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_1 (Dense)              (<span class="keyword">None</span>, <span class="number">1</span>)                 <span class="number">33</span></span><br><span class="line">=================================================================</span><br><span class="line">Total params: <span class="number">433</span></span><br><span class="line">Trainable params: <span class="number">433</span></span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure>
<p>最后，我们可以拟合和评估分类模型。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># fit the model 拟合</span></span><br><span class="line">model.fit(padded_docs, labels, epochs=<span class="number">50</span>, verbose=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># evaluate the model 评估</span></span><br><span class="line">loss, accuracy = model.evaluate(padded_docs, labels, verbose=<span class="number">0</span>)</span><br><span class="line">print(<span class="string">'Accuracy: %f'</span> % (accuracy*<span class="number">100</span>))</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Accuracy: <span class="number">100.000000</span></span><br></pre></td></tr></table></figure>
<hr>
<p>参考：<br><a href="https://fuhailin.github.io/Embedding/" target="_blank" rel="noopener">https://fuhailin.github.io/Embedding/</a><br><a href="https://blog.csdn.net/sinat_22510827/article/details/90727435?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-2&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-2" target="_blank" rel="noopener">https://blog.csdn.net/sinat_22510827/article/details/90727435?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-2&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-2</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/26/python——re模块/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/04/26/python——re模块/" class="post-title-link" itemprop="url">python——re模块</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-04-26 00:48:08 / 修改时间：00:48:47" itemprop="dateCreated datePublished" datetime="2020-04-26T00:48:08+08:00">2020-04-26</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>很久没有编写爬虫程序，不过前不久又再次上手。对于文本数据获取指定信息，或者进行清洗，正则表达式都是一个强力的工具。它的使用场景也十分多样。借此回顾总结一下，python中的re模块的主要功能与示例。</p>
<h3 id="compile"><a href="#compile" class="headerlink" title="compile"></a>compile</h3><p>re.compile()可以对传入的字符串进行编译，来返回一个目标的匹配模式，从而提高正则的效率。主要参数：</p>
<blockquote>
<p>pattern : 需要编译的字符串<br>flags : 修改匹配方式，包括以下可选：</p>
<blockquote>
<p>re.S : 使.匹配包括换行在内的所有字符<br>re.I : 使匹配对大小写不敏感<br>re.U : 根据Unicode规则解析字符，主要用于对中文匹配</p>
</blockquote>
</blockquote>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">text = <span class="string">'正则表达式（英语：Regular Expression，常简写为regex、regexp或RE），又称正则表示式、正则表示法、规则表达式、常规表示法，是计算机科学的一个概念。'</span></span><br><span class="line"><span class="comment">#  获取文本中被中文括号包含的内容</span></span><br><span class="line">pattern = re.compile(<span class="string">'（(.*)）'</span>,flags=re.U)</span><br><span class="line">pattern.findall(text)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200425233415856.png" alt="在这里插入图片描述"><br>匹配内容以列表的形式返回<br>看一下flags的作用对比：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">text = <span class="string">'正则表达式（英语：Regular Expression，常简写为regex、regexp或RE），又称正则表示式、正则表示法、规则表达式、常规表示法，是计算机科学的一个概念。'</span></span><br><span class="line"><span class="comment">#小写英文字母至少出现一次的内容</span></span><br><span class="line">pattern1 = re.compile(<span class="string">'[a-z]+'</span>)</span><br><span class="line"><span class="comment">#无视大小写</span></span><br><span class="line">pattern2 = re.compile(<span class="string">'[a-z]+'</span>,flags=re.I)</span><br><span class="line">print(pattern1.findall(text))</span><br><span class="line">print(pattern2.findall(text))</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200425234009463.png" alt="在这里插入图片描述"></p>
<h3 id="match"><a href="#match" class="headerlink" title="match"></a>match</h3><p>re.match()，以定义的正则表达式作为对目标字符串开头的匹配（对非开头部分不匹配）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">text = <span class="string">'许多程序设计语言都支持利用正则表达式进行字符串操作。'</span></span><br><span class="line">print(re.match(<span class="string">'许多'</span>,text,re.U))</span><br><span class="line">print(re.match(<span class="string">'语言'</span>,text,re.U))</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/2020042523540991.png" alt="在这里插入图片描述"><br>似乎有些鸡肋？其实和re.search()类似，熟悉re模块下的分组对于他们的使用就会有些提升，如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">text = <span class="string">'许多程序设计语言都支持利用正则表达式进行字符串操作。'</span></span><br><span class="line">print(re.match(<span class="string">'(许多).*(语言)'</span>,text,re.U))</span><br><span class="line">print(re.match(<span class="string">'(许多).*(语言)'</span>,text,re.U).group(<span class="number">0</span>))</span><br><span class="line">print(re.match(<span class="string">'(许多.*)(语言)'</span>,text,re.U).group(<span class="number">1</span>))</span><br><span class="line">print(re.match(<span class="string">'(许多)(.*语言)'</span>,text,re.U).group(<span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/2020042600143986.png" alt="在这里插入图片描述"></p>
<h3 id="search"><a href="#search" class="headerlink" title="search"></a>search</h3><p>re.search()格式类似re.match()，即三个传入参数：pattern，string，flags，但与match匹配开头不同的是，search匹配的是文中出现的第一个满足条件的字符串部分并返回，对后续的不再进行匹配。<br>例子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">text = <span class="string">'最初的正则表达式出现于理论计算机科学的自动控制理论和形式化语言理论中。'</span></span><br><span class="line">print(re.search(<span class="string">'理论'</span>,text,re.U))</span><br><span class="line">print(re.search(<span class="string">'理论'</span>,text,re.U).group())</span><br><span class="line">print(re.search(<span class="string">'(理论).*(理论)'</span>,text,re.U).group(<span class="number">0</span>))</span><br><span class="line">print(re.search(<span class="string">'(理论.*)(理论)'</span>,text,re.U).group(<span class="number">1</span>))</span><br><span class="line">print(re.search(<span class="string">'(理论)(.*理论)'</span>,text,re.U).group(<span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200426001924129.png" alt="在这里插入图片描述"></p>
<p>在前面几个例子中使用到的group()方法，是针对match或search成功匹配并返回的对象，称为match object，围绕它的常用方法如下：</p>
<blockquote>
<p>　　strat()：返回匹配开始的位置</p>
<p>　　end()：返回匹配结束的位置</p>
<p>　　group()：返回被re匹配的字符串</p>
<p>　　span()：返回一个tuple格式的对象，标记了匹配开始，结束的位置，形如(start,end)</p>
</blockquote>
<p>事实上，虽然说search只返回一个对象，但我们可以通过将正则表达式改造成若干子表达式拼接的形式，来返回多个分块的对象。</p>
<h3 id="findall"><a href="#findall" class="headerlink" title="findall"></a>findall</h3><p>re.findall()会根据传入的正则表达式部分来提取目标字符串中所有符合规则的部分，并传出为列表的形式。<br>例子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">text = <span class="string">'1940年，沃伦·麦卡洛克与Walter Pitts将神经系统中的神经元描述成小而简单的自动控制元。'</span></span><br><span class="line"></span><br><span class="line">print(re.findall(<span class="string">'..元'</span>,text))</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200426002420879.png" alt="在这里插入图片描述"><br>与前面在介绍re.compile()时对findall的用法不同，这里是re.findall(正则表达式,目标字符串)的格式，前面的是 编译好的正则模式.findall(目标字符串)，这两种格式的功能等价</p>
<h3 id="finditer"><a href="#finditer" class="headerlink" title="finditer"></a>finditer</h3><p>我们有时候会遇到这样的情况：目标字符串非常长（可能是一整篇小说），而符合我们正则表达式的目标内容也非常的多，这种时候如果沿用前面的做法使用re.findall()来一口气将所有结果提取出来保存在一个硕大的列表中，是件非常占用内存的事情，而Python中用来节省内存的生成器（generator）就派上了用场；</p>
<p>re.finditer(pattern,string,flags=0)就利用了这种机制，它构造出一个基于正则表达式pattern和目标字符串string的生成器，使得我们可以在对该生成器的循环中边循环边计算对应位置的值，即从始至终每一轮只保存了当前的位置和当前匹配到的内容，达到节省内存的作用，下面是一个简单的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">text = <span class="string">'''优美优于丑陋，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">明了优于隐晦；</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">简单优于复杂，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">复杂优于凌乱，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">扁平优于嵌套，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">稀疏优于稠密，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">可读性很重要！'''</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''构造我们的替代规则'''</span></span><br><span class="line">iterator = re.finditer(<span class="string">'(..)优于'</span>,text)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> iterator:</span><br><span class="line">    print(i.group(<span class="number">0</span>))</span><br><span class="line">    print(i.group(<span class="number">1</span>))</span><br><span class="line">    print(i.span())</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/2020042600375248.png" alt="在这里插入图片描述"></p>
<h3 id="sub"><a href="#sub" class="headerlink" title="sub"></a>sub</h3><p>类似字符串操作中的replace()，只不过replace()中只能死板地设置固定的内容作为替换项，利用re.sub(pattern,repl,string,count)则可以基于正则表达式达到灵活匹配替换内容，pattern指定了正则表达式部分，repl指定了进行替换的新内容，string指定目标字符串，count指定了替换的次数，默认全部替换，其实前一篇文章结尾处我们得到一篇干净的新闻报道就用到了这种方法，下面再举一个简单的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">text = <span class="string">'''优美优于丑陋，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">明了优于隐晦；</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">简单优于复杂，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">复杂优于凌乱，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">扁平优于嵌套，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">稀疏优于稠密，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">可读性很重要！'''</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''构造我们的替代规则'''</span></span><br><span class="line">print(re.sub(<span class="string">'优于'</span>,<span class="string">'胜过'</span>,text))</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200426003215285.png" alt="在这里插入图片描述"></p>
<h3 id="split"><a href="#split" class="headerlink" title="split"></a>split</h3><p>类似于字符串处理中的split()，re.split()在原有基础上扩充了正则表达式的功能，re.split(pattern,string,maxsplit)，其中pattern指定分隔符的正则表达式，string指定目标字符串，maxsplit指定最大分割个数，下面是一个简单的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">text = <span class="string">'''优美优于丑陋，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">明了优于隐晦，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">简单优于复杂，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">复杂优于凌乱，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">扁平优于嵌套，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">稀疏优于稠密，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">可读性很重要！'''</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''构造我们的替代规则'''</span></span><br><span class="line">print(re.split(<span class="string">'，\n\n'</span>,text))</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200426004116940.png" alt="在这里插入图片描述"><br>以上就是关于re模块的常用功能。以后有机会再写一篇结合爬虫来实践以下。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/12/初识反向传播/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/04/12/初识反向传播/" class="post-title-link" itemprop="url">初识反向传播</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-04-12 00:06:15 / 修改时间：00:06:46" itemprop="dateCreated datePublished" datetime="2020-04-12T00:06:15+08:00">2020-04-12</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h3><p>在单层感知机模型中，对于输入与输出之间的权重调整依赖于预测产生的误差，由于不含隐藏层，误差可以直接计算得到。而对于多层网络来说，由于隐藏层的存在，输入输出之间的权重变得复杂，显然直接计算并不合理，而是需要通过从输出到输入反方向逐层计算。</p>
<p>由于是从输出到输入，所以我们一定需要先有一个正向传播的过程。使得样本从输入层开始，由上至下逐层经隐节点计算处理，最终样本信息被传送到输出层节点，得到预测的结果。再根据正向传播得到的结果也就是预测经行误差计算。</p>
<p>由于有了误差和输出，以及之前传播过来的网络，就有了反向传播的原材料，可以开始进行反向传播了。反向传播无法直接计算隐节点的预测误差，但却可以利用输出节点的预测误差来逐层估计隐节点的误差，也就是将输出节点的预测误差反方向传播到上层隐节点，逐层调整连接权重，直至输入节点和隐节点的权重全部得到调整，使得网络输出值越来越逼近实际值。</p>
<h3 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h3><p>每个神经元都由两个单元组成。第一个单元的功能是把每个输入信号和对应权重系数($w_{i}$)作乘积然后对这些乘积求和。第二个单元实现了一个非线性函数。这个函数也称为神经元激活函数(neuron activation function)。如下图所示，信号$e$是加法器（即第一个单元）的输出信号，而$y = f(e)$是非线性函数（即第二个单元）的输出信号。显然，$y$也同时是这整个神经元的输出信号。<br><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2dhbGF4eS5hZ2guZWR1LnBsL352bHNpL0FJL2JhY2twX3RfZW4vYmFja3Byb3BfZmlsZXMvaW1nMDFiLmdpZg" alt="在这里插入图片描述"><br>一般这个非线性激活函数采用sigmoid函数。节点的输出被限制在0~1的范围内。对于分类问题，输出节点给出的是预测类别的概率值；对于回归问题，输出节点给出的标准化处理后的预测值，只需还原处理即可。<br>使用sigmoid的原因：</p>
<ul>
<li>在模型开始训练阶段，由于连接权重的设置要求满足均值为0的均匀分布，所以初期的连接权重在0值附近，使得加法器结果也在0附近。此时sigmoid函数的斜率近似为一个常数，输入输出间呈近似线性关系，模型较为简单；</li>
<li>随着模型训练的进行，网络权重不断调整，节点加法器的结果逐渐偏离0值，输入输出逐渐呈现出非线性关系，模型逐渐复杂起来，并且输入的变化对输出的影响程度逐渐减小</li>
<li>到模型训练的后期，节点加法器结果远离0，此时输入的变化将不再引起输出的明显变动，输出基本趋于稳定。神经网络的预测误差不再随着连接权重的调整而得到明显改善，预测结果稳定，模型训练结束。</li>
</ul>
<p>可见，sigmoid激活函数较好的体现了连接权重修正过程中，模型从近似线性到非线性的渐进转变过程。</p>
<p>除此之外，sigmoid激活函数还具有无限次可微的特点，这使得反向传播能够采用梯度下降法来挑战连接权重。</p>
<h3 id="关键过程"><a href="#关键过程" class="headerlink" title="关键过程"></a>关键过程</h3><p>由于权重的不同，会导致前置神经元对后续紧邻的后置神经元传递的误差不同，或者说对后置神经元的误差贡献度不同，所以在进行反向传播的时候，不能将误差均分到前置 的神经元，而是应该根据权值对误差进行反向的传递。<br><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2dhbGF4eS5hZ2guZWR1LnBsL352bHNpL0FJL2JhY2twX3RfZW4vYmFja3Byb3BfZmlsZXMvaW1nMDkuZ2lm" alt="在这里插入图片描述"><br><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2dhbGF4eS5hZ2guZWR1LnBsL352bHNpL0FJL2JhY2twX3RfZW4vYmFja3Byb3BfZmlsZXMvaW1nMTAuZ2lm" alt="在这里插入图片描述"><br>如果前置的神经元对后置多个神经元都传递了误差，那反向传播的时候潜质的误差需要按照权重对后置的误差求和。<br><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2dhbGF4eS5hZ2guZWR1LnBsL352bHNpL0FJL2JhY2twX3RfZW4vYmFja3Byb3BfZmlsZXMvaW1nMTEuZ2lm" alt="在这里插入图片描述"><br>当每个神经元的误差信号都已经被计算出来之后，我们就可以开始修改每个神经元输入结点的权重系数了。在下面的公式中，$\frac{\mathrm{df_1(e)} }{\mathrm{d} e}$表示对应的神经元激活函数的导数。<br><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2dhbGF4eS5hZ2guZWR1LnBsL352bHNpL0FJL2JhY2twX3RfZW4vYmFja3Byb3BfZmlsZXMvaW1nMTQuZ2lm" alt="在这里插入图片描述"><br><img src="http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop_files/img17.gif" alt="
"><br><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2dhbGF4eS5hZ2guZWR1LnBsL352bHNpL0FJL2JhY2twX3RfZW4vYmFja3Byb3BfZmlsZXMvaW1nMTkuZ2lm" alt="在这里插入图片描述"><br><em>计算是逐层的，这里只展示一条链上（$x_1—&gt;f_1{e}—&gt;f_4{e}—&gt;f_5{e}$）的计算，同层计算方式相同不多描述</em><br><em>具体的调整和计算方法涉及到<a href="https://zhuanlan.zhihu.com/p/36564434" target="_blank" rel="noopener">梯度下降算法</a></em><br>系数$\eta$会影响网络的训练速度。有几种技术可以用于选择系数$\eta$。第一种方法是以较大的参数值开始训练过程。在建立权重系数的同时，参数也逐渐减小。第二种方法更复杂，它以较小的参数值开始训练。在训练的过程中，参数增加，然后在最后阶段参数再次减小。以较小的参数值开始训练使得我们可以确定权重参数的符号。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/05/k-medoids聚类/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/04/05/k-medoids聚类/" class="post-title-link" itemprop="url">k-medoids聚类</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-04-05 01:54:17 / 修改时间：01:54:49" itemprop="dateCreated datePublished" datetime="2020-04-05T01:54:17+08:00">2020-04-05</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>我们知道对于K-means算法来说，如果数据样本中出现极端的离群值，导致样本数据分布出现一定的扭曲或者说偏离，则会导致聚类效果不好，与期望的效果之间有误差存在，也就是我们所说的极端值敏感。聚类很多情况下都是通过计算点之间的欧式距离来表现亲缘远近的，所以解决在其上产生的问题就逃不过中心点。这次介绍的k-medoids算法自然在处理一些异常值方面具有得天独厚的优势。<br>就原理而言，k-means与k-medoids区别在于中心点的选择。前者通过不断产生K个新的中心点（初始为随机，后续计算均值进行）来划分群集，以达到准则函数收敛的结果，其本质就是在不断寻找符合条件的群集重心；后者，顾名思义，不选用平均值，而是选用最中心的点，即需要计算当前簇各个点到中心点的距离。这也使得我们得到一个其与k-means的不同点，或者说k-medoids的性质，它所选取的中心点都是簇中的点。<br>不过由于算法的复杂性（需要遍历簇中所有点），导致更大的计算资源倾斜。</p>
<hr>
<p>简单复习一下顺便对比一下两者算法过程：</p>
<p>k-means：</p>
<blockquote>
<p>1） 任意选择K个对象作为初始的簇中心；<br>2） 分别计算数据集中每个元素与所选簇的中心计算距离（一般采用欧式距离），根据最近邻原则，将元素划分到相应的簇中；<br>3） 计算每个簇中对象的平均值，更新簇的中心；<br>4） 重复上面的步骤，直至更新的簇的中心与原簇的中心的差值在预定范围内，或者达到预设的迭代次数；<br>5） 输出K个簇中心。</p>
</blockquote>
<p>k-medoids:</p>
<blockquote>
<p>1、任意选取 k 个点作为 medoids<br>2、按照与medoids最近的原则，将剩余点分配到当前最佳的medoids代表的类中<br>3、在每一类中，计算每个成员点对应的准则函数，选取准则函数最小时对应的点作为新的 medoids<br>4、重复2-3的过程，直到所有的 medoids 点不再发生变化，或已达到设定的最大迭代次数<br>其中准则函数为，一类中，某个成员点和其他成员点的距离之和</p>
</blockquote>
<hr>
<p>来一点python的实战<br>由于可使用的第三方库不多，参考了很多文章和博客，找到了可用的库——pyclust，它还依赖于treelib库。<br>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyclust <span class="keyword">import</span> KMedoids</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="string">'''构造示例数据集（加入少量脏数据）'''</span></span><br><span class="line">data1 = np.random.normal(<span class="number">0</span>,<span class="number">0.9</span>,(<span class="number">1000</span>,<span class="number">10</span>))</span><br><span class="line">data2 = np.random.normal(<span class="number">1</span>,<span class="number">0.9</span>,(<span class="number">1000</span>,<span class="number">10</span>))</span><br><span class="line">data3 = np.random.normal(<span class="number">2</span>,<span class="number">0.9</span>,(<span class="number">1000</span>,<span class="number">10</span>))</span><br><span class="line">data4 = np.random.normal(<span class="number">3</span>,<span class="number">0.9</span>,(<span class="number">1000</span>,<span class="number">10</span>))</span><br><span class="line">data5 = np.random.normal(<span class="number">50</span>,<span class="number">0.9</span>,(<span class="number">50</span>,<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">data = np.concatenate((data1,data2,data3,data4,data5))</span><br><span class="line"></span><br><span class="line"><span class="string">'''准备可视化需要的降维数据'''</span></span><br><span class="line">data_TSNE = TSNE(learning_rate=<span class="number">100</span>).fit_transform(data)</span><br><span class="line"></span><br><span class="line"><span class="string">'''对不同的k进行试探性K-medoids聚类并可视化'''</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,<span class="number">6</span>):</span><br><span class="line">    k = KMedoids(n_clusters=i,distance=<span class="string">'euclidean'</span>,max_iter=<span class="number">1000</span>).fit_predict(data)</span><br><span class="line">    colors = ([[<span class="string">'red'</span>,<span class="string">'blue'</span>,<span class="string">'black'</span>,<span class="string">'yellow'</span>,<span class="string">'green'</span>][i] <span class="keyword">for</span> i <span class="keyword">in</span> k])</span><br><span class="line">    plt.subplot(<span class="number">219</span>+i)</span><br><span class="line">    plt.scatter(data_TSNE[:,<span class="number">0</span>],data_TSNE[:,<span class="number">1</span>],c=colors,s=<span class="number">10</span>)</span><br><span class="line">    plt.title(<span class="string">'K-medoids Resul of '</span>.format(str(i)))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>效果：<br><img src="https://img-blog.csdnimg.cn/20200405014833433.PNG" alt="在这里插入图片描述"></p>
<p>小结：k-medoids面对孤立的点时有更好的效果，但因此也占用了较多的计算资源，后续关于k-medoids有变种算法如：PAM算法 和 CLARA算法这里不作展开。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/29/梯度下降与脱离鞍点/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/03/29/梯度下降与脱离鞍点/" class="post-title-link" itemprop="url">梯度下降与脱离鞍点</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-03-29 00:48:13 / 修改时间：00:48:38" itemprop="dateCreated datePublished" datetime="2020-03-29T00:48:13+08:00">2020-03-29</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="直奔主题"><a href="#直奔主题" class="headerlink" title="直奔主题"></a>直奔主题</h4><p>如果要优化(找到它的最小值)一个函数$f(x)$ , 通常能够用Gradient Descent (GD), 也就是梯度下降，也称最速下降，因为梯度的本意是一个向量（矢量），表示某一函数在该点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向（此梯度的方向）变化最快，变化率最大（为该梯度的模）。<br>听着很简单就是每次沿着当前位置的导数方向走一小步, 走啊走啊就能够走到一个最小值点。<br>如图：<br><img src="https://img-blog.csdnimg.cn/20200328214122985.jpg" alt="在这里插入图片描述"></p>
<p>对应的数学语言：$x_{t+1}=x_t+γ_t\nabla f(x_t)$<br>$x_t$为第t步的位置，$γ_t$为第t步的步长，$\nabla f(x_t)$为梯度求导得到，nabla算子也可以写作$grad()$，具体可以这样表示$\nabla=\frac{\partial }{\partial x}\pmb i+\frac{\partial }{\partial y}\pmb j$。</p>
<p>在机器学习的中使用, 会面临非常大的数据集。这个时候如果硬要算$f(x)$的精确导数, 往往意味着我们要花几个小时把整个数据集都扫描一遍, 然后还只能走一小步。一般GD要几千步几万步才能收敛, 所以这样就根本跑不完了。其次, 我们可能就会不小心陷入了鞍点, 或者比较差的局部最优点。<br>局部最优解与鞍点：<br><img src="https://img-blog.csdnimg.cn/20200329001122896.jpg" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329001151684.jpg" alt="在这里插入图片描述"><br>Stochastic Gradient Descent (SGD) 算法而可以将将两个问题一并解决，<br>数学语言描述的形式：$x_{t+1}=x_t+γ_tg_t$<br>$g_t$就是随机梯度，满足约束：$E(g_t)= \nabla f(x_t)$<br>就是说虽然包含一定的随机性，但是从期望上来看，它是等于正确的导数的。其实SGD需要更多步才能够收敛的。由于它对导数的要求非常低，可以包含大量的噪声（或者说扰动），只要期望正确就行，所以导数算起来非常快。拿机器学习的例子来说，比如神经网络吧，训练的时候都是每次只从百万数据点里面拿128或者256个数据点，算一个不那么准的导数，然后用SGD走一步的。想想看，这样每次算的时间就快了10000倍，就算是多走几倍的路，算算也是挺值的了。</p>
<p>所以它可以完美解决GD的第一个问题——算得慢．这也是当初人们使用SGD的主要目的。而且，大家并不用担心导数中包含的噪声会有什么负面影响。有大量的理论工作说明，只要噪声不离谱，其实（至少在f是凸函数的情况下），SGD都能够很好地收敛。<br>再来说一下脱离鞍点的问题，导数为零的点叫Stationary points，即驻点。<br>可以是（局部）最小值，（局部）最大值，也可以是鞍点。<br>我们可以通过计算它的Hessian矩阵H来判断。</p>
<ul>
<li>如果H是负定的，说明所有的特征值都是负的．这个时候，你无论往什么方向走，导数都会变负，也就是说函数值会下降．所以，这是（局部）最大值．</li>
<li>如果H是正定的，说明所有的特征值都是正的．这个时候，你无论往什么方向走，导数都会变正，也就是说函数值会上升．所以，这是（局部）最小值．</li>
<li>如果Ｈ既包含正的特征值，又包含负的特征值，那么这个稳定点就是一个鞍点．具体参照之前的图片．也就是说有些方向函数值会上升，有些方向函数值会下降．</li>
<li>虽然看起来上面已经包含了所有的情况，但是其实不是的！还有一个非常重要的情况就是H可能包含特征值为０的情况．这种情况下面，我们无法判断稳定点到底属于哪一类，往往需要参照更高维的导数才行．想想看，如果特征值是０，就说明有些方向一马平川一望无际，函数值一直不变，那我们当然不知道是怎么回事了：）<br>今天讨论的情况只包含前三种，不包含第四种．第四种被称为退化了的情况，所以我们考虑的情况就叫做非退化情况。</li>
</ul>
<p>在这种非退化的情况下面，我们考虑一个重要的类别，即strict saddle函数．这种函数有这样的特点：对于每个点x</p>
<ul>
<li>要么x的导数比较大 </li>
<li>要么x的Hessian矩阵包含一个负的特征值 </li>
<li>要么x已经离某一个（局部）最小值很近了</li>
</ul>
<p>为什么我们要x满足这三个情况的至少一个呢？因为如果x的导数大，那么沿着这个导数一定可以大大降低函数值（我们对函数有光滑性假设）<br>如果x的Hessian矩阵有一个负的特征值，那么我们通过加噪声随机扰动，跑跑就能够跑到这个方向上，沿着这个方向就能够像滑滑梯一样一路滑下去，大大降低函数值<br>如果x已经离某一个（局部）最小值很近了，那么我们就完成任务了，毕竟这个世界上没有十全十美的事情，离得近和精确跑到这个点也没什么区别．<br>所以说，如果我们考虑的函数满足这个strict saddle性质，那么SGD算法其实是不会被困在鞍点的。那么strict saddle性质是不是一个合理的性质呢？</p>
<p>实际上，有大量的机器学习的问题使用的函数都满足这样的性质。比如Orthogonal tensor decomposition，dictionary learning, matrix completion等等。而且，其实并不用担心最后得到的点只是一个局部最优，而不是全局最优。因为实际上人们发现大量的机器学习问题，几乎所有的局部最优是几乎一样好的，也就是说，只要找到一个局部最优点，其实就已经找到了全局最优，比如Orthogonal tensor decomposition就满足这样的性质，还有NIPS16的best student paper证明了matrix completion也满足这样的性质。</p>
<p>下面讨论一下证明，主要讨论一下第二篇．第一篇论文其实就是用数学的语言在说＂在鞍点加扰动，能够顺着负的特征值方向滑下去＂．第二篇非常有意思，我觉得值得介绍一下想法。</p>
<p>首先，算法上有了一些改动．算法不再是SGD，而是跑若干步GD，然后跑一步SGD。当然实际上大家是不会这么用的，但是理论分析么，这么考虑没问题。什么时候跑SGD呢？只有当导数比较小，而且已经很长时间没有跑过SGD的时候，才会跑一次。也就是说，只有确实陷在鞍点上了，才会随机扰动一下下。</p>
<p>因为鞍点有负的特征值，所以只要扰动之后在这个方向上有那么一点点分量，就能够一马平川地滑下去。除非分量非常非常小的情况下才可能会继续陷在鞍点附近。换句话说，如果加了一个随机扰动，其实大概率情况下是能够逃离鞍点的！</p>
<p>虽然这个想法也很直观，但是要严格地证明很不容易，因为具体函数可能是很复杂的，Hessian矩阵也在不断地变化，所以要说明＂扰动之后会陷在鞍点附近的概率是小概率＂这件事情并不容易。</p>
<p>作者们采取了一个很巧妙的方法：对于负特征值的那个方向，任何两个点在这两个方向上的投影的距离只要大于u/2, 那么它们中间至少有一个点能够通过多跑几步GD逃离鞍点。也就是说，会持续陷在鞍点附近的点所在的区间至多只有u那么宽！通过计算宽度，我们也就可以计算出概率的上届，说明大概率下这个SGD+GD算法能够逃离鞍点了。</p>
<hr>
<p>引自百度百科，相关知识点的简要，方便理解</p>
<blockquote>
<p>凸集：具体地说，在欧氏空间中，凸集是对于集合内的每一对点，连接该对点的直线段上的每个点也在该集合内。</p>
</blockquote>
<p>倘若出现中空或凹陷如此这般，为非凸集。</p>
<blockquote>
<p>凸函数：大陆数学界某些机构关于函数凹凸性定义和国外的定义是相反的。Convex Function在某些中国大陆的数学书中指凹函数。Concave Function指凸函数。举个例子，同济大学高等数学教材对函数的凹凸性定义与此时我们讲的相反，这里的凹凸性是指其上方图是凸集，而同济大学高等数学教材则是指其下方图是凸集，两者定义正好相反。</p>
<p>凸优化：研究定义于凸集中的凸函数最小化的问题。</p>
</blockquote>
<p>额外强调一点：在凸优化中局部最优值必定是全局最优值。</p>
<blockquote>
<p>Hessian Matrix：是一个多元函数的二阶偏导数构成的方阵，描述了函数的局部曲率。常用于牛顿法解决优化问题，利用它可判定多元函数的极值问题。在工程实际问题的优化设计中，所列的目标函数往往很复杂，为了使问题简化，常常将目标函数在某点邻域展开成泰勒多项式来逼近原函数，此时函数在某点泰勒展开式的矩阵形式中会涉及到。</p>
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/22/tensorflow1-0-多元线性回归/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/03/22/tensorflow1-0-多元线性回归/" class="post-title-link" itemprop="url">tensorflow1.0 多元线性回归</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-03-22 00:04:12 / 修改时间：00:07:03" itemprop="dateCreated datePublished" datetime="2020-03-22T00:04:12+08:00">2020-03-22</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>上一篇说到了单变量的线性回归，这次接着来说多元线性回归的问题。定义如下不必多说：</p>
<blockquote>
<p>在回归分析中，如果有两个或两个以上的自变量，就称为多元回归。事实上，一种现象常常是与多个因素相联系的，由多个自变量的最优组合共同来预测或估计因变量，比只用一个自变量进行预测或估计更有效，更符合实际。因此多元线性回归比一元线性回归的实用意义更大。<br>多元线性回归与一元线性回归类似，可以用最小二乘法估计模型参数，也需对模型及模型参数进行统计检验。</p>
</blockquote>
<p>社会经济现象的变化往往受到多个因素的影响，正好可以从UCI上获取到波士顿房价的数据集，于1978年开始统计，包括506个样本，每个样本包括12个特征变量和该地区的平均房价。虽然是有些年代的数据，但不影响我们的学习使用。<br>房价（单价）显然和多个特征变量相关，不是单变量线性回归（一元线性回归）问题，选择多个特征变量来建立线性方程，这就是多变量线性回归（多元线性回归）问题。<br>数据节选如下：<br><img src="https://img-blog.csdnimg.cn/20200321231046193.PNG" alt="在这里插入图片描述"><br>各列特征数据的含义，分别为：</p>
<ul>
<li>CRIM: 城镇人均犯罪率                                                        </li>
<li>AGE: 1940年之前建成的自用房屋比例</li>
<li>ZN：住宅用地超过 25000 sq.ft. 的比例</li>
<li>DIS：到波士顿5个中心区域的加权距离</li>
<li>INDUS: 城镇非零售商用土地的比例                               </li>
<li>RAD: 辐射性公路的靠近指数</li>
<li>CHAS: 边界是河流为1，否则0                                    </li>
<li>TAX: 每10000美元的全值财产税率</li>
<li>NOX: 一氧化氮浓度                                     </li>
<li>PTRATIO: 城镇师生比例</li>
<li>RM: 住宅平均房间数</li>
<li>LSTAT: 人口中地位低下者的比例</li>
<li>MEDV: 自住房的平均房价，单位：千美元</li>
</ul>
<p>对于模型的理解需要一定的矩阵代数知识，在此不多赘述。</p>
<p>下面来进行基于tensorflow1.0的实战：</p>
<p>首先导入相关的包以及使用pandas读取csv数据。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib notebook</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据文件</span></span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">"data/boston.csv"</span>, header=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#显示数据摘要描述信息</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (df.describe())</span><br></pre></td></tr></table></figure></p>
<p>载入数据，归一化<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取df的值</span></span><br><span class="line">df = df.values</span><br><span class="line">print(df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把 df 转换为 np 的数组格式</span></span><br><span class="line">df = np.array(df)</span><br><span class="line">print(df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对特征数据 【0到11】列 做（0-1）归一化</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">12</span>):</span><br><span class="line">    df[:,i]=df[:,i]/(df[:,i].max()-df[:,i].min())</span><br><span class="line">    </span><br><span class="line"><span class="comment"># x_data 为 归一化后的前12列特征数据</span></span><br><span class="line">x_data = df[:,:<span class="number">12</span>] </span><br><span class="line"></span><br><span class="line"><span class="comment"># y_data 为最后1列标签数据</span></span><br><span class="line">y_data = df[:,<span class="number">12</span>]</span><br></pre></td></tr></table></figure></p>
<p>进行数据归一化后可以加快梯度下降求解最优解的速度，也可以提高精度。</p>
<p>定义训练数据占位符，定义模型结构</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>,<span class="number">12</span>], name = <span class="string">"X"</span>) <span class="comment"># 12个特征数据（12列）</span></span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="keyword">None</span>,<span class="number">1</span>], name = <span class="string">"Y"</span>) <span class="comment"># 1个标签数据（1列）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义了一个命名空间</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"Model"</span>):</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># w 初始化值为shape=(12,1)的随机数</span></span><br><span class="line">    w = tf.Variable(tf.random_normal([<span class="number">12</span>,<span class="number">1</span>], stddev=<span class="number">0.01</span>), name=<span class="string">"W"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># b 初始化值为 1.0</span></span><br><span class="line">    b = tf.Variable(<span class="number">1.0</span>, name=<span class="string">"b"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># w和x是矩阵相乘，用matmul,不能用mutiply或者*</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(x, w, b)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tf.matmul(x, w) + b</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测计算操作，前向计算节点</span></span><br><span class="line">    pred= model(x, w, b)</span><br></pre></td></tr></table></figure>
<p>TensorFlow中的命名空间（name_scope），Tensorflow中常有数以千计节点，在可视化过程中很难一下子全部展示出来，因此可用name_scope为变量划分范围，在可视化中，这表示在计算图中的一个层级。</p>
<p>设置训练超参数，定义均方差损失函数，创建优化器，初始化变量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 迭代轮次</span></span><br><span class="line">train_epochs = <span class="number">50</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 学习率</span></span><br><span class="line">learning_rate = <span class="number">0.01</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"LossFunction"</span>):</span><br><span class="line">    loss_function = tf.reduce_mean(tf.pow(y-pred, <span class="number">2</span>)) <span class="comment">#均方误差</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建优化器</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_function)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="comment"># 定义初始化变量的操作</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">sess.run(init)</span><br></pre></td></tr></table></figure>
<p>训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range (train_epochs):</span><br><span class="line">    loss_sum = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> xs, ys <span class="keyword">in</span> zip(x_data, y_data):   </span><br><span class="line"></span><br><span class="line">        xs = xs.reshape(<span class="number">1</span>,<span class="number">12</span>)</span><br><span class="line">        ys = ys.reshape(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        _, summary_str, loss = sess.run([optimizer,sum_loss_op,loss_function], feed_dict=&#123;x: xs, y: ys&#125;) </span><br><span class="line"></span><br><span class="line">        writer.add_summary(summary_str, epoch)</span><br><span class="line">        loss_sum = loss_sum + loss</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 打乱数据顺序，避免过拟合</span></span><br><span class="line">    xvalues, yvalues = shuffle(x_data, y_data)</span><br><span class="line">    </span><br><span class="line">    b0temp=b.eval(session=sess)</span><br><span class="line">    w0temp=w.eval(session=sess)</span><br><span class="line">    loss_average = loss_sum/len(y_data)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"epoch="</span>, epoch+<span class="number">1</span>,<span class="string">"loss="</span>, loss_average,<span class="string">"b="</span>, b0temp,<span class="string">"w="</span>, w0temp )</span><br></pre></td></tr></table></figure>
<p>预测：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">n=<span class="number">348</span>                     <span class="comment">#指定一条数据来看效果</span></span><br><span class="line"></span><br><span class="line">x_test = x_data[n]</span><br><span class="line"></span><br><span class="line">x_test = x_test.reshape(<span class="number">1</span>,<span class="number">12</span>)</span><br><span class="line">predict = sess.run(pred, feed_dict=&#123;x: x_test&#125;)</span><br><span class="line">print(<span class="string">"预测值：%f"</span> % predict)</span><br><span class="line"></span><br><span class="line">target = y_data[n]</span><br><span class="line">print(<span class="string">"标签值：%f"</span> % target)</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/14/tensorflow1-0-线性回归实战/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/03/14/tensorflow1-0-线性回归实战/" class="post-title-link" itemprop="url">tensorflow1.0 线性回归实战</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-03-14 23:49:56 / 修改时间：23:50:22" itemprop="dateCreated datePublished" datetime="2020-03-14T23:49:56+08:00">2020-03-14</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>一个简单的线性回归案例，是一簇点，很容易想到用一条直线去拟合它，所以我们也会希望机器能用y=w</em>x+b这条直线去对其进行拟合，也可以说是去让机器学习w和b的值。</p>
<h2 id="损失"><a href="#损失" class="headerlink" title="损失"></a>损失</h2><p>损失是对糟糕预测的惩罚，损失是一个数值，表示对于单个样本而言模型预测的准确程度。<br>如果模型的预测完全准确，则损失为零，否则损失会较大。<br>训练模型的目标是从所有样本中找到一组平均损失“较小”的权重（w）和偏差（b）。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>损失函数也称为代价函数是将随机事件或其有关随机变量的取值映射为非负实数以表示该随机事件的“风险”或“损失”的函数。<br>下面介绍三个比较常见的损失函数：</p>
<blockquote>
<p>L1损失<br>L1范数损失函数，也被称为最小绝对值偏差（LAD），最小绝对值误差（LAE）。总的说来，它是把目标值（Yi）与估计值（f(xi)）的绝对差值的总和（S）最小化：$S=\sum_{i=1}^n|Y_i-f(x_i)|$</p>
<p>L2范数损失函数，也被称为最小平方误差（LSE）。总的来说，它是把目标值（Yi）与估计值（f(xi)）的差值的平方和（S）最小化：<br>$S=\sum_{i=1}^n(Y_i-f(x_i))^2$</p>
<p>均方误差 (MSE)<br>均方误差 (MSE) 指的是每个样本的平均平方损失<br>$MSE=\frac{1}{N}\sum_{(x,y)\in D}(y-prediction(x))^2$</p>
</blockquote>
<h2 id="模型训练与降低损失"><a href="#模型训练与降低损失" class="headerlink" title="模型训练与降低损失"></a>模型训练与降低损失</h2><p>迭代:首先我们先对模型的中的权重w和偏差b进行猜测，然后将特征点输入，执行预测和推理（Inference），将计算出的值和该样本的标签值进行对比，计算出损失值，我们的目标是使推理的值和标签值的差距越小越好，也就是损失的值越小越好，所以需要不断对计算参数进行更新，直到损失值尽可能地最低为止。</p>
<p>收敛:在学习优化过程中，机器学习系统将根据所有标签去重新评估所有特征，为损失函数生成一个新值，而该值又产生新的参数值。通常，可以不断迭代，直到总体损失不再变化或至少变化极其缓慢为止。这时候，我们可以说该模型已收敛。<br><em>易知该线性回归问题产生的损失与权重的关系图为如下图</em><br><img src="https://img-blog.csdnimg.cn/20200314233427520.png" alt=""></p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>梯度下降法的基本思想可以类比为一个下山的过程，假设一个人在山上，此时他要以最快的速度下山，就需要梯度下降来帮助自己下山。具体来说，就是以自己现在所处的位置为基准，寻找一个山势最陡峭的方向，沿着高度下降的方向走，就能以最快速度到山底。</p>
<p>同理，将上一节所提到的损失函数看作一座山，我们的目标就是找到这个损失函数的最小值（山底），那么我们就可以在初始点找到该点函数的梯度，沿着函数值下降的方向对参数进行更新，这就是梯度下降法。</p>
<h2 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h2><p>那么沿着负梯度方向进行下一步探索，前进多少才合适呢？这时我们就要引入学习率的概念了。用梯度乘以一个称为学习率（有时也称为步长）的标量，以确定下一个点的位置。例如：如果梯度大小为 2.5，学习速率为 0.01，则梯度下降法算法会选择距离前一个点 0.025 的位置作为下一个点。</p>
<p>所以学习率是指导我们该如何通过损失函数的梯度调整网络权重的一个参数（也成为超参数）。学习率越低，损失函数的变化速度就越慢。</p>
<h2 id="Tensorflow实战"><a href="#Tensorflow实战" class="headerlink" title="Tensorflow实战"></a>Tensorflow实战</h2><p>本例通过生成人工数据集。随机生成一个近似采样随机分布，使得w=2.0, b=1, 并加入一个噪声，噪声的最大振幅为0.4。</p>
<p>下面我们来展示具体代码，假设我们要学习的函数为线性函数 𝑦=2𝑥+1。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在Jupyter中，使用matplotlib显示图像需要设置为 inline 模式，否则不会现实图像</span></span><br><span class="line">%matplotlib inline</span><br><span class="line">​</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># 载入matplotlib</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># 载入numpy</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf <span class="comment"># 载入Tensorflow</span></span><br><span class="line">​</span><br><span class="line">np.random.seed(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>我们需要构造满足这个函数的 𝑥 和 𝑦 同时加入一些不满足方程的噪声。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#直接采用np生成等差数列的方法，生成100个点，每个点的取值在-1~1之间</span></span><br><span class="line">x_data = np.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">100</span>) </span><br><span class="line"><span class="comment"># y = 2x +1 + 噪声， 其中，噪声的维度与x_data一致</span></span><br><span class="line">y_data = <span class="number">2</span> * x_data + <span class="number">1.0</span> + np.random.randn(*x_data.shape) * <span class="number">0.4</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#画出随机生成数据的散点图</span></span><br><span class="line">plt.scatter(x_data, y_data) </span><br><span class="line"><span class="comment"># 画出我们想要学习到的线性函数 y = 2x +1</span></span><br><span class="line">plt.plot (x_data, <span class="number">2</span> * x_data + <span class="number">1.0</span>, color = <span class="string">'red'</span>,linewidth=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>上面的代码产生了随机的-1到1的100个点，我们使用matplotlib库将这些点和要学习得到的线性函数可视化出来。</p>
<p>首先我们定义训练数据的占位符，这是后面数据输入的入口，x是特征值，y是标签值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义训练数据的占位符，x是特征值，y是标签值</span></span><br><span class="line">​</span><br><span class="line">x = tf.placeholder(<span class="string">"float"</span>, name = <span class="string">"x"</span>) </span><br><span class="line">y = tf.placeholder(<span class="string">"float"</span>, name = <span class="string">"y"</span>)</span><br></pre></td></tr></table></figure>
<p>然后我们定义模型函数，在本例中是个简单的线性函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(x, w, b)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.multiply(x, w) + b</span><br></pre></td></tr></table></figure>
<p>接下来我们创建模型的变量，Tensorflow变量的声明函数是tf.Variable，tf.Variable的作用是保存和更新参数，变量的初始值可以是随机数、常数或是通过其他变量的初始值计算得到。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建线性函数的斜率，变量w</span></span><br><span class="line">w = tf.Variable(<span class="number">1.0</span>, name=<span class="string">"w0"</span>)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 构建线性函数的截距，变量b</span></span><br><span class="line">b = tf.Variable(<span class="number">0.0</span>, name=<span class="string">"b0"</span>)   </span><br><span class="line">​</span><br><span class="line"><span class="comment"># pred是预测值，前向计算</span></span><br><span class="line">pred = model(x, w, b)</span><br></pre></td></tr></table></figure>
<p>定义一些超参数，包括训练的轮数和学习率。其中如果学习率设置过大，可能导致参数在极值附近来回摇摆，无法保证收敛。如果学习率设置过小，虽然能保证收敛，但优化速度会大大降低，我们需要更多迭代次数才能达到较理想的优化效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 迭代次数（训练轮数）</span></span><br><span class="line">train_epochs = <span class="number">10</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># 学习率</span></span><br><span class="line">learning_rate = <span class="number">0.05</span> </span><br><span class="line">​</span><br><span class="line"><span class="comment"># 控制显示loss值的粒度</span></span><br><span class="line">display_step = <span class="number">10</span></span><br></pre></td></tr></table></figure>
<p>定义损失函数和优化器。损失函数用于描述预测值与真实值之间的误差，从而指导模型收敛方向。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 采用均方差作为损失函数</span></span><br><span class="line">loss_function = tf.reduce_mean(tf.square(y-pred))  </span><br><span class="line">​</span><br><span class="line"><span class="comment"># 梯度下降优化器</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_function)</span><br></pre></td></tr></table></figure>
<p>声明会话及初始化。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">​</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">​</span><br><span class="line">sess.run(init)</span><br></pre></td></tr></table></figure></p>
<p>模型训练阶段，设置迭代轮次，每次通过将样本逐个输入模型，进行梯度下降优化操作，每轮迭代后，绘制出模型曲线。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开始训练，轮数为 epoch，采用SGD随机梯度下降优化方法</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(train_epochs):</span><br><span class="line">    <span class="keyword">for</span> xs,ys <span class="keyword">in</span> zip(x_data, y_data):</span><br><span class="line">        _, loss=sess.run([optimizer,loss_function], feed_dict=&#123;x: xs, y: ys&#125;) </span><br><span class="line">                </span><br><span class="line">    b0temp=b.eval(session=sess)</span><br><span class="line">    w0temp=w.eval(session=sess)</span><br><span class="line">    plt.plot (x_data, w0temp * x_data + b0temp  )<span class="comment"># 画图</span></span><br></pre></td></tr></table></figure>
<p>从上图可以看出，本案例所拟合的模型较简单，训练3次之后已经接近收敛 对于复杂模型，需要更多次训练才能收敛。</p>
<p>预测：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x_test = <span class="number">3.21</span></span><br><span class="line">​</span><br><span class="line">predict = sess.run(pred, feed_dict=&#123;x: x_test&#125;)</span><br><span class="line">print(<span class="string">"预测值：%f"</span> % predict)</span><br><span class="line">​</span><br><span class="line">target = <span class="number">2</span> * x_test + <span class="number">1.0</span></span><br><span class="line">print(<span class="string">"目标值：%f"</span> % target)</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/08/tensorflow1-0中conv2的细节/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/03/08/tensorflow1-0中conv2的细节/" class="post-title-link" itemprop="url">tensorflow1.0中conv2的细节</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-03-08 01:46:44 / 修改时间：01:47:25" itemprop="dateCreated datePublished" datetime="2020-03-08T01:46:44+08:00">2020-03-08</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>对比2.0可以知道tensorflow1.0对于网络的搭建更复杂一些，因此细节上的容易出现差错，在此总结一下使用conv2d的一些小问题。</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.conv2d（input, filter, strides, padding, use_cudnn_on_gpu=<span class="keyword">None</span>,</span><br><span class="line">                data_format=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>先来看一下各个参数的意义。</p>
<ul>
<li><strong>input</strong>：需要做卷积的输入数据。注意：这是一个4维的张量（[batch, in_height, in_width,in_channels]）。对于图像数据来说，batch是这一批样本的个数，in_height和in_width是图像的尺寸，in_channels是图像的通道数，而且要求图像的类型为float32或float64。因此，我们在对图像进行处理的时候，首先要把图像转换成这种特定的类型。</li>
<li><strong>filter</strong>：卷积核。这也是一个4维的张量（[filter_height, filter_width, in_channels,out_channels]）。filter_height,和filter_width是图像的尺寸，in_channels,是输入的通道数，out_channels是输出的通道数。</li>
<li><strong>strides</strong>：图像每一维的步长。是一个一维向量，长度为4。</li>
<li><strong>padding</strong>：定义元素边框与元素内容之间的空间。这里只能选择”SAME”或”VALID”，这个值决定了不同的卷积方式。当它为”SAME”时，表示边缘填充，适用于全尺寸操作；当它为”VALID”时，表示边缘不填充。</li>
<li><strong>use_cudnn_on_gpu</strong>：bool类型，是否使用cudnn加速。</li>
<li><strong>name</strong>：该操作的名称。</li>
<li><p>返回值：返回一个张量（tensor），即特征图（feature map）。</p>
<p>需要额外注意一点就是dataformat，关系到网络的output的排列方式，以及下一层的对接工作能否正确完成,它有两个选项，<strong>NHWC</strong>以及<strong>NCHW</strong>，前者为默认值。设置为 “NHWC” 时，排列顺序为 [batch, height, width, channels]。N是说这批图片有几张，H和W描述图像size，C是通道数（黑白图C=1，RBG图C=3）。以RGB为例，直观来说如下：<br><img src="https://img-blog.csdnimg.cn/20200308010449539.png" alt="在这里插入图片描述"><br>这里以灰度计算为例，说明各自的优劣。<br>对NCHW进行计算的时候，对将分成三个独立通道分别计算，，即全红一组，全绿一组这样。而NHWC得排列方式，是单个的三个相邻通道为一组计算。两者计算成本相同。我们可以知道，这样的话NHWC的局部访问存储性能更好（每三个输入像素即可得到一个输出像素）。NCHW 则必须等所有通道输入准备好才能得到最终输出结果，需要占用较大的临时空间。简单来说，就是想得到某个或某些独立像素像素的灰度计算结果，NCHW需要将全部图片计算出来，再取出特定的计算结果，而NHWC可以直接一个像素一个像素的得到结果。</p>
<p>在 CNN 中常常见到 1x1 卷积（例如：用于移动和嵌入式视觉应用的 MobileNets），也是每个输入 channel 乘一个权值，然后将所有 channel 结果累加得到一个输出 channel。如果使用 NHWC 数据格式，可以将卷积计算简化为矩阵乘计算，即 1x1 卷积核实现了每个输入像素组到每个输出像素组的线性变换。</p>
</li>
</ul>
<p>TensorFlow 为什么选择 NHWC 格式作为默认格式？因为早期开发都是基于 CPU，使用 NHWC 比 NCHW 稍快一些（不难理解，NHWC 局部性更好，cache 利用率高）。而NCHW 则是 Nvidia cuDNN 默认格式，使用 GPU 加速时用 NCHW 格式速度会更快（个别情况例外）。<br>所以设计网络的时候，需要根据具体的实践环境进行切换。</p>
<p><em>再回过头来看看conv2d的使用例子</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># 实践基于1.0</span></span><br><span class="line"> <span class="comment"># 2.0想实现请使用这段替换第一行：</span></span><br><span class="line"> <span class="comment"># import tensorflow.compat.v1 as tf</span></span><br><span class="line"> <span class="comment"># tf.disable_eager_execution()</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">input_data = tf.Varible(np.random.rand(<span class="number">10</span>,<span class="number">9</span>,<span class="number">9</span>,<span class="number">4</span>),dtype=np.float32)</span><br><span class="line">filter_data = tf.Varible(np.random.rand(<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">2</span>),dtype=np.float32)</span><br><span class="line">y = tf.nn.conv2d(input_data,filter_data,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">'VALID'</span>)</span><br><span class="line"></span><br><span class="line">print(input_data)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Tensor(“Variable/read:0”,shape=(10,9,9,4),dtype=float32)<br>Tensor(“Conv2D:0”,shape=(10,7,7,2),dtype=float32)</p>
</blockquote>
<p>导入所需要的库，然后我们定义需要做卷积的输入以及卷积核，这里的步长为1，padding为”VALID”。<br>我们可以看到，原本输入的shape是（10，9，9，4），由于padding为”VALID”,不对图像的边缘进行填充，所以在进行卷积之后，图像的尺寸发生了改变。<br>如果将padding改为”SAME”，图像的尺寸不变。</p>
<hr>
<p>2.0中keras的封装十分完善，对于使用来说比较友好，但是我们也更应该关注被封装一场隐藏起来的环节究竟有哪些细节在发生，在学习1.0的过程中我们会有不少收获。顺便<a href="https://minghuiwu.gitbook.io/tfbook/" target="_blank" rel="noopener">这里有一本1.0的开源书籍，对初学者十分友好，大家可以去看看</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/29/keras-layers-核心网络层摘要/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/02/29/keras-layers-核心网络层摘要/" class="post-title-link" itemprop="url">keras.layers--核心网络层摘要</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-02-29 23:20:58 / 修改时间：23:21:35" itemprop="dateCreated datePublished" datetime="2020-02-29T23:20:58+08:00">2020-02-29</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>完成一定实践后仔细阅读keras文档，做了关于核心网络层的一些摘要，主要汇总一些常用的网络层及其使用指南，大都在实践中使用过。</p>
<hr>
<p><strong>Dense</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Dense(units, activation=<span class="keyword">None</span>, use_bias=<span class="keyword">True</span>,</span><br><span class="line">                 kernel_initializer=<span class="string">'glorot_uniform'</span>, </span><br><span class="line">                 bias_initializer=<span class="string">'zeros'</span>, kernel_regularizer=<span class="keyword">None</span>, </span><br><span class="line">                 bias_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>, </span><br><span class="line">                 kernel_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>简要</li>
</ul>
<blockquote>
<p>全连接层。</p>
<p><em>output = activation(dot(input, kernel) + bias)</em>  其中 <em>activation</em> 是按逐个元素计算的激活函数，<em>kernel</em> 是由网络层创建的权值矩阵，以及 <em>bias</em> 是其创建的偏置向量 (只在 <em>use_bias</em><br>为 <em>True</em> 时才有用)。</p>
<p>如果该层的输入的秩大于2，那么它首先被展平然后 再计算与 kernel 的点乘。</p>
</blockquote>
<ul>
<li>重要参数</li>
</ul>
<blockquote>
<p>units: 正整数，输出空间维度。<br>activation: 激活函数。若不指定，则不使用激活函数 (即「线性」激活: a(x)=x。<br>use_bias: 布尔值，该层是否使用偏置向量。</p>
</blockquote>
<ul>
<li>输入输出</li>
</ul>
<blockquote>
<p>输入尺寸：<br>nD 张量，尺寸: (batch_size, …, input_dim)。 最常见的情况是一个尺寸为 (batch_size, input_dim) 的 2D 输入。<br>输出尺寸：<br>nD 张量，尺寸: (batch_size, …, units)。 例如，对于尺寸为 (batch_size, input_dim) 的 2D 输入， 输出的尺寸为 (batch_size, units)。</p>
</blockquote>
<hr>
<p><strong>Activation</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Activation(activation)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>摘要</p>
<blockquote>
<p>激活函数可单独作为一层，也可以嵌入在Dense层中，发挥作用。</p>
</blockquote>
</li>
<li><p>输入输出</p>
<blockquote>
<p>输入尺寸：<br>任意尺寸。 当使用此层作为模型中的第一层时， 使用参数 input_shape （整数元组，不包括样本数的轴）。<br>输出尺寸：<br>与输入相同。</p>
</blockquote>
</li>
</ul>
<hr>
<p><strong>Dropout</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Dropout(rate, noise_shape=<span class="keyword">None</span>, seed=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>摘要<blockquote>
<p>Dropout 包括在训练中每次更新时， 将输入单元的按比率随机设置为 0， 这有助于防止过拟合。随机舍去一定的神经元，来降低过拟合程度。</p>
</blockquote>
</li>
<li>重要参数<blockquote>
<p>rate: 在 0 和 1 之间浮动。需要丢弃的输入比例。<br>noise_shape: 1D 整数张量， 表示将与输入相乘的二进制 dropout 掩层的形状。 例如，如果你的输入尺寸为 (batch_size, timesteps, features)，然后 你希望 dropout 掩层在所有时间步都是一样的， 你可以使用 noise_shape=(batch_size, 1, features)。<br>seed: 一个作为随机种子的 Python 整数。</p>
</blockquote>
</li>
</ul>
<hr>
<p><strong>Flatten</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Flatten(data_format=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>摘要<blockquote>
<p>将输入展平。不影响批量大小。</p>
</blockquote>
</li>
<li>重要参数<blockquote>
<p>data_format：一个字符串，其值为 channels_last（默认值）或者 channels_first。它表明输入的维度的顺序。此参数的目的是当模型从一种数据格式切换到另一种数据格式时保留权重顺序。channels_last 对应着尺寸为 (batch, …, channels) 的输入，而 channels_first 对应着尺寸为 (batch, channels, …) 的输入。默认为 image_data_format 的值，你可以在 Keras 的配置文件 ~/.keras/keras.json 中找到它。如果你从未设置过它，那么它将是 channels_last</p>
</blockquote>
</li>
</ul>
<hr>
<p><strong>Lambda</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Lambda(function, output_shape=<span class="keyword">None</span>, mask=<span class="keyword">None</span>, arguments=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure></p>
<ul>
<li>摘要<blockquote>
<p>将任意表达式封装为 Layer 对象。</p>
</blockquote>
</li>
<li>重要参数<blockquote>
<p>function: 需要封装的函数。 将输入张量作为第一个参数。<br>output_shape: 预期的函数输出尺寸。 只在使用 Theano 时有意义。 可以是元组或者函数。 如果是元组，它只指定第一个维度； 样本维度假设与输入相同： output_shape = (input_shape[0], ) + output_shape 或者，输入是 None 且样本维度也是 None： output_shape = (None, ) + output_shape 如果是函数，它指定整个尺寸为输入尺寸的一个函数： output_shape = f(input_shape)<br>arguments: 可选的需要传递给函数的关键字参数。</p>
</blockquote>
</li>
<li>输入输出<blockquote>
<p>输入尺寸：<br>任意。当使用此层作为模型中的第一层时， 使用参数 input_shape （整数元组，不包括样本数的轴）。<br>输出尺寸：<br>由 output_shape 参数指定 (或者在使用 TensorFlow 时，自动推理得到)。</p>
</blockquote>
</li>
</ul>
<hr>
<p><strong>Conv2D</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Conv2D(filters, kernel_size, strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">                    padding=<span class="string">'valid'</span>, data_format=<span class="keyword">None</span>,</span><br><span class="line">                    dilation_rate=(<span class="number">1</span>, <span class="number">1</span>), activation=<span class="keyword">None</span>,</span><br><span class="line">                    use_bias=<span class="keyword">True</span>, kernel_initializer=<span class="string">'glorot_uniform'</span>,</span><br><span class="line">                    bias_initializer=<span class="string">'zeros'</span>, kernel_regularizer=<span class="keyword">None</span>,</span><br><span class="line">                    bias_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>,</span><br><span class="line">                    kernel_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>摘要<blockquote>
<p>2D 卷积层 (例如对图像的空间卷积)。<br>该层创建了一个卷积核， 该卷积核对层输入进行卷积， 以生成输出张量。 如果 use_bias 为 True， 则会创建一个偏置向量并将其添加到输出中。 最后，如果 activation 不是 None，它也会应用于输出。<br>当使用该层作为模型第一层时，需要提供 input_shape 参数 （整数元组，不包含样本表示的轴），例如， input_shape=(128, 128, 3) 表示 128x128 RGB 图像， 在 data_format=”channels_last” 时。</p>
</blockquote>
</li>
<li><p>重要参数</p>
<blockquote>
<p>filters: 整数，输出空间的维度 （即卷积中滤波器的输出数量）。<br>kernel_size: 一个整数，或者 2 个整数表示的元组或列表， 指明 2D 卷积窗口的宽度和高度。 可以是一个整数，为所有空间维度指定相同的值。<br>strides: 一个整数，或者 2 个整数表示的元组或列表， 指明卷积沿宽度和高度方向的步长。 可以是一个整数，为所有空间维度指定相同的值。 指定任何 stride 值 != 1 与指定 dilation_rate 值 != 1 两者不兼容。<br>padding: “valid” 或 “same” (大小写敏感)。</p>
</blockquote>
</li>
<li><p>输入输出</p>
<blockquote>
<p>输入尺寸：<br>如果 data_format=’channels_first’， 输入 4D 张量，尺寸为 (samples, channels, rows, cols)。<br>如果 data_format=’channels_last’， 输入 4D 张量，尺寸为 (samples, rows, cols, channels)。<br>输出尺寸：<br>如果 data_format=’channels_first’， 输出 4D 张量，尺寸为 (samples, filters, new_rows, new_cols)。<br>如果 data_format=’channels_last’， 输出 4D 张量，尺寸为 (samples, new_rows, new_cols, filters)。</p>
</blockquote>
</li>
</ul>
<hr>
<p><strong>MaxPooling2D</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">                          strides=<span class="keyword">None</span>, padding=<span class="string">'valid'</span>,</span><br><span class="line">                          data_format=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>摘要<blockquote>
<p>对于空间数据的最大池化。</p>
</blockquote>
</li>
<li>重要参数<blockquote>
<p>pool_size: 整数，或者 2 个整数表示的元组， 沿（垂直，水平）方向缩小比例的因数。 （2，2）会把输入张量的两个维度都缩小一半。 如果只使用一个整数，那么两个维度都会使用同样的窗口长度。</p>
</blockquote>
</li>
</ul>
<p><strong>*AveragePooling2D</strong>与之相似不过完成的是平均池化操作。*</p>
<hr>
<p><strong>SimpleRNN</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.SimpleRNN(units, activation=<span class="string">'tanh'</span>, use_bias=<span class="keyword">True</span>,</span><br><span class="line">                       kernel_initializer=<span class="string">'glorot_uniform'</span>,</span><br><span class="line">                       recurrent_initializer=<span class="string">'orthogonal'</span>, </span><br><span class="line">                       bias_initializer=<span class="string">'zeros'</span>, kernel_regularizer=<span class="keyword">None</span>, </span><br><span class="line">                       recurrent_regularizer=<span class="keyword">None</span>, bias_regularizer=<span class="keyword">None</span>,</span><br><span class="line">                       activity_regularizer=<span class="keyword">None</span>, kernel_constraint=<span class="keyword">None</span>, </span><br><span class="line">                       recurrent_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>, </span><br><span class="line">                       dropout=<span class="number">0.0</span>, recurrent_dropout=<span class="number">0.0</span>, </span><br><span class="line">                       return_sequences=<span class="keyword">False</span>, return_state=<span class="keyword">False</span>, </span><br><span class="line">                       go_backwards=<span class="keyword">False</span>, stateful=<span class="keyword">False</span>, unroll=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>摘要</li>
</ul>
<blockquote>
<p>全连接的 RNN，其输出将被反馈到输入。</p>
<ul>
<li>重要参数<br>input_dim: 输入的维度（整数）。 将此层用作模型中的第一层时，此参数（或者，关键字参数 input_shape）是必需的。</li>
<li>输入输出<br>输入尺寸：<br>3D 张量，尺寸为 (batch_size, timesteps, input_dim)。<br>输出尺寸：</li>
<li>如果 return_state：返回张量列表。 第一个张量为输出。剩余的张量为最后的状态， 每个张量的尺寸为 (batch_size, units)。</li>
<li>如果 return_sequences：返回 3D 张量， 尺寸为 (batch_size, timesteps, units)。</li>
<li>否则，返回尺寸为 (batch_size, units) 的 2D 张量。</li>
</ul>
<hr>
<p><strong>LSTM</strong></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.LSTM(units, activation=<span class="string">'tanh'</span>, </span><br><span class="line">                  recurrent_activation=<span class="string">'hard_sigmoid'</span>, use_bias=<span class="keyword">True</span>, </span><br><span class="line">                  kernel_initializer=<span class="string">'glorot_uniform'</span>, </span><br><span class="line">                  recurrent_initializer=<span class="string">'orthogonal'</span>, </span><br><span class="line">                  bias_initializer=<span class="string">'zeros'</span>, unit_forget_bias=<span class="keyword">True</span>, </span><br><span class="line">                  kernel_regularizer=<span class="keyword">None</span>, recurrent_regularizer=<span class="keyword">None</span>, </span><br><span class="line">                  bias_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>, </span><br><span class="line">                  kernel_constraint=<span class="keyword">None</span>, recurrent_constraint=<span class="keyword">None</span>, </span><br><span class="line">                  bias_constraint=<span class="keyword">None</span>, dropout=<span class="number">0.0</span>, recurrent_dropout=<span class="number">0.0</span>, </span><br><span class="line">                  implementation=<span class="number">1</span>, return_sequences=<span class="keyword">False</span>, </span><br><span class="line">                  return_state=<span class="keyword">False</span>, go_backwards=<span class="keyword">False</span>, stateful=<span class="keyword">False</span>, unroll=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>摘要<blockquote>
<p>长短期记忆网络层</p>
</blockquote>
</li>
</ul>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="X">
            
              <p class="site-author-name" itemprop="name">X</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">22</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              

              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">X</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v6.6.0</div>



<div class="powered-by">
  <i class="fa fa-child" font="" style="font-size:26px;"></i>
  <span id="busuanzi_container_site_uv">
    本站访客数:<span id="busuanzi_value_site_uv"></span>
  </span>
</div>

        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=6.6.0"></script>

  <script src="/js/src/motion.js?v=6.6.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.6.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.6.0"></script>



  

  


  <script src="/js/src/bootstrap.js?v=6.6.0"></script>



  



  











  





  

  

  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {equationNumbers: { autoNumber: "AMS" }}
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<style>
.MathJax_Display {
  overflow: auto hidden;
}
</style>

    
  


  

  

  

  

  

  

  

  

</body>
</html>
