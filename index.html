<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">


























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=6.6.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.6.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.6.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.6.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.6.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '6.6.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta property="og:type" content="website">
<meta property="og:title" content="X">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="X">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="X">



  <link rel="alternate" href="/atom.xml" title="X" type="application/atom+xml">




  <link rel="canonical" href="http://yoursite.com/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>X</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <a href="https://github.com/ZSDDZA" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">X</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">x</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-schedule">

    
    
    
      
    

    

    <a href="/schedule/" rel="section"><i class="menu-item-icon fa fa-fw fa-calendar"></i> <br>日程表</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-sitemap">

    
    
    
      
    

    

    <a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br>站点地图</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/26/python——re模块/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/04/26/python——re模块/" class="post-title-link" itemprop="url">python——re模块</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-04-26 00:48:08 / 修改时间：00:48:47" itemprop="dateCreated datePublished" datetime="2020-04-26T00:48:08+08:00">2020-04-26</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>很久没有编写爬虫程序，不过前不久又再次上手。对于文本数据获取指定信息，或者进行清洗，正则表达式都是一个强力的工具。它的使用场景也十分多样。借此回顾总结一下，python中的re模块的主要功能与示例。</p>
<h3 id="compile"><a href="#compile" class="headerlink" title="compile"></a>compile</h3><p>re.compile()可以对传入的字符串进行编译，来返回一个目标的匹配模式，从而提高正则的效率。主要参数：</p>
<blockquote>
<p>pattern : 需要编译的字符串<br>flags : 修改匹配方式，包括以下可选：</p>
<blockquote>
<p>re.S : 使.匹配包括换行在内的所有字符<br>re.I : 使匹配对大小写不敏感<br>re.U : 根据Unicode规则解析字符，主要用于对中文匹配</p>
</blockquote>
</blockquote>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">text = <span class="string">'正则表达式（英语：Regular Expression，常简写为regex、regexp或RE），又称正则表示式、正则表示法、规则表达式、常规表示法，是计算机科学的一个概念。'</span></span><br><span class="line"><span class="comment">#  获取文本中被中文括号包含的内容</span></span><br><span class="line">pattern = re.compile(<span class="string">'（(.*)）'</span>,flags=re.U)</span><br><span class="line">pattern.findall(text)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200425233415856.png" alt="在这里插入图片描述"><br>匹配内容以列表的形式返回<br>看一下flags的作用对比：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">text = <span class="string">'正则表达式（英语：Regular Expression，常简写为regex、regexp或RE），又称正则表示式、正则表示法、规则表达式、常规表示法，是计算机科学的一个概念。'</span></span><br><span class="line"><span class="comment">#小写英文字母至少出现一次的内容</span></span><br><span class="line">pattern1 = re.compile(<span class="string">'[a-z]+'</span>)</span><br><span class="line"><span class="comment">#无视大小写</span></span><br><span class="line">pattern2 = re.compile(<span class="string">'[a-z]+'</span>,flags=re.I)</span><br><span class="line">print(pattern1.findall(text))</span><br><span class="line">print(pattern2.findall(text))</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200425234009463.png" alt="在这里插入图片描述"></p>
<h3 id="match"><a href="#match" class="headerlink" title="match"></a>match</h3><p>re.match()，以定义的正则表达式作为对目标字符串开头的匹配（对非开头部分不匹配）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">text = <span class="string">'许多程序设计语言都支持利用正则表达式进行字符串操作。'</span></span><br><span class="line">print(re.match(<span class="string">'许多'</span>,text,re.U))</span><br><span class="line">print(re.match(<span class="string">'语言'</span>,text,re.U))</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/2020042523540991.png" alt="在这里插入图片描述"><br>似乎有些鸡肋？其实和re.search()类似，熟悉re模块下的分组对于他们的使用就会有些提升，如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">text = <span class="string">'许多程序设计语言都支持利用正则表达式进行字符串操作。'</span></span><br><span class="line">print(re.match(<span class="string">'(许多).*(语言)'</span>,text,re.U))</span><br><span class="line">print(re.match(<span class="string">'(许多).*(语言)'</span>,text,re.U).group(<span class="number">0</span>))</span><br><span class="line">print(re.match(<span class="string">'(许多.*)(语言)'</span>,text,re.U).group(<span class="number">1</span>))</span><br><span class="line">print(re.match(<span class="string">'(许多)(.*语言)'</span>,text,re.U).group(<span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/2020042600143986.png" alt="在这里插入图片描述"></p>
<h3 id="search"><a href="#search" class="headerlink" title="search"></a>search</h3><p>re.search()格式类似re.match()，即三个传入参数：pattern，string，flags，但与match匹配开头不同的是，search匹配的是文中出现的第一个满足条件的字符串部分并返回，对后续的不再进行匹配。<br>例子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">text = <span class="string">'最初的正则表达式出现于理论计算机科学的自动控制理论和形式化语言理论中。'</span></span><br><span class="line">print(re.search(<span class="string">'理论'</span>,text,re.U))</span><br><span class="line">print(re.search(<span class="string">'理论'</span>,text,re.U).group())</span><br><span class="line">print(re.search(<span class="string">'(理论).*(理论)'</span>,text,re.U).group(<span class="number">0</span>))</span><br><span class="line">print(re.search(<span class="string">'(理论.*)(理论)'</span>,text,re.U).group(<span class="number">1</span>))</span><br><span class="line">print(re.search(<span class="string">'(理论)(.*理论)'</span>,text,re.U).group(<span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200426001924129.png" alt="在这里插入图片描述"></p>
<p>在前面几个例子中使用到的group()方法，是针对match或search成功匹配并返回的对象，称为match object，围绕它的常用方法如下：</p>
<blockquote>
<p>　　strat()：返回匹配开始的位置</p>
<p>　　end()：返回匹配结束的位置</p>
<p>　　group()：返回被re匹配的字符串</p>
<p>　　span()：返回一个tuple格式的对象，标记了匹配开始，结束的位置，形如(start,end)</p>
</blockquote>
<p>事实上，虽然说search只返回一个对象，但我们可以通过将正则表达式改造成若干子表达式拼接的形式，来返回多个分块的对象。</p>
<h3 id="findall"><a href="#findall" class="headerlink" title="findall"></a>findall</h3><p>re.findall()会根据传入的正则表达式部分来提取目标字符串中所有符合规则的部分，并传出为列表的形式。<br>例子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">text = <span class="string">'1940年，沃伦·麦卡洛克与Walter Pitts将神经系统中的神经元描述成小而简单的自动控制元。'</span></span><br><span class="line"></span><br><span class="line">print(re.findall(<span class="string">'..元'</span>,text))</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200426002420879.png" alt="在这里插入图片描述"><br>与前面在介绍re.compile()时对findall的用法不同，这里是re.findall(正则表达式,目标字符串)的格式，前面的是 编译好的正则模式.findall(目标字符串)，这两种格式的功能等价</p>
<h3 id="finditer"><a href="#finditer" class="headerlink" title="finditer"></a>finditer</h3><p>我们有时候会遇到这样的情况：目标字符串非常长（可能是一整篇小说），而符合我们正则表达式的目标内容也非常的多，这种时候如果沿用前面的做法使用re.findall()来一口气将所有结果提取出来保存在一个硕大的列表中，是件非常占用内存的事情，而Python中用来节省内存的生成器（generator）就派上了用场；</p>
<p>re.finditer(pattern,string,flags=0)就利用了这种机制，它构造出一个基于正则表达式pattern和目标字符串string的生成器，使得我们可以在对该生成器的循环中边循环边计算对应位置的值，即从始至终每一轮只保存了当前的位置和当前匹配到的内容，达到节省内存的作用，下面是一个简单的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">text = <span class="string">'''优美优于丑陋，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">明了优于隐晦；</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">简单优于复杂，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">复杂优于凌乱，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">扁平优于嵌套，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">稀疏优于稠密，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">可读性很重要！'''</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''构造我们的替代规则'''</span></span><br><span class="line">iterator = re.finditer(<span class="string">'(..)优于'</span>,text)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> iterator:</span><br><span class="line">    print(i.group(<span class="number">0</span>))</span><br><span class="line">    print(i.group(<span class="number">1</span>))</span><br><span class="line">    print(i.span())</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/2020042600375248.png" alt="在这里插入图片描述"></p>
<h3 id="sub"><a href="#sub" class="headerlink" title="sub"></a>sub</h3><p>类似字符串操作中的replace()，只不过replace()中只能死板地设置固定的内容作为替换项，利用re.sub(pattern,repl,string,count)则可以基于正则表达式达到灵活匹配替换内容，pattern指定了正则表达式部分，repl指定了进行替换的新内容，string指定目标字符串，count指定了替换的次数，默认全部替换，其实前一篇文章结尾处我们得到一篇干净的新闻报道就用到了这种方法，下面再举一个简单的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">text = <span class="string">'''优美优于丑陋，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">明了优于隐晦；</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">简单优于复杂，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">复杂优于凌乱，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">扁平优于嵌套，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">稀疏优于稠密，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">可读性很重要！'''</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''构造我们的替代规则'''</span></span><br><span class="line">print(re.sub(<span class="string">'优于'</span>,<span class="string">'胜过'</span>,text))</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200426003215285.png" alt="在这里插入图片描述"></p>
<h3 id="split"><a href="#split" class="headerlink" title="split"></a>split</h3><p>类似于字符串处理中的split()，re.split()在原有基础上扩充了正则表达式的功能，re.split(pattern,string,maxsplit)，其中pattern指定分隔符的正则表达式，string指定目标字符串，maxsplit指定最大分割个数，下面是一个简单的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">text = <span class="string">'''优美优于丑陋，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">明了优于隐晦，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">简单优于复杂，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">复杂优于凌乱，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">扁平优于嵌套，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">稀疏优于稠密，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">可读性很重要！'''</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''构造我们的替代规则'''</span></span><br><span class="line">print(re.split(<span class="string">'，\n\n'</span>,text))</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200426004116940.png" alt="在这里插入图片描述"><br>以上就是关于re模块的常用功能。以后有机会再写一篇结合爬虫来实践以下。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/12/初识反向传播/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/04/12/初识反向传播/" class="post-title-link" itemprop="url">初识反向传播</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-04-12 00:06:15 / 修改时间：00:06:46" itemprop="dateCreated datePublished" datetime="2020-04-12T00:06:15+08:00">2020-04-12</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h3><p>在单层感知机模型中，对于输入与输出之间的权重调整依赖于预测产生的误差，由于不含隐藏层，误差可以直接计算得到。而对于多层网络来说，由于隐藏层的存在，输入输出之间的权重变得复杂，显然直接计算并不合理，而是需要通过从输出到输入反方向逐层计算。</p>
<p>由于是从输出到输入，所以我们一定需要先有一个正向传播的过程。使得样本从输入层开始，由上至下逐层经隐节点计算处理，最终样本信息被传送到输出层节点，得到预测的结果。再根据正向传播得到的结果也就是预测经行误差计算。</p>
<p>由于有了误差和输出，以及之前传播过来的网络，就有了反向传播的原材料，可以开始进行反向传播了。反向传播无法直接计算隐节点的预测误差，但却可以利用输出节点的预测误差来逐层估计隐节点的误差，也就是将输出节点的预测误差反方向传播到上层隐节点，逐层调整连接权重，直至输入节点和隐节点的权重全部得到调整，使得网络输出值越来越逼近实际值。</p>
<h3 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h3><p>每个神经元都由两个单元组成。第一个单元的功能是把每个输入信号和对应权重系数($w_{i}$)作乘积然后对这些乘积求和。第二个单元实现了一个非线性函数。这个函数也称为神经元激活函数(neuron activation function)。如下图所示，信号$e$是加法器（即第一个单元）的输出信号，而$y = f(e)$是非线性函数（即第二个单元）的输出信号。显然，$y$也同时是这整个神经元的输出信号。<br><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2dhbGF4eS5hZ2guZWR1LnBsL352bHNpL0FJL2JhY2twX3RfZW4vYmFja3Byb3BfZmlsZXMvaW1nMDFiLmdpZg" alt="在这里插入图片描述"><br>一般这个非线性激活函数采用sigmoid函数。节点的输出被限制在0~1的范围内。对于分类问题，输出节点给出的是预测类别的概率值；对于回归问题，输出节点给出的标准化处理后的预测值，只需还原处理即可。<br>使用sigmoid的原因：</p>
<ul>
<li>在模型开始训练阶段，由于连接权重的设置要求满足均值为0的均匀分布，所以初期的连接权重在0值附近，使得加法器结果也在0附近。此时sigmoid函数的斜率近似为一个常数，输入输出间呈近似线性关系，模型较为简单；</li>
<li>随着模型训练的进行，网络权重不断调整，节点加法器的结果逐渐偏离0值，输入输出逐渐呈现出非线性关系，模型逐渐复杂起来，并且输入的变化对输出的影响程度逐渐减小</li>
<li>到模型训练的后期，节点加法器结果远离0，此时输入的变化将不再引起输出的明显变动，输出基本趋于稳定。神经网络的预测误差不再随着连接权重的调整而得到明显改善，预测结果稳定，模型训练结束。</li>
</ul>
<p>可见，sigmoid激活函数较好的体现了连接权重修正过程中，模型从近似线性到非线性的渐进转变过程。</p>
<p>除此之外，sigmoid激活函数还具有无限次可微的特点，这使得反向传播能够采用梯度下降法来挑战连接权重。</p>
<h3 id="关键过程"><a href="#关键过程" class="headerlink" title="关键过程"></a>关键过程</h3><p>由于权重的不同，会导致前置神经元对后续紧邻的后置神经元传递的误差不同，或者说对后置神经元的误差贡献度不同，所以在进行反向传播的时候，不能将误差均分到前置 的神经元，而是应该根据权值对误差进行反向的传递。<br><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2dhbGF4eS5hZ2guZWR1LnBsL352bHNpL0FJL2JhY2twX3RfZW4vYmFja3Byb3BfZmlsZXMvaW1nMDkuZ2lm" alt="在这里插入图片描述"><br><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2dhbGF4eS5hZ2guZWR1LnBsL352bHNpL0FJL2JhY2twX3RfZW4vYmFja3Byb3BfZmlsZXMvaW1nMTAuZ2lm" alt="在这里插入图片描述"><br>如果前置的神经元对后置多个神经元都传递了误差，那反向传播的时候潜质的误差需要按照权重对后置的误差求和。<br><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2dhbGF4eS5hZ2guZWR1LnBsL352bHNpL0FJL2JhY2twX3RfZW4vYmFja3Byb3BfZmlsZXMvaW1nMTEuZ2lm" alt="在这里插入图片描述"><br>当每个神经元的误差信号都已经被计算出来之后，我们就可以开始修改每个神经元输入结点的权重系数了。在下面的公式中，$\frac{\mathrm{df_1(e)} }{\mathrm{d} e}$表示对应的神经元激活函数的导数。<br><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2dhbGF4eS5hZ2guZWR1LnBsL352bHNpL0FJL2JhY2twX3RfZW4vYmFja3Byb3BfZmlsZXMvaW1nMTQuZ2lm" alt="在这里插入图片描述"><br><img src="http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop_files/img17.gif" alt="
"><br><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2dhbGF4eS5hZ2guZWR1LnBsL352bHNpL0FJL2JhY2twX3RfZW4vYmFja3Byb3BfZmlsZXMvaW1nMTkuZ2lm" alt="在这里插入图片描述"><br><em>计算是逐层的，这里只展示一条链上（$x_1—&gt;f_1{e}—&gt;f_4{e}—&gt;f_5{e}$）的计算，同层计算方式相同不多描述</em><br><em>具体的调整和计算方法涉及到<a href="https://zhuanlan.zhihu.com/p/36564434" target="_blank" rel="noopener">梯度下降算法</a></em><br>系数$\eta$会影响网络的训练速度。有几种技术可以用于选择系数$\eta$。第一种方法是以较大的参数值开始训练过程。在建立权重系数的同时，参数也逐渐减小。第二种方法更复杂，它以较小的参数值开始训练。在训练的过程中，参数增加，然后在最后阶段参数再次减小。以较小的参数值开始训练使得我们可以确定权重参数的符号。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/05/k-medoids聚类/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/04/05/k-medoids聚类/" class="post-title-link" itemprop="url">k-medoids聚类</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-04-05 01:54:17 / 修改时间：01:54:49" itemprop="dateCreated datePublished" datetime="2020-04-05T01:54:17+08:00">2020-04-05</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>我们知道对于K-means算法来说，如果数据样本中出现极端的离群值，导致样本数据分布出现一定的扭曲或者说偏离，则会导致聚类效果不好，与期望的效果之间有误差存在，也就是我们所说的极端值敏感。聚类很多情况下都是通过计算点之间的欧式距离来表现亲缘远近的，所以解决在其上产生的问题就逃不过中心点。这次介绍的k-medoids算法自然在处理一些异常值方面具有得天独厚的优势。<br>就原理而言，k-means与k-medoids区别在于中心点的选择。前者通过不断产生K个新的中心点（初始为随机，后续计算均值进行）来划分群集，以达到准则函数收敛的结果，其本质就是在不断寻找符合条件的群集重心；后者，顾名思义，不选用平均值，而是选用最中心的点，即需要计算当前簇各个点到中心点的距离。这也使得我们得到一个其与k-means的不同点，或者说k-medoids的性质，它所选取的中心点都是簇中的点。<br>不过由于算法的复杂性（需要遍历簇中所有点），导致更大的计算资源倾斜。</p>
<hr>
<p>简单复习一下顺便对比一下两者算法过程：</p>
<p>k-means：</p>
<blockquote>
<p>1） 任意选择K个对象作为初始的簇中心；<br>2） 分别计算数据集中每个元素与所选簇的中心计算距离（一般采用欧式距离），根据最近邻原则，将元素划分到相应的簇中；<br>3） 计算每个簇中对象的平均值，更新簇的中心；<br>4） 重复上面的步骤，直至更新的簇的中心与原簇的中心的差值在预定范围内，或者达到预设的迭代次数；<br>5） 输出K个簇中心。</p>
</blockquote>
<p>k-medoids:</p>
<blockquote>
<p>1、任意选取 k 个点作为 medoids<br>2、按照与medoids最近的原则，将剩余点分配到当前最佳的medoids代表的类中<br>3、在每一类中，计算每个成员点对应的准则函数，选取准则函数最小时对应的点作为新的 medoids<br>4、重复2-3的过程，直到所有的 medoids 点不再发生变化，或已达到设定的最大迭代次数<br>其中准则函数为，一类中，某个成员点和其他成员点的距离之和</p>
</blockquote>
<hr>
<p>来一点python的实战<br>由于可使用的第三方库不多，参考了很多文章和博客，找到了可用的库——pyclust，它还依赖于treelib库。<br>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyclust <span class="keyword">import</span> KMedoids</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="string">'''构造示例数据集（加入少量脏数据）'''</span></span><br><span class="line">data1 = np.random.normal(<span class="number">0</span>,<span class="number">0.9</span>,(<span class="number">1000</span>,<span class="number">10</span>))</span><br><span class="line">data2 = np.random.normal(<span class="number">1</span>,<span class="number">0.9</span>,(<span class="number">1000</span>,<span class="number">10</span>))</span><br><span class="line">data3 = np.random.normal(<span class="number">2</span>,<span class="number">0.9</span>,(<span class="number">1000</span>,<span class="number">10</span>))</span><br><span class="line">data4 = np.random.normal(<span class="number">3</span>,<span class="number">0.9</span>,(<span class="number">1000</span>,<span class="number">10</span>))</span><br><span class="line">data5 = np.random.normal(<span class="number">50</span>,<span class="number">0.9</span>,(<span class="number">50</span>,<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">data = np.concatenate((data1,data2,data3,data4,data5))</span><br><span class="line"></span><br><span class="line"><span class="string">'''准备可视化需要的降维数据'''</span></span><br><span class="line">data_TSNE = TSNE(learning_rate=<span class="number">100</span>).fit_transform(data)</span><br><span class="line"></span><br><span class="line"><span class="string">'''对不同的k进行试探性K-medoids聚类并可视化'''</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,<span class="number">6</span>):</span><br><span class="line">    k = KMedoids(n_clusters=i,distance=<span class="string">'euclidean'</span>,max_iter=<span class="number">1000</span>).fit_predict(data)</span><br><span class="line">    colors = ([[<span class="string">'red'</span>,<span class="string">'blue'</span>,<span class="string">'black'</span>,<span class="string">'yellow'</span>,<span class="string">'green'</span>][i] <span class="keyword">for</span> i <span class="keyword">in</span> k])</span><br><span class="line">    plt.subplot(<span class="number">219</span>+i)</span><br><span class="line">    plt.scatter(data_TSNE[:,<span class="number">0</span>],data_TSNE[:,<span class="number">1</span>],c=colors,s=<span class="number">10</span>)</span><br><span class="line">    plt.title(<span class="string">'K-medoids Resul of '</span>.format(str(i)))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>效果：<br><img src="https://img-blog.csdnimg.cn/20200405014833433.PNG" alt="在这里插入图片描述"></p>
<p>小结：k-medoids面对孤立的点时有更好的效果，但因此也占用了较多的计算资源，后续关于k-medoids有变种算法如：PAM算法 和 CLARA算法这里不作展开。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/29/梯度下降与脱离鞍点/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/03/29/梯度下降与脱离鞍点/" class="post-title-link" itemprop="url">梯度下降与脱离鞍点</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-03-29 00:48:13 / 修改时间：00:48:38" itemprop="dateCreated datePublished" datetime="2020-03-29T00:48:13+08:00">2020-03-29</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="直奔主题"><a href="#直奔主题" class="headerlink" title="直奔主题"></a>直奔主题</h4><p>如果要优化(找到它的最小值)一个函数$f(x)$ , 通常能够用Gradient Descent (GD), 也就是梯度下降，也称最速下降，因为梯度的本意是一个向量（矢量），表示某一函数在该点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向（此梯度的方向）变化最快，变化率最大（为该梯度的模）。<br>听着很简单就是每次沿着当前位置的导数方向走一小步, 走啊走啊就能够走到一个最小值点。<br>如图：<br><img src="https://img-blog.csdnimg.cn/20200328214122985.jpg" alt="在这里插入图片描述"></p>
<p>对应的数学语言：$x_{t+1}=x_t+γ_t\nabla f(x_t)$<br>$x_t$为第t步的位置，$γ_t$为第t步的步长，$\nabla f(x_t)$为梯度求导得到，nabla算子也可以写作$grad()$，具体可以这样表示$\nabla=\frac{\partial }{\partial x}\pmb i+\frac{\partial }{\partial y}\pmb j$。</p>
<p>在机器学习的中使用, 会面临非常大的数据集。这个时候如果硬要算$f(x)$的精确导数, 往往意味着我们要花几个小时把整个数据集都扫描一遍, 然后还只能走一小步。一般GD要几千步几万步才能收敛, 所以这样就根本跑不完了。其次, 我们可能就会不小心陷入了鞍点, 或者比较差的局部最优点。<br>局部最优解与鞍点：<br><img src="https://img-blog.csdnimg.cn/20200329001122896.jpg" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329001151684.jpg" alt="在这里插入图片描述"><br>Stochastic Gradient Descent (SGD) 算法而可以将将两个问题一并解决，<br>数学语言描述的形式：$x_{t+1}=x_t+γ_tg_t$<br>$g_t$就是随机梯度，满足约束：$E(g_t)= \nabla f(x_t)$<br>就是说虽然包含一定的随机性，但是从期望上来看，它是等于正确的导数的。其实SGD需要更多步才能够收敛的。由于它对导数的要求非常低，可以包含大量的噪声（或者说扰动），只要期望正确就行，所以导数算起来非常快。拿机器学习的例子来说，比如神经网络吧，训练的时候都是每次只从百万数据点里面拿128或者256个数据点，算一个不那么准的导数，然后用SGD走一步的。想想看，这样每次算的时间就快了10000倍，就算是多走几倍的路，算算也是挺值的了。</p>
<p>所以它可以完美解决GD的第一个问题——算得慢．这也是当初人们使用SGD的主要目的。而且，大家并不用担心导数中包含的噪声会有什么负面影响。有大量的理论工作说明，只要噪声不离谱，其实（至少在f是凸函数的情况下），SGD都能够很好地收敛。<br>再来说一下脱离鞍点的问题，导数为零的点叫Stationary points，即驻点。<br>可以是（局部）最小值，（局部）最大值，也可以是鞍点。<br>我们可以通过计算它的Hessian矩阵H来判断。</p>
<ul>
<li>如果H是负定的，说明所有的特征值都是负的．这个时候，你无论往什么方向走，导数都会变负，也就是说函数值会下降．所以，这是（局部）最大值．</li>
<li>如果H是正定的，说明所有的特征值都是正的．这个时候，你无论往什么方向走，导数都会变正，也就是说函数值会上升．所以，这是（局部）最小值．</li>
<li>如果Ｈ既包含正的特征值，又包含负的特征值，那么这个稳定点就是一个鞍点．具体参照之前的图片．也就是说有些方向函数值会上升，有些方向函数值会下降．</li>
<li>虽然看起来上面已经包含了所有的情况，但是其实不是的！还有一个非常重要的情况就是H可能包含特征值为０的情况．这种情况下面，我们无法判断稳定点到底属于哪一类，往往需要参照更高维的导数才行．想想看，如果特征值是０，就说明有些方向一马平川一望无际，函数值一直不变，那我们当然不知道是怎么回事了：）<br>今天讨论的情况只包含前三种，不包含第四种．第四种被称为退化了的情况，所以我们考虑的情况就叫做非退化情况。</li>
</ul>
<p>在这种非退化的情况下面，我们考虑一个重要的类别，即strict saddle函数．这种函数有这样的特点：对于每个点x</p>
<ul>
<li>要么x的导数比较大 </li>
<li>要么x的Hessian矩阵包含一个负的特征值 </li>
<li>要么x已经离某一个（局部）最小值很近了</li>
</ul>
<p>为什么我们要x满足这三个情况的至少一个呢？因为如果x的导数大，那么沿着这个导数一定可以大大降低函数值（我们对函数有光滑性假设）<br>如果x的Hessian矩阵有一个负的特征值，那么我们通过加噪声随机扰动，跑跑就能够跑到这个方向上，沿着这个方向就能够像滑滑梯一样一路滑下去，大大降低函数值<br>如果x已经离某一个（局部）最小值很近了，那么我们就完成任务了，毕竟这个世界上没有十全十美的事情，离得近和精确跑到这个点也没什么区别．<br>所以说，如果我们考虑的函数满足这个strict saddle性质，那么SGD算法其实是不会被困在鞍点的。那么strict saddle性质是不是一个合理的性质呢？</p>
<p>实际上，有大量的机器学习的问题使用的函数都满足这样的性质。比如Orthogonal tensor decomposition，dictionary learning, matrix completion等等。而且，其实并不用担心最后得到的点只是一个局部最优，而不是全局最优。因为实际上人们发现大量的机器学习问题，几乎所有的局部最优是几乎一样好的，也就是说，只要找到一个局部最优点，其实就已经找到了全局最优，比如Orthogonal tensor decomposition就满足这样的性质，还有NIPS16的best student paper证明了matrix completion也满足这样的性质。</p>
<p>下面讨论一下证明，主要讨论一下第二篇．第一篇论文其实就是用数学的语言在说＂在鞍点加扰动，能够顺着负的特征值方向滑下去＂．第二篇非常有意思，我觉得值得介绍一下想法。</p>
<p>首先，算法上有了一些改动．算法不再是SGD，而是跑若干步GD，然后跑一步SGD。当然实际上大家是不会这么用的，但是理论分析么，这么考虑没问题。什么时候跑SGD呢？只有当导数比较小，而且已经很长时间没有跑过SGD的时候，才会跑一次。也就是说，只有确实陷在鞍点上了，才会随机扰动一下下。</p>
<p>因为鞍点有负的特征值，所以只要扰动之后在这个方向上有那么一点点分量，就能够一马平川地滑下去。除非分量非常非常小的情况下才可能会继续陷在鞍点附近。换句话说，如果加了一个随机扰动，其实大概率情况下是能够逃离鞍点的！</p>
<p>虽然这个想法也很直观，但是要严格地证明很不容易，因为具体函数可能是很复杂的，Hessian矩阵也在不断地变化，所以要说明＂扰动之后会陷在鞍点附近的概率是小概率＂这件事情并不容易。</p>
<p>作者们采取了一个很巧妙的方法：对于负特征值的那个方向，任何两个点在这两个方向上的投影的距离只要大于u/2, 那么它们中间至少有一个点能够通过多跑几步GD逃离鞍点。也就是说，会持续陷在鞍点附近的点所在的区间至多只有u那么宽！通过计算宽度，我们也就可以计算出概率的上届，说明大概率下这个SGD+GD算法能够逃离鞍点了。</p>
<hr>
<p>引自百度百科，相关知识点的简要，方便理解</p>
<blockquote>
<p>凸集：具体地说，在欧氏空间中，凸集是对于集合内的每一对点，连接该对点的直线段上的每个点也在该集合内。</p>
</blockquote>
<p>倘若出现中空或凹陷如此这般，为非凸集。</p>
<blockquote>
<p>凸函数：大陆数学界某些机构关于函数凹凸性定义和国外的定义是相反的。Convex Function在某些中国大陆的数学书中指凹函数。Concave Function指凸函数。举个例子，同济大学高等数学教材对函数的凹凸性定义与此时我们讲的相反，这里的凹凸性是指其上方图是凸集，而同济大学高等数学教材则是指其下方图是凸集，两者定义正好相反。</p>
<p>凸优化：研究定义于凸集中的凸函数最小化的问题。</p>
</blockquote>
<p>额外强调一点：在凸优化中局部最优值必定是全局最优值。</p>
<blockquote>
<p>Hessian Matrix：是一个多元函数的二阶偏导数构成的方阵，描述了函数的局部曲率。常用于牛顿法解决优化问题，利用它可判定多元函数的极值问题。在工程实际问题的优化设计中，所列的目标函数往往很复杂，为了使问题简化，常常将目标函数在某点邻域展开成泰勒多项式来逼近原函数，此时函数在某点泰勒展开式的矩阵形式中会涉及到。</p>
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/22/tensorflow1-0-多元线性回归/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/03/22/tensorflow1-0-多元线性回归/" class="post-title-link" itemprop="url">tensorflow1.0 多元线性回归</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-03-22 00:04:12 / 修改时间：00:07:03" itemprop="dateCreated datePublished" datetime="2020-03-22T00:04:12+08:00">2020-03-22</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>上一篇说到了单变量的线性回归，这次接着来说多元线性回归的问题。定义如下不必多说：</p>
<blockquote>
<p>在回归分析中，如果有两个或两个以上的自变量，就称为多元回归。事实上，一种现象常常是与多个因素相联系的，由多个自变量的最优组合共同来预测或估计因变量，比只用一个自变量进行预测或估计更有效，更符合实际。因此多元线性回归比一元线性回归的实用意义更大。<br>多元线性回归与一元线性回归类似，可以用最小二乘法估计模型参数，也需对模型及模型参数进行统计检验。</p>
</blockquote>
<p>社会经济现象的变化往往受到多个因素的影响，正好可以从UCI上获取到波士顿房价的数据集，于1978年开始统计，包括506个样本，每个样本包括12个特征变量和该地区的平均房价。虽然是有些年代的数据，但不影响我们的学习使用。<br>房价（单价）显然和多个特征变量相关，不是单变量线性回归（一元线性回归）问题，选择多个特征变量来建立线性方程，这就是多变量线性回归（多元线性回归）问题。<br>数据节选如下：<br><img src="https://img-blog.csdnimg.cn/20200321231046193.PNG" alt="在这里插入图片描述"><br>各列特征数据的含义，分别为：</p>
<ul>
<li>CRIM: 城镇人均犯罪率                                                        </li>
<li>AGE: 1940年之前建成的自用房屋比例</li>
<li>ZN：住宅用地超过 25000 sq.ft. 的比例</li>
<li>DIS：到波士顿5个中心区域的加权距离</li>
<li>INDUS: 城镇非零售商用土地的比例                               </li>
<li>RAD: 辐射性公路的靠近指数</li>
<li>CHAS: 边界是河流为1，否则0                                    </li>
<li>TAX: 每10000美元的全值财产税率</li>
<li>NOX: 一氧化氮浓度                                     </li>
<li>PTRATIO: 城镇师生比例</li>
<li>RM: 住宅平均房间数</li>
<li>LSTAT: 人口中地位低下者的比例</li>
<li>MEDV: 自住房的平均房价，单位：千美元</li>
</ul>
<p>对于模型的理解需要一定的矩阵代数知识，在此不多赘述。</p>
<p>下面来进行基于tensorflow1.0的实战：</p>
<p>首先导入相关的包以及使用pandas读取csv数据。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib notebook</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据文件</span></span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">"data/boston.csv"</span>, header=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#显示数据摘要描述信息</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (df.describe())</span><br></pre></td></tr></table></figure></p>
<p>载入数据，归一化<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取df的值</span></span><br><span class="line">df = df.values</span><br><span class="line">print(df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把 df 转换为 np 的数组格式</span></span><br><span class="line">df = np.array(df)</span><br><span class="line">print(df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对特征数据 【0到11】列 做（0-1）归一化</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">12</span>):</span><br><span class="line">    df[:,i]=df[:,i]/(df[:,i].max()-df[:,i].min())</span><br><span class="line">    </span><br><span class="line"><span class="comment"># x_data 为 归一化后的前12列特征数据</span></span><br><span class="line">x_data = df[:,:<span class="number">12</span>] </span><br><span class="line"></span><br><span class="line"><span class="comment"># y_data 为最后1列标签数据</span></span><br><span class="line">y_data = df[:,<span class="number">12</span>]</span><br></pre></td></tr></table></figure></p>
<p>进行数据归一化后可以加快梯度下降求解最优解的速度，也可以提高精度。</p>
<p>定义训练数据占位符，定义模型结构</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>,<span class="number">12</span>], name = <span class="string">"X"</span>) <span class="comment"># 12个特征数据（12列）</span></span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="keyword">None</span>,<span class="number">1</span>], name = <span class="string">"Y"</span>) <span class="comment"># 1个标签数据（1列）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义了一个命名空间</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"Model"</span>):</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># w 初始化值为shape=(12,1)的随机数</span></span><br><span class="line">    w = tf.Variable(tf.random_normal([<span class="number">12</span>,<span class="number">1</span>], stddev=<span class="number">0.01</span>), name=<span class="string">"W"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># b 初始化值为 1.0</span></span><br><span class="line">    b = tf.Variable(<span class="number">1.0</span>, name=<span class="string">"b"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># w和x是矩阵相乘，用matmul,不能用mutiply或者*</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(x, w, b)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tf.matmul(x, w) + b</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测计算操作，前向计算节点</span></span><br><span class="line">    pred= model(x, w, b)</span><br></pre></td></tr></table></figure>
<p>TensorFlow中的命名空间（name_scope），Tensorflow中常有数以千计节点，在可视化过程中很难一下子全部展示出来，因此可用name_scope为变量划分范围，在可视化中，这表示在计算图中的一个层级。</p>
<p>设置训练超参数，定义均方差损失函数，创建优化器，初始化变量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 迭代轮次</span></span><br><span class="line">train_epochs = <span class="number">50</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 学习率</span></span><br><span class="line">learning_rate = <span class="number">0.01</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"LossFunction"</span>):</span><br><span class="line">    loss_function = tf.reduce_mean(tf.pow(y-pred, <span class="number">2</span>)) <span class="comment">#均方误差</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建优化器</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_function)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="comment"># 定义初始化变量的操作</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">sess.run(init)</span><br></pre></td></tr></table></figure>
<p>训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range (train_epochs):</span><br><span class="line">    loss_sum = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> xs, ys <span class="keyword">in</span> zip(x_data, y_data):   </span><br><span class="line"></span><br><span class="line">        xs = xs.reshape(<span class="number">1</span>,<span class="number">12</span>)</span><br><span class="line">        ys = ys.reshape(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        _, summary_str, loss = sess.run([optimizer,sum_loss_op,loss_function], feed_dict=&#123;x: xs, y: ys&#125;) </span><br><span class="line"></span><br><span class="line">        writer.add_summary(summary_str, epoch)</span><br><span class="line">        loss_sum = loss_sum + loss</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 打乱数据顺序，避免过拟合</span></span><br><span class="line">    xvalues, yvalues = shuffle(x_data, y_data)</span><br><span class="line">    </span><br><span class="line">    b0temp=b.eval(session=sess)</span><br><span class="line">    w0temp=w.eval(session=sess)</span><br><span class="line">    loss_average = loss_sum/len(y_data)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"epoch="</span>, epoch+<span class="number">1</span>,<span class="string">"loss="</span>, loss_average,<span class="string">"b="</span>, b0temp,<span class="string">"w="</span>, w0temp )</span><br></pre></td></tr></table></figure>
<p>预测：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">n=<span class="number">348</span>                     <span class="comment">#指定一条数据来看效果</span></span><br><span class="line"></span><br><span class="line">x_test = x_data[n]</span><br><span class="line"></span><br><span class="line">x_test = x_test.reshape(<span class="number">1</span>,<span class="number">12</span>)</span><br><span class="line">predict = sess.run(pred, feed_dict=&#123;x: x_test&#125;)</span><br><span class="line">print(<span class="string">"预测值：%f"</span> % predict)</span><br><span class="line"></span><br><span class="line">target = y_data[n]</span><br><span class="line">print(<span class="string">"标签值：%f"</span> % target)</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/14/tensorflow1-0-线性回归实战/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/03/14/tensorflow1-0-线性回归实战/" class="post-title-link" itemprop="url">tensorflow1.0 线性回归实战</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-03-14 23:49:56 / 修改时间：23:50:22" itemprop="dateCreated datePublished" datetime="2020-03-14T23:49:56+08:00">2020-03-14</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>一个简单的线性回归案例，是一簇点，很容易想到用一条直线去拟合它，所以我们也会希望机器能用y=w</em>x+b这条直线去对其进行拟合，也可以说是去让机器学习w和b的值。</p>
<h2 id="损失"><a href="#损失" class="headerlink" title="损失"></a>损失</h2><p>损失是对糟糕预测的惩罚，损失是一个数值，表示对于单个样本而言模型预测的准确程度。<br>如果模型的预测完全准确，则损失为零，否则损失会较大。<br>训练模型的目标是从所有样本中找到一组平均损失“较小”的权重（w）和偏差（b）。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>损失函数也称为代价函数是将随机事件或其有关随机变量的取值映射为非负实数以表示该随机事件的“风险”或“损失”的函数。<br>下面介绍三个比较常见的损失函数：</p>
<blockquote>
<p>L1损失<br>L1范数损失函数，也被称为最小绝对值偏差（LAD），最小绝对值误差（LAE）。总的说来，它是把目标值（Yi）与估计值（f(xi)）的绝对差值的总和（S）最小化：$S=\sum_{i=1}^n|Y_i-f(x_i)|$</p>
<p>L2范数损失函数，也被称为最小平方误差（LSE）。总的来说，它是把目标值（Yi）与估计值（f(xi)）的差值的平方和（S）最小化：<br>$S=\sum_{i=1}^n(Y_i-f(x_i))^2$</p>
<p>均方误差 (MSE)<br>均方误差 (MSE) 指的是每个样本的平均平方损失<br>$MSE=\frac{1}{N}\sum_{(x,y)\in D}(y-prediction(x))^2$</p>
</blockquote>
<h2 id="模型训练与降低损失"><a href="#模型训练与降低损失" class="headerlink" title="模型训练与降低损失"></a>模型训练与降低损失</h2><p>迭代:首先我们先对模型的中的权重w和偏差b进行猜测，然后将特征点输入，执行预测和推理（Inference），将计算出的值和该样本的标签值进行对比，计算出损失值，我们的目标是使推理的值和标签值的差距越小越好，也就是损失的值越小越好，所以需要不断对计算参数进行更新，直到损失值尽可能地最低为止。</p>
<p>收敛:在学习优化过程中，机器学习系统将根据所有标签去重新评估所有特征，为损失函数生成一个新值，而该值又产生新的参数值。通常，可以不断迭代，直到总体损失不再变化或至少变化极其缓慢为止。这时候，我们可以说该模型已收敛。<br><em>易知该线性回归问题产生的损失与权重的关系图为如下图</em><br><img src="https://img-blog.csdnimg.cn/20200314233427520.png" alt=""></p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>梯度下降法的基本思想可以类比为一个下山的过程，假设一个人在山上，此时他要以最快的速度下山，就需要梯度下降来帮助自己下山。具体来说，就是以自己现在所处的位置为基准，寻找一个山势最陡峭的方向，沿着高度下降的方向走，就能以最快速度到山底。</p>
<p>同理，将上一节所提到的损失函数看作一座山，我们的目标就是找到这个损失函数的最小值（山底），那么我们就可以在初始点找到该点函数的梯度，沿着函数值下降的方向对参数进行更新，这就是梯度下降法。</p>
<h2 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h2><p>那么沿着负梯度方向进行下一步探索，前进多少才合适呢？这时我们就要引入学习率的概念了。用梯度乘以一个称为学习率（有时也称为步长）的标量，以确定下一个点的位置。例如：如果梯度大小为 2.5，学习速率为 0.01，则梯度下降法算法会选择距离前一个点 0.025 的位置作为下一个点。</p>
<p>所以学习率是指导我们该如何通过损失函数的梯度调整网络权重的一个参数（也成为超参数）。学习率越低，损失函数的变化速度就越慢。</p>
<h2 id="Tensorflow实战"><a href="#Tensorflow实战" class="headerlink" title="Tensorflow实战"></a>Tensorflow实战</h2><p>本例通过生成人工数据集。随机生成一个近似采样随机分布，使得w=2.0, b=1, 并加入一个噪声，噪声的最大振幅为0.4。</p>
<p>下面我们来展示具体代码，假设我们要学习的函数为线性函数 𝑦=2𝑥+1。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在Jupyter中，使用matplotlib显示图像需要设置为 inline 模式，否则不会现实图像</span></span><br><span class="line">%matplotlib inline</span><br><span class="line">​</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># 载入matplotlib</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># 载入numpy</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf <span class="comment"># 载入Tensorflow</span></span><br><span class="line">​</span><br><span class="line">np.random.seed(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>我们需要构造满足这个函数的 𝑥 和 𝑦 同时加入一些不满足方程的噪声。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#直接采用np生成等差数列的方法，生成100个点，每个点的取值在-1~1之间</span></span><br><span class="line">x_data = np.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">100</span>) </span><br><span class="line"><span class="comment"># y = 2x +1 + 噪声， 其中，噪声的维度与x_data一致</span></span><br><span class="line">y_data = <span class="number">2</span> * x_data + <span class="number">1.0</span> + np.random.randn(*x_data.shape) * <span class="number">0.4</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#画出随机生成数据的散点图</span></span><br><span class="line">plt.scatter(x_data, y_data) </span><br><span class="line"><span class="comment"># 画出我们想要学习到的线性函数 y = 2x +1</span></span><br><span class="line">plt.plot (x_data, <span class="number">2</span> * x_data + <span class="number">1.0</span>, color = <span class="string">'red'</span>,linewidth=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>上面的代码产生了随机的-1到1的100个点，我们使用matplotlib库将这些点和要学习得到的线性函数可视化出来。</p>
<p>首先我们定义训练数据的占位符，这是后面数据输入的入口，x是特征值，y是标签值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义训练数据的占位符，x是特征值，y是标签值</span></span><br><span class="line">​</span><br><span class="line">x = tf.placeholder(<span class="string">"float"</span>, name = <span class="string">"x"</span>) </span><br><span class="line">y = tf.placeholder(<span class="string">"float"</span>, name = <span class="string">"y"</span>)</span><br></pre></td></tr></table></figure>
<p>然后我们定义模型函数，在本例中是个简单的线性函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(x, w, b)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.multiply(x, w) + b</span><br></pre></td></tr></table></figure>
<p>接下来我们创建模型的变量，Tensorflow变量的声明函数是tf.Variable，tf.Variable的作用是保存和更新参数，变量的初始值可以是随机数、常数或是通过其他变量的初始值计算得到。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建线性函数的斜率，变量w</span></span><br><span class="line">w = tf.Variable(<span class="number">1.0</span>, name=<span class="string">"w0"</span>)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 构建线性函数的截距，变量b</span></span><br><span class="line">b = tf.Variable(<span class="number">0.0</span>, name=<span class="string">"b0"</span>)   </span><br><span class="line">​</span><br><span class="line"><span class="comment"># pred是预测值，前向计算</span></span><br><span class="line">pred = model(x, w, b)</span><br></pre></td></tr></table></figure>
<p>定义一些超参数，包括训练的轮数和学习率。其中如果学习率设置过大，可能导致参数在极值附近来回摇摆，无法保证收敛。如果学习率设置过小，虽然能保证收敛，但优化速度会大大降低，我们需要更多迭代次数才能达到较理想的优化效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 迭代次数（训练轮数）</span></span><br><span class="line">train_epochs = <span class="number">10</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># 学习率</span></span><br><span class="line">learning_rate = <span class="number">0.05</span> </span><br><span class="line">​</span><br><span class="line"><span class="comment"># 控制显示loss值的粒度</span></span><br><span class="line">display_step = <span class="number">10</span></span><br></pre></td></tr></table></figure>
<p>定义损失函数和优化器。损失函数用于描述预测值与真实值之间的误差，从而指导模型收敛方向。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 采用均方差作为损失函数</span></span><br><span class="line">loss_function = tf.reduce_mean(tf.square(y-pred))  </span><br><span class="line">​</span><br><span class="line"><span class="comment"># 梯度下降优化器</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_function)</span><br></pre></td></tr></table></figure>
<p>声明会话及初始化。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">​</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">​</span><br><span class="line">sess.run(init)</span><br></pre></td></tr></table></figure></p>
<p>模型训练阶段，设置迭代轮次，每次通过将样本逐个输入模型，进行梯度下降优化操作，每轮迭代后，绘制出模型曲线。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开始训练，轮数为 epoch，采用SGD随机梯度下降优化方法</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(train_epochs):</span><br><span class="line">    <span class="keyword">for</span> xs,ys <span class="keyword">in</span> zip(x_data, y_data):</span><br><span class="line">        _, loss=sess.run([optimizer,loss_function], feed_dict=&#123;x: xs, y: ys&#125;) </span><br><span class="line">                </span><br><span class="line">    b0temp=b.eval(session=sess)</span><br><span class="line">    w0temp=w.eval(session=sess)</span><br><span class="line">    plt.plot (x_data, w0temp * x_data + b0temp  )<span class="comment"># 画图</span></span><br></pre></td></tr></table></figure>
<p>从上图可以看出，本案例所拟合的模型较简单，训练3次之后已经接近收敛 对于复杂模型，需要更多次训练才能收敛。</p>
<p>预测：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x_test = <span class="number">3.21</span></span><br><span class="line">​</span><br><span class="line">predict = sess.run(pred, feed_dict=&#123;x: x_test&#125;)</span><br><span class="line">print(<span class="string">"预测值：%f"</span> % predict)</span><br><span class="line">​</span><br><span class="line">target = <span class="number">2</span> * x_test + <span class="number">1.0</span></span><br><span class="line">print(<span class="string">"目标值：%f"</span> % target)</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/08/tensorflow1-0中conv2的细节/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/03/08/tensorflow1-0中conv2的细节/" class="post-title-link" itemprop="url">tensorflow1.0中conv2的细节</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-03-08 01:46:44 / 修改时间：01:47:25" itemprop="dateCreated datePublished" datetime="2020-03-08T01:46:44+08:00">2020-03-08</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>对比2.0可以知道tensorflow1.0对于网络的搭建更复杂一些，因此细节上的容易出现差错，在此总结一下使用conv2d的一些小问题。</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.conv2d（input, filter, strides, padding, use_cudnn_on_gpu=<span class="keyword">None</span>,</span><br><span class="line">                data_format=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>先来看一下各个参数的意义。</p>
<ul>
<li><strong>input</strong>：需要做卷积的输入数据。注意：这是一个4维的张量（[batch, in_height, in_width,in_channels]）。对于图像数据来说，batch是这一批样本的个数，in_height和in_width是图像的尺寸，in_channels是图像的通道数，而且要求图像的类型为float32或float64。因此，我们在对图像进行处理的时候，首先要把图像转换成这种特定的类型。</li>
<li><strong>filter</strong>：卷积核。这也是一个4维的张量（[filter_height, filter_width, in_channels,out_channels]）。filter_height,和filter_width是图像的尺寸，in_channels,是输入的通道数，out_channels是输出的通道数。</li>
<li><strong>strides</strong>：图像每一维的步长。是一个一维向量，长度为4。</li>
<li><strong>padding</strong>：定义元素边框与元素内容之间的空间。这里只能选择”SAME”或”VALID”，这个值决定了不同的卷积方式。当它为”SAME”时，表示边缘填充，适用于全尺寸操作；当它为”VALID”时，表示边缘不填充。</li>
<li><strong>use_cudnn_on_gpu</strong>：bool类型，是否使用cudnn加速。</li>
<li><strong>name</strong>：该操作的名称。</li>
<li><p>返回值：返回一个张量（tensor），即特征图（feature map）。</p>
<p>需要额外注意一点就是dataformat，关系到网络的output的排列方式，以及下一层的对接工作能否正确完成,它有两个选项，<strong>NHWC</strong>以及<strong>NCHW</strong>，前者为默认值。设置为 “NHWC” 时，排列顺序为 [batch, height, width, channels]。N是说这批图片有几张，H和W描述图像size，C是通道数（黑白图C=1，RBG图C=3）。以RGB为例，直观来说如下：<br><img src="https://img-blog.csdnimg.cn/20200308010449539.png" alt="在这里插入图片描述"><br>这里以灰度计算为例，说明各自的优劣。<br>对NCHW进行计算的时候，对将分成三个独立通道分别计算，，即全红一组，全绿一组这样。而NHWC得排列方式，是单个的三个相邻通道为一组计算。两者计算成本相同。我们可以知道，这样的话NHWC的局部访问存储性能更好（每三个输入像素即可得到一个输出像素）。NCHW 则必须等所有通道输入准备好才能得到最终输出结果，需要占用较大的临时空间。简单来说，就是想得到某个或某些独立像素像素的灰度计算结果，NCHW需要将全部图片计算出来，再取出特定的计算结果，而NHWC可以直接一个像素一个像素的得到结果。</p>
<p>在 CNN 中常常见到 1x1 卷积（例如：用于移动和嵌入式视觉应用的 MobileNets），也是每个输入 channel 乘一个权值，然后将所有 channel 结果累加得到一个输出 channel。如果使用 NHWC 数据格式，可以将卷积计算简化为矩阵乘计算，即 1x1 卷积核实现了每个输入像素组到每个输出像素组的线性变换。</p>
</li>
</ul>
<p>TensorFlow 为什么选择 NHWC 格式作为默认格式？因为早期开发都是基于 CPU，使用 NHWC 比 NCHW 稍快一些（不难理解，NHWC 局部性更好，cache 利用率高）。而NCHW 则是 Nvidia cuDNN 默认格式，使用 GPU 加速时用 NCHW 格式速度会更快（个别情况例外）。<br>所以设计网络的时候，需要根据具体的实践环境进行切换。</p>
<p><em>再回过头来看看conv2d的使用例子</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># 实践基于1.0</span></span><br><span class="line"> <span class="comment"># 2.0想实现请使用这段替换第一行：</span></span><br><span class="line"> <span class="comment"># import tensorflow.compat.v1 as tf</span></span><br><span class="line"> <span class="comment"># tf.disable_eager_execution()</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">input_data = tf.Varible(np.random.rand(<span class="number">10</span>,<span class="number">9</span>,<span class="number">9</span>,<span class="number">4</span>),dtype=np.float32)</span><br><span class="line">filter_data = tf.Varible(np.random.rand(<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">2</span>),dtype=np.float32)</span><br><span class="line">y = tf.nn.conv2d(input_data,filter_data,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">'VALID'</span>)</span><br><span class="line"></span><br><span class="line">print(input_data)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Tensor(“Variable/read:0”,shape=(10,9,9,4),dtype=float32)<br>Tensor(“Conv2D:0”,shape=(10,7,7,2),dtype=float32)</p>
</blockquote>
<p>导入所需要的库，然后我们定义需要做卷积的输入以及卷积核，这里的步长为1，padding为”VALID”。<br>我们可以看到，原本输入的shape是（10，9，9，4），由于padding为”VALID”,不对图像的边缘进行填充，所以在进行卷积之后，图像的尺寸发生了改变。<br>如果将padding改为”SAME”，图像的尺寸不变。</p>
<hr>
<p>2.0中keras的封装十分完善，对于使用来说比较友好，但是我们也更应该关注被封装一场隐藏起来的环节究竟有哪些细节在发生，在学习1.0的过程中我们会有不少收获。顺便<a href="https://minghuiwu.gitbook.io/tfbook/" target="_blank" rel="noopener">这里有一本1.0的开源书籍，对初学者十分友好，大家可以去看看</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/29/keras-layers-核心网络层摘要/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/02/29/keras-layers-核心网络层摘要/" class="post-title-link" itemprop="url">keras.layers--核心网络层摘要</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-02-29 23:20:58 / 修改时间：23:21:35" itemprop="dateCreated datePublished" datetime="2020-02-29T23:20:58+08:00">2020-02-29</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>完成一定实践后仔细阅读keras文档，做了关于核心网络层的一些摘要，主要汇总一些常用的网络层及其使用指南，大都在实践中使用过。</p>
<hr>
<p><strong>Dense</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Dense(units, activation=<span class="keyword">None</span>, use_bias=<span class="keyword">True</span>,</span><br><span class="line">                 kernel_initializer=<span class="string">'glorot_uniform'</span>, </span><br><span class="line">                 bias_initializer=<span class="string">'zeros'</span>, kernel_regularizer=<span class="keyword">None</span>, </span><br><span class="line">                 bias_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>, </span><br><span class="line">                 kernel_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>简要</li>
</ul>
<blockquote>
<p>全连接层。</p>
<p><em>output = activation(dot(input, kernel) + bias)</em>  其中 <em>activation</em> 是按逐个元素计算的激活函数，<em>kernel</em> 是由网络层创建的权值矩阵，以及 <em>bias</em> 是其创建的偏置向量 (只在 <em>use_bias</em><br>为 <em>True</em> 时才有用)。</p>
<p>如果该层的输入的秩大于2，那么它首先被展平然后 再计算与 kernel 的点乘。</p>
</blockquote>
<ul>
<li>重要参数</li>
</ul>
<blockquote>
<p>units: 正整数，输出空间维度。<br>activation: 激活函数。若不指定，则不使用激活函数 (即「线性」激活: a(x)=x。<br>use_bias: 布尔值，该层是否使用偏置向量。</p>
</blockquote>
<ul>
<li>输入输出</li>
</ul>
<blockquote>
<p>输入尺寸：<br>nD 张量，尺寸: (batch_size, …, input_dim)。 最常见的情况是一个尺寸为 (batch_size, input_dim) 的 2D 输入。<br>输出尺寸：<br>nD 张量，尺寸: (batch_size, …, units)。 例如，对于尺寸为 (batch_size, input_dim) 的 2D 输入， 输出的尺寸为 (batch_size, units)。</p>
</blockquote>
<hr>
<p><strong>Activation</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Activation(activation)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>摘要</p>
<blockquote>
<p>激活函数可单独作为一层，也可以嵌入在Dense层中，发挥作用。</p>
</blockquote>
</li>
<li><p>输入输出</p>
<blockquote>
<p>输入尺寸：<br>任意尺寸。 当使用此层作为模型中的第一层时， 使用参数 input_shape （整数元组，不包括样本数的轴）。<br>输出尺寸：<br>与输入相同。</p>
</blockquote>
</li>
</ul>
<hr>
<p><strong>Dropout</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Dropout(rate, noise_shape=<span class="keyword">None</span>, seed=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>摘要<blockquote>
<p>Dropout 包括在训练中每次更新时， 将输入单元的按比率随机设置为 0， 这有助于防止过拟合。随机舍去一定的神经元，来降低过拟合程度。</p>
</blockquote>
</li>
<li>重要参数<blockquote>
<p>rate: 在 0 和 1 之间浮动。需要丢弃的输入比例。<br>noise_shape: 1D 整数张量， 表示将与输入相乘的二进制 dropout 掩层的形状。 例如，如果你的输入尺寸为 (batch_size, timesteps, features)，然后 你希望 dropout 掩层在所有时间步都是一样的， 你可以使用 noise_shape=(batch_size, 1, features)。<br>seed: 一个作为随机种子的 Python 整数。</p>
</blockquote>
</li>
</ul>
<hr>
<p><strong>Flatten</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Flatten(data_format=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>摘要<blockquote>
<p>将输入展平。不影响批量大小。</p>
</blockquote>
</li>
<li>重要参数<blockquote>
<p>data_format：一个字符串，其值为 channels_last（默认值）或者 channels_first。它表明输入的维度的顺序。此参数的目的是当模型从一种数据格式切换到另一种数据格式时保留权重顺序。channels_last 对应着尺寸为 (batch, …, channels) 的输入，而 channels_first 对应着尺寸为 (batch, channels, …) 的输入。默认为 image_data_format 的值，你可以在 Keras 的配置文件 ~/.keras/keras.json 中找到它。如果你从未设置过它，那么它将是 channels_last</p>
</blockquote>
</li>
</ul>
<hr>
<p><strong>Lambda</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Lambda(function, output_shape=<span class="keyword">None</span>, mask=<span class="keyword">None</span>, arguments=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure></p>
<ul>
<li>摘要<blockquote>
<p>将任意表达式封装为 Layer 对象。</p>
</blockquote>
</li>
<li>重要参数<blockquote>
<p>function: 需要封装的函数。 将输入张量作为第一个参数。<br>output_shape: 预期的函数输出尺寸。 只在使用 Theano 时有意义。 可以是元组或者函数。 如果是元组，它只指定第一个维度； 样本维度假设与输入相同： output_shape = (input_shape[0], ) + output_shape 或者，输入是 None 且样本维度也是 None： output_shape = (None, ) + output_shape 如果是函数，它指定整个尺寸为输入尺寸的一个函数： output_shape = f(input_shape)<br>arguments: 可选的需要传递给函数的关键字参数。</p>
</blockquote>
</li>
<li>输入输出<blockquote>
<p>输入尺寸：<br>任意。当使用此层作为模型中的第一层时， 使用参数 input_shape （整数元组，不包括样本数的轴）。<br>输出尺寸：<br>由 output_shape 参数指定 (或者在使用 TensorFlow 时，自动推理得到)。</p>
</blockquote>
</li>
</ul>
<hr>
<p><strong>Conv2D</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Conv2D(filters, kernel_size, strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">                    padding=<span class="string">'valid'</span>, data_format=<span class="keyword">None</span>,</span><br><span class="line">                    dilation_rate=(<span class="number">1</span>, <span class="number">1</span>), activation=<span class="keyword">None</span>,</span><br><span class="line">                    use_bias=<span class="keyword">True</span>, kernel_initializer=<span class="string">'glorot_uniform'</span>,</span><br><span class="line">                    bias_initializer=<span class="string">'zeros'</span>, kernel_regularizer=<span class="keyword">None</span>,</span><br><span class="line">                    bias_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>,</span><br><span class="line">                    kernel_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>摘要<blockquote>
<p>2D 卷积层 (例如对图像的空间卷积)。<br>该层创建了一个卷积核， 该卷积核对层输入进行卷积， 以生成输出张量。 如果 use_bias 为 True， 则会创建一个偏置向量并将其添加到输出中。 最后，如果 activation 不是 None，它也会应用于输出。<br>当使用该层作为模型第一层时，需要提供 input_shape 参数 （整数元组，不包含样本表示的轴），例如， input_shape=(128, 128, 3) 表示 128x128 RGB 图像， 在 data_format=”channels_last” 时。</p>
</blockquote>
</li>
<li><p>重要参数</p>
<blockquote>
<p>filters: 整数，输出空间的维度 （即卷积中滤波器的输出数量）。<br>kernel_size: 一个整数，或者 2 个整数表示的元组或列表， 指明 2D 卷积窗口的宽度和高度。 可以是一个整数，为所有空间维度指定相同的值。<br>strides: 一个整数，或者 2 个整数表示的元组或列表， 指明卷积沿宽度和高度方向的步长。 可以是一个整数，为所有空间维度指定相同的值。 指定任何 stride 值 != 1 与指定 dilation_rate 值 != 1 两者不兼容。<br>padding: “valid” 或 “same” (大小写敏感)。</p>
</blockquote>
</li>
<li><p>输入输出</p>
<blockquote>
<p>输入尺寸：<br>如果 data_format=’channels_first’， 输入 4D 张量，尺寸为 (samples, channels, rows, cols)。<br>如果 data_format=’channels_last’， 输入 4D 张量，尺寸为 (samples, rows, cols, channels)。<br>输出尺寸：<br>如果 data_format=’channels_first’， 输出 4D 张量，尺寸为 (samples, filters, new_rows, new_cols)。<br>如果 data_format=’channels_last’， 输出 4D 张量，尺寸为 (samples, new_rows, new_cols, filters)。</p>
</blockquote>
</li>
</ul>
<hr>
<p><strong>MaxPooling2D</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">                          strides=<span class="keyword">None</span>, padding=<span class="string">'valid'</span>,</span><br><span class="line">                          data_format=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>摘要<blockquote>
<p>对于空间数据的最大池化。</p>
</blockquote>
</li>
<li>重要参数<blockquote>
<p>pool_size: 整数，或者 2 个整数表示的元组， 沿（垂直，水平）方向缩小比例的因数。 （2，2）会把输入张量的两个维度都缩小一半。 如果只使用一个整数，那么两个维度都会使用同样的窗口长度。</p>
</blockquote>
</li>
</ul>
<p><strong>*AveragePooling2D</strong>与之相似不过完成的是平均池化操作。*</p>
<hr>
<p><strong>SimpleRNN</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.SimpleRNN(units, activation=<span class="string">'tanh'</span>, use_bias=<span class="keyword">True</span>,</span><br><span class="line">                       kernel_initializer=<span class="string">'glorot_uniform'</span>,</span><br><span class="line">                       recurrent_initializer=<span class="string">'orthogonal'</span>, </span><br><span class="line">                       bias_initializer=<span class="string">'zeros'</span>, kernel_regularizer=<span class="keyword">None</span>, </span><br><span class="line">                       recurrent_regularizer=<span class="keyword">None</span>, bias_regularizer=<span class="keyword">None</span>,</span><br><span class="line">                       activity_regularizer=<span class="keyword">None</span>, kernel_constraint=<span class="keyword">None</span>, </span><br><span class="line">                       recurrent_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>, </span><br><span class="line">                       dropout=<span class="number">0.0</span>, recurrent_dropout=<span class="number">0.0</span>, </span><br><span class="line">                       return_sequences=<span class="keyword">False</span>, return_state=<span class="keyword">False</span>, </span><br><span class="line">                       go_backwards=<span class="keyword">False</span>, stateful=<span class="keyword">False</span>, unroll=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>摘要</li>
</ul>
<blockquote>
<p>全连接的 RNN，其输出将被反馈到输入。</p>
<ul>
<li>重要参数<br>input_dim: 输入的维度（整数）。 将此层用作模型中的第一层时，此参数（或者，关键字参数 input_shape）是必需的。</li>
<li>输入输出<br>输入尺寸：<br>3D 张量，尺寸为 (batch_size, timesteps, input_dim)。<br>输出尺寸：</li>
<li>如果 return_state：返回张量列表。 第一个张量为输出。剩余的张量为最后的状态， 每个张量的尺寸为 (batch_size, units)。</li>
<li>如果 return_sequences：返回 3D 张量， 尺寸为 (batch_size, timesteps, units)。</li>
<li>否则，返回尺寸为 (batch_size, units) 的 2D 张量。</li>
</ul>
<hr>
<p><strong>LSTM</strong></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.LSTM(units, activation=<span class="string">'tanh'</span>, </span><br><span class="line">                  recurrent_activation=<span class="string">'hard_sigmoid'</span>, use_bias=<span class="keyword">True</span>, </span><br><span class="line">                  kernel_initializer=<span class="string">'glorot_uniform'</span>, </span><br><span class="line">                  recurrent_initializer=<span class="string">'orthogonal'</span>, </span><br><span class="line">                  bias_initializer=<span class="string">'zeros'</span>, unit_forget_bias=<span class="keyword">True</span>, </span><br><span class="line">                  kernel_regularizer=<span class="keyword">None</span>, recurrent_regularizer=<span class="keyword">None</span>, </span><br><span class="line">                  bias_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>, </span><br><span class="line">                  kernel_constraint=<span class="keyword">None</span>, recurrent_constraint=<span class="keyword">None</span>, </span><br><span class="line">                  bias_constraint=<span class="keyword">None</span>, dropout=<span class="number">0.0</span>, recurrent_dropout=<span class="number">0.0</span>, </span><br><span class="line">                  implementation=<span class="number">1</span>, return_sequences=<span class="keyword">False</span>, </span><br><span class="line">                  return_state=<span class="keyword">False</span>, go_backwards=<span class="keyword">False</span>, stateful=<span class="keyword">False</span>, unroll=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>摘要<blockquote>
<p>长短期记忆网络层</p>
</blockquote>
</li>
</ul>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/19/卷积知识小结/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/02/19/卷积知识小结/" class="post-title-link" itemprop="url">卷积知识小结</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-02-19 23:59:04 / 修改时间：23:59:26" itemprop="dateCreated datePublished" datetime="2020-02-19T23:59:04+08:00">2020-02-19</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>简单接触了一些卷积神经网络的知识，在此小结一下。由于它的一般应用于图像分类领域，所以在此不便详细图文并茂的解释，只是浅谈一些自己积累到的小知识点。关于卷积神经网络的入门级知识，我推荐<a href="https://zhuanlan.zhihu.com/p/27908027" target="_blank" rel="noopener">前往这里了解</a>。</p>
<hr>
<p>我们知道cnn其实就是在做一个特征提取器的工作。计算过程就是一个累乘累加的过程。将卷积核的</p>
<h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><ul>
<li>卷积神经网络：（卷积层+（可选）池化层）<em>N+全连接层</em>M</li>
<li>全卷积神经网络：（卷积层+（可选）池化层）<em>N+反卷积层</em>M</li>
</ul>
<p>由于卷积层和池化层一般情况下会使输出的尺寸不断变小提取出抽象的特征，由此可以用来处理分类问题。而全卷积神经网络将最后的全连接层全部换成反卷积层，使得我们可以得到和输入图片尺寸相同的输出，从而完成图片的物体分割工作。</p>
<h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>卷积解决了以下问题：</p>
<p>比如在图像分类问题中，网络由全连接层组成就会导致<strong>参数过多</strong>。会浪费我们的计算资源，并且能承载很大信息量的神经网络会轻易模拟出数据中的规律，而产生过拟合，降低模型的泛化能力。</p>
<h3 id="卷积核"><a href="#卷积核" class="headerlink" title="卷积核"></a>卷积核</h3><p>卷积如何解决问题涉及到卷积核的一部分内容，首先不得不提到我们必须了解的卷积神经网络的两个重要性质：</p>
<ul>
<li><p>[ ] 局部链接</p>
<p>输出单元单元通过卷积核与原图进行连接操作，而卷积核的大小一般不会太大所以减少了连接数目，一定程度上缓解了问题。</p>
</li>
<li>[ ] 参数共享</li>
</ul>
<p>卷积核一次只能和原图相同尺寸的区域产生来连接，而其余的地方采用滑动窗口的方式依次连接，也就是说卷积核前后是不变的，只是按规定的步长进行移动来生成最终的feature map。</p>
<p>经过以上两部分的操作可以大大降低参数过多的问题。</p>
<p>我们使用卷积其实是提取到特征，那这两种操作虽然解决了一些问题，但是否会对我们的特征提取产生影响呢？继续看图像处理的例子，比如人脸识别中脸颊部分的像素相近，嘴唇部分的像素相近，也就是说这样的图片是有信息冗余的，由此可见图像是有一定的区域性，所以在经过局部连接之后，仍然能够保留提取特征的能力。而特征应该还与其所在的位置无关，也就是说，假如人脸在图片的右下角或左上角，对于脸部的特征不论出现在图片的什么位置，都应该被识别出来，而参数共享就恰好对应了这一特点，使得无论这一特征出现在什么位置都会得到好的匹配结果。只能匹配到固定位置的特征毫无疑问就是一种过拟合的表现，而恰恰参数共享避免了这种情况。</p>
<h3 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h3><p>卷积后的数据量减小了，但仍然过于庞大，池化操作就是在减少数据量。池化分为最大值池化和平均池化。最常用的是最大值池化，我们主要介绍这种。最大池化保留了每一个小块内的最大值，所以它相当于保留了这一块最佳匹配结果。这也就意味着它不会具体关注窗口内到底是哪一个地方匹配了，而只关注是不是有某个地方匹配上了。这也就能够看出，CNN能够发现图像中是否具有某种特征，而不用在意到底在哪里具有这种特征（似乎和参数共享有类似的作用）。</p>
<h3 id="非线性激活"><a href="#非线性激活" class="headerlink" title="非线性激活"></a>非线性激活</h3><p>激活函数比如Relu，它的公式：$f(x)=max(0,x)$即，保留大于等于0的值，其余所有小于0的数值直接改写为0。卷积后的图中的值，越小则越不相关，我们进行特征提取时，为了使得数据更少，操作更方便，就直接舍弃掉那些不相关联的数据（直接取零）。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/12/HDF5-python-使用简介/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/02/12/HDF5-python-使用简介/" class="post-title-link" itemprop="url">HDF5--python 使用简介</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-02-12 23:52:23 / 修改时间：23:56:28" itemprop="dateCreated datePublished" datetime="2020-02-12T23:52:23+08:00">2020-02-12</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>引：在使用TensorFlow，学习回调函数时，使用了ModelCheckpoint()，产生了.h5的文件。为了了解回调函数产生的信息，需要了解HDF5的相关内容，以及在python中的使用与相关问题解决。</em><br>文中一些叙述为了方便初次接触者理解，表述并不严谨，仅供简单参考。</p>
<hr>
<h2 id="初次见面"><a href="#初次见面" class="headerlink" title="初次见面"></a>初次见面</h2><p>HDF5（Hierarchical Data Formal）是用于存储大规模数值数据的较为理想的存储格式，文件后缀名为h5，存储读取速度非常快，且可在文件内部按照明确的层次存储数据，同一个HDF5可以看做一个高度整合的文件夹，其内部可存放不同类型的数据。</p>
<p>在Python中操纵HDF5文件的方式主要有两种</p>
<ul>
<li>是利用<strong>pandas</strong>中内建的一系列HDF5文件操作相关的方法，来完成相关操作。</li>
<li>二是利用<strong>h5py</strong>模块来完成Python原生数据结构与HDF5格式的转化</li>
</ul>
<p>本篇主要介绍hdf5的基础内容和对应模块使用的快速入门。</p>
<h2 id="初遇时的差池"><a href="#初遇时的差池" class="headerlink" title="初遇时的差池"></a>初遇时的差池</h2><p><del>一段小插曲</del><br>HDF是HDF(Hierarchical Data File)是美国国家高级计算应用中心(National Center for Supercomputing Application,NCSA)为了满足各种领域研究需求而研制的一种能高效存储和分发科学数据的新型数据格式 。阅读的文档中提到了到国家卫星气象中心（NSMC）曾经发布过一份《HDF5.0 使用简介》,抱着些许迷信权威的心态阅读后发现它涉及的信息就我目前来说价值不大，其中主要有讲HDF5文件的组织，API，创建，数据集数据空间，组群，属性等等，内容大而全，但似乎这篇教材发布的相对较早，所以产生了一定的局限性并且内部的相关URL都失效了，它本身也在国家卫星气象中心的官网上没有什么存在的痕迹。相关的API只涉及了C和FORTRAN的外壳包装函数。对于像我这样在使用python且第一次接触HDF5的使用者并不友好，索性只读了开头的基础内容并建立了更详细的认知后就放弃了继续阅读的打算。</p>
<h2 id="相识"><a href="#相识" class="headerlink" title="相识"></a>相识</h2><p>首先从hdf5文件讲起。</p>
<p>HDF5文件具有两类存储对象，dataset和group。dataset是类似于数组的数据集，而group是类似文件夹一样的容器，存放dataset和其他group。</p>
<p>HDF本意即是<strong>层次数据格式</strong>，所以就其存储结构来说是类似与POSIX风格的。其实现的方式就是group。每层都用’/‘分隔。我们创建的file object其实也可以看作一个group，是一个root group，其余的groups可以称为subgroups。</p>
<p>dataset与numpy中的array相似，比如都具有shape、dtype、以及一些切片操作等。虽然与Numpy的数组在接口上很相近，但是支持更多对外透明的存储特征，如数据压缩，误差检测，分块传输。</p>
<p>HDF5的一个很好的features就是可以在数据旁边存储元数据<sup><a href="#fn_1" id="reffn_1">1</a></sup>。所有的group和dataset都支持叫做<strong>属性</strong>的数据形式。</p>
<h4 id="h5py"><a href="#h5py" class="headerlink" title="h5py"></a>h5py</h4><p>想到python一定有对应的文件解析库，于是我开始了h5py的“快速”入门。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">import h5py</span><br><span class="line"># 读取</span><br><span class="line">file = h5py.File(&apos;test.hdf5&apos;, &apos;r&apos;)# .hdf5与.h5意义相同</span><br><span class="line"># 一下也可以完成读取</span><br><span class="line">with h5py.File(&quot;mytestfile.hdf5&quot;, &quot;w&quot;) as f:</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"># 我们需要注意的是group（包括File对象）与python中的字典相似</span><br><span class="line"># 通过该方法可以获得对应group下的subgroups或datasets，返回包含字符串的列表</span><br><span class="line">f.keys()</span><br><span class="line"></span><br><span class="line"># 此时我们假设存在一个名为DataSet的dataset对象</span><br><span class="line"># 利用对应键来索引值的方法可以获取该对象</span><br><span class="line">dest = f[&apos;DataSet&apos;]</span><br><span class="line"></span><br><span class="line"># dataset对象满足我们平时使用的numpy数组的一些操作，如下：</span><br><span class="line">dest.shape</span><br><span class="line">dest.dtype</span><br><span class="line">dest[:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 创建HDF5文件</span><br><span class="line">f = h5py.File(&apos;test.h5&apos;, &apos;w&apos;)</span><br><span class="line"></span><br><span class="line"># 使用create_dataset创建给定形状和数据类型的空dataset</span><br><span class="line">dataset = f.create_dataset(&apos;DS&apos;,(100,), dtype=&apos;i&apos;)</span><br><span class="line"># 也可以使用numpy中数组来初始化</span><br><span class="line">array = np.arange(100)</span><br><span class="line">dataset = f.create_dataset(&apos;init&apos;, data=array)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 分块存储</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">在缺省设置下，HDF5数据集在内存中是连续布局的，也就是按照传统的C序。</span><br><span class="line">Dataset也可以在HDF5的分块存储布局下创建。</span><br><span class="line">也就是dataset被分为大小相同的若干块随意地分布在磁盘上，并使用B树建立索引。</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"># 为了进行分块存储，将关键字设为一个元组来指示块的形状。</span><br><span class="line">dset = f.create_dataset(&quot;chunked&quot;, (1000, 1000), chunks=(100, 100))</span><br><span class="line"># 也可以自动分块，不必指定块的形状。</span><br><span class="line">dset = f.create_dataset(&quot;autochunk&quot;, (1000, 1000), chunks=True)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 分层结构</span><br><span class="line"># 遍历subgroups</span><br><span class="line">for name in f:</span><br><span class="line">    print(name)</span><br><span class="line"># 递归遍历所有subgroups</span><br><span class="line">def print_name(name):</span><br><span class="line">    print(name)</span><br><span class="line"></span><br><span class="line">f.visit(print_name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 属性通过attrs成员访问，类似于python中词典格式。</span><br><span class="line"> dataset.attrs[&apos;bias&apos;] = 60</span><br><span class="line"> &apos;bias&apos; in dataset.attrs</span><br></pre></td></tr></table></figure>
<p>一些其他的特性</p>
<ol>
<li>滤波器组<br>HDF5的滤波器组能够对分块数组进行变换。最常用的变换是高保真压缩。使用一个特定的压缩滤波器创建dataset之后，读写都可以向平常一样，不必添加额外的步骤。<br>用关键词compression来指定压缩滤波器，而滤波器的可选参数使用关键词compression_opt来指定：</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dset = f.create_dataset(&quot;zipped&quot;, (100, 100), compression=&quot;gzip&quot;)</span><br></pre></td></tr></table></figure>
<ol>
<li>HDF5文件的限制</li>
</ol>
<ul>
<li>HDF5文件本身大小没有限制，但是HDF5的一个dataset最高允许32个维，每个维度最多可有2^64个值，每个值大小理论上可以任意大</li>
<li>目前一个chunk允许的最大容量为2^32-1 byte (4GB). 大小固定的dataset的块的大小不能超过dataset的大小。</li>
</ul>
<p><a href="http://docs.h5py.org/en/latest/index.html" title="HDF5 for Python -- h5py 2.10.0 doc" target="_blank" rel="noopener">更多信息</a></p>
<h4 id="pandas"><a href="#pandas" class="headerlink" title="pandas"></a>pandas</h4><p><strong> 写出 </strong></p>
<p>　　pandas中的HDFStore()用于生成管理HDF5文件IO操作的对象，其主要参数如下：</p>
<p>　　path：字符型输入，用于指定h5文件的名称（不在当前工作目录时需要带上完整路径信息）</p>
<p>　　mode：用于指定IO操作的模式，与Python内建的open()中的参数一致，默认为’a’，即当指定文件已存在时不影响原有数据写入，指定文件不存在时则新建文件；’r’，只读模式；’w’，创建新文件（会覆盖同名旧文件）；’r+’，与’a’作用相似，但要求文件必须已经存在；</p>
<p>　　complevel：int型，用于控制h5文件的压缩水平，取值范围在0-9之间，越大则文件的压缩程度越大，占用的空间越小，但相对应的在读取文件时需要付出更多解压缩的时间成本，默认为0，代表不压缩</p>
<p>　　下面我们创建一个HDF5 IO对象store：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">store = pd.HDFStore(<span class="string">'demo.h5'</span>)</span><br><span class="line"><span class="string">'''查看store类型'''</span></span><br><span class="line">print(store)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">pandas</span>.<span class="title">io</span>.<span class="title">pytables</span>.<span class="title">HDFStore</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">File</span> <span class="title">path</span>:</span> demo.h5</span><br></pre></td></tr></table></figure>
<p>可以看到store对象属于pandas的io类，通过上面的语句我们已经成功的初始化名为demo.h5的的文件，本地也相应的出现了对应文件。</p>
<p>接下来我们创建pandas中不同的两种对象，并将它们共同保存到store中，首先创建series对象：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建一个series对象</span></span><br><span class="line">s = pd.Series(np.random.randn(<span class="number">5</span>), index=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>, <span class="string">'e'</span>])</span><br><span class="line"><span class="comment">#创建一个dataframe对象</span></span><br><span class="line">df = pd.DataFrame(np.random.randn(<span class="number">8</span>, <span class="number">3</span>),</span><br><span class="line">                 columns=[<span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'C'</span>])</span><br></pre></td></tr></table></figure>
<p>第一种方式利用键值对将不同的数据存入store对象中，这里为了代码简洁使用了元组赋值法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">store[<span class="string">'s'</span>],store[<span class="string">'df'</span>] = s,df</span><br></pre></td></tr></table></figure>
<p>第二种方式利用store对象的put()方法，其主要参数如下：</p>
<p>　　key：指定h5文件中待写入数据的key</p>
<p>　　value：指定与key对应的待写入的数据</p>
<p>　　format：字符型输入，用于指定写出的模式，’fixed’对应的模式速度快，但是不支持追加也不支持检索；’table’对应的模式以表格的模式写出，速度稍慢，但是支持直接通过store对象进行追加和表格查询操作</p>
<p>使用put()方法将数据存入store对象中：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">store.put(key=<span class="string">'s'</span>,value=s);store.put(key=<span class="string">'df'</span>,value=df)</span><br></pre></td></tr></table></figure></p>
<p>既然是键值对的格式，那么可以查看store的items属性（注意这里store对象只有items和keys属性，没有values属性）：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">store.items</span><br></pre></td></tr></table></figure></p>
<p>调用store对象中的数据直接用对应的键名来索引即可：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">store[<span class="string">'df'</span>]</span><br></pre></td></tr></table></figure></p>
<p>删除store对象中指定数据的方法有两种，一是使用remove()方法，传入要删除数据对应的键：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">store.remove(<span class="string">'s'</span>)</span><br><span class="line">print(store.keys())</span><br></pre></td></tr></table></figure>
<p>　　二是使用Python中的关键词del来删除指定数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span> store[<span class="string">'s'</span>]</span><br><span class="line">print(store.keys())</span><br></pre></td></tr></table></figure>
<p>这时若想将当前的store对象持久化到本地，只需要利用close()方法关闭store对象即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">store.close()</span><br><span class="line"><span class="string">'''查看store连接状况，False则代表已关闭'''</span></span><br><span class="line">store.is_open</span><br><span class="line"><span class="comment"># 这时本地的h5文件也相应的存储进store对象关闭前包含的文件</span></span><br></pre></td></tr></table></figure>
<p>　除了通过定义一个确切的store对象的方式，还可以从pandas中的数据结构直接导出到本地h5文件中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建新的数据框</span></span><br><span class="line">df_ = pd.DataFrame(np.random.randn(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line"><span class="comment">#导出到已存在的h5文件中，这里需要指定key</span></span><br><span class="line">df_.to_hdf(path_or_buf=<span class="string">'demo.h5'</span>,key=<span class="string">'df_'</span>)</span><br><span class="line"><span class="comment">#创建于本地demo.h5进行IO连接的store对象</span></span><br><span class="line">store = pd.HDFStore(<span class="string">'demo.h5'</span>)</span><br><span class="line"><span class="comment">#查看指定h5对象中的所有键</span></span><br><span class="line">print(store.keys())</span><br></pre></td></tr></table></figure>
<p><strong>读入</strong><br>在pandas中读入HDF5文件的方式主要有两种，一是通过上一节中类似的方式创建与本地h5文件连接的IO对象，接着使用键索引或者store对象的get()方法传入要提取数据的key来读入指定数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">store = pd.HDFStore(<span class="string">'demo.h5'</span>)</span><br><span class="line"><span class="string">'''方式1'''</span></span><br><span class="line">df1 = store[<span class="string">'df'</span>]</span><br><span class="line"><span class="string">'''方式2'''</span></span><br><span class="line">df2 = store.get(<span class="string">'df'</span>)</span><br><span class="line">df1 == df2</span><br></pre></td></tr></table></figure>
<p>可以看出这两种方式都能顺利读取键对应的数据。</p>
<p>　　第二种读入h5格式文件中数据的方法是pandas中的read_hdf()，其主要参数如下：</p>
<p>　　path_or_buf：传入指定h5文件的名称</p>
<p>　　key：要提取数据的键</p>
<p>　　需要注意的是利用read_hdf()读取h5文件时对应文件不可以同时存在其他未关闭的IO对象，否则会报错，如下例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(store.is_open)</span><br><span class="line">df = pd.read_hdf(<span class="string">'demo.h5'</span>,key=<span class="string">'df'</span>)</span><br></pre></td></tr></table></figure>
<p>　把IO对象关闭后再次提取：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">store.close()</span><br><span class="line">print(store.is_open)</span><br><span class="line">df = pd.read_hdf(<span class="string">'demo.h5'</span>,key=<span class="string">'df'</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>
<hr>
<p>参考：<br><a href="https://blog.csdn.net/yudf2010/article/details/50353292" target="_blank" rel="noopener">https://blog.csdn.net/yudf2010/article/details/50353292</a><br><a href="https://segmentfault.com/a/119000001667088" target="_blank" rel="noopener">https://segmentfault.com/a/119000001667088</a><br><a href="https://www.cnblogs.com/feffery/p/11135082.html" target="_blank" rel="noopener">https://www.cnblogs.com/feffery/p/11135082.html</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="X">
            
              <p class="site-author-name" itemprop="name">X</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">20</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              

              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">X</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v6.6.0</div>



<div class="powered-by">
  <i class="fa fa-child" font="" style="font-size:26px;"></i>
  <span id="busuanzi_container_site_uv">
    本站访客数:<span id="busuanzi_value_site_uv"></span>
  </span>
</div>

        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=6.6.0"></script>

  <script src="/js/src/motion.js?v=6.6.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.6.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.6.0"></script>



  

  


  <script src="/js/src/bootstrap.js?v=6.6.0"></script>



  



  











  





  

  

  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {equationNumbers: { autoNumber: "AMS" }}
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<style>
.MathJax_Display {
  overflow: auto hidden;
}
</style>

    
  


  

  

  

  

  

  

  

  

</body>
</html>
