<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">


























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=6.6.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.6.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.6.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.6.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.6.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '6.6.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta property="og:type" content="website">
<meta property="og:title" content="X">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="X">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="X">



  <link rel="alternate" href="/atom.xml" title="X" type="application/atom+xml">




  <link rel="canonical" href="http://yoursite.com/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>X</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <a href="https://github.com/ZSDDZA" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">X</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">x</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-schedule">

    
    
    
      
    

    

    <a href="/schedule/" rel="section"><i class="menu-item-icon fa fa-fw fa-calendar"></i> <br>日程表</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-sitemap">

    
    
    
      
    

    

    <a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br>站点地图</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/14/tensorflow1-0-线性回归实战/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/03/14/tensorflow1-0-线性回归实战/" class="post-title-link" itemprop="url">tensorflow1.0 线性回归实战</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-03-14 23:49:56 / 修改时间：23:50:22" itemprop="dateCreated datePublished" datetime="2020-03-14T23:49:56+08:00">2020-03-14</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>一个简单的线性回归案例，是一簇点，很容易想到用一条直线去拟合它，所以我们也会希望机器能用y=w</em>x+b这条直线去对其进行拟合，也可以说是去让机器学习w和b的值。</p>
<h2 id="损失"><a href="#损失" class="headerlink" title="损失"></a>损失</h2><p>损失是对糟糕预测的惩罚，损失是一个数值，表示对于单个样本而言模型预测的准确程度。<br>如果模型的预测完全准确，则损失为零，否则损失会较大。<br>训练模型的目标是从所有样本中找到一组平均损失“较小”的权重（w）和偏差（b）。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>损失函数也称为代价函数是将随机事件或其有关随机变量的取值映射为非负实数以表示该随机事件的“风险”或“损失”的函数。<br>下面介绍三个比较常见的损失函数：</p>
<blockquote>
<p>L1损失<br>L1范数损失函数，也被称为最小绝对值偏差（LAD），最小绝对值误差（LAE）。总的说来，它是把目标值（Yi）与估计值（f(xi)）的绝对差值的总和（S）最小化：$S=\sum_{i=1}^n|Y_i-f(x_i)|$</p>
<p>L2范数损失函数，也被称为最小平方误差（LSE）。总的来说，它是把目标值（Yi）与估计值（f(xi)）的差值的平方和（S）最小化：<br>$S=\sum_{i=1}^n(Y_i-f(x_i))^2$</p>
<p>均方误差 (MSE)<br>均方误差 (MSE) 指的是每个样本的平均平方损失<br>$MSE=\frac{1}{N}\sum_{(x,y)\in D}(y-prediction(x))^2$</p>
</blockquote>
<h2 id="模型训练与降低损失"><a href="#模型训练与降低损失" class="headerlink" title="模型训练与降低损失"></a>模型训练与降低损失</h2><p>迭代:首先我们先对模型的中的权重w和偏差b进行猜测，然后将特征点输入，执行预测和推理（Inference），将计算出的值和该样本的标签值进行对比，计算出损失值，我们的目标是使推理的值和标签值的差距越小越好，也就是损失的值越小越好，所以需要不断对计算参数进行更新，直到损失值尽可能地最低为止。</p>
<p>收敛:在学习优化过程中，机器学习系统将根据所有标签去重新评估所有特征，为损失函数生成一个新值，而该值又产生新的参数值。通常，可以不断迭代，直到总体损失不再变化或至少变化极其缓慢为止。这时候，我们可以说该模型已收敛。<br><em>易知该线性回归问题产生的损失与权重的关系图为如下图</em><br><img src="https://img-blog.csdnimg.cn/20200314233427520.png" alt=""></p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>梯度下降法的基本思想可以类比为一个下山的过程，假设一个人在山上，此时他要以最快的速度下山，就需要梯度下降来帮助自己下山。具体来说，就是以自己现在所处的位置为基准，寻找一个山势最陡峭的方向，沿着高度下降的方向走，就能以最快速度到山底。</p>
<p>同理，将上一节所提到的损失函数看作一座山，我们的目标就是找到这个损失函数的最小值（山底），那么我们就可以在初始点找到该点函数的梯度，沿着函数值下降的方向对参数进行更新，这就是梯度下降法。</p>
<h2 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h2><p>那么沿着负梯度方向进行下一步探索，前进多少才合适呢？这时我们就要引入学习率的概念了。用梯度乘以一个称为学习率（有时也称为步长）的标量，以确定下一个点的位置。例如：如果梯度大小为 2.5，学习速率为 0.01，则梯度下降法算法会选择距离前一个点 0.025 的位置作为下一个点。</p>
<p>所以学习率是指导我们该如何通过损失函数的梯度调整网络权重的一个参数（也成为超参数）。学习率越低，损失函数的变化速度就越慢。</p>
<h2 id="Tensorflow实战"><a href="#Tensorflow实战" class="headerlink" title="Tensorflow实战"></a>Tensorflow实战</h2><p>本例通过生成人工数据集。随机生成一个近似采样随机分布，使得w=2.0, b=1, 并加入一个噪声，噪声的最大振幅为0.4。</p>
<p>下面我们来展示具体代码，假设我们要学习的函数为线性函数 𝑦=2𝑥+1。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在Jupyter中，使用matplotlib显示图像需要设置为 inline 模式，否则不会现实图像</span></span><br><span class="line">%matplotlib inline</span><br><span class="line">​</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># 载入matplotlib</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># 载入numpy</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf <span class="comment"># 载入Tensorflow</span></span><br><span class="line">​</span><br><span class="line">np.random.seed(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>我们需要构造满足这个函数的 𝑥 和 𝑦 同时加入一些不满足方程的噪声。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#直接采用np生成等差数列的方法，生成100个点，每个点的取值在-1~1之间</span></span><br><span class="line">x_data = np.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">100</span>) </span><br><span class="line"><span class="comment"># y = 2x +1 + 噪声， 其中，噪声的维度与x_data一致</span></span><br><span class="line">y_data = <span class="number">2</span> * x_data + <span class="number">1.0</span> + np.random.randn(*x_data.shape) * <span class="number">0.4</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#画出随机生成数据的散点图</span></span><br><span class="line">plt.scatter(x_data, y_data) </span><br><span class="line"><span class="comment"># 画出我们想要学习到的线性函数 y = 2x +1</span></span><br><span class="line">plt.plot (x_data, <span class="number">2</span> * x_data + <span class="number">1.0</span>, color = <span class="string">'red'</span>,linewidth=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>上面的代码产生了随机的-1到1的100个点，我们使用matplotlib库将这些点和要学习得到的线性函数可视化出来。</p>
<p>首先我们定义训练数据的占位符，这是后面数据输入的入口，x是特征值，y是标签值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义训练数据的占位符，x是特征值，y是标签值</span></span><br><span class="line">​</span><br><span class="line">x = tf.placeholder(<span class="string">"float"</span>, name = <span class="string">"x"</span>) </span><br><span class="line">y = tf.placeholder(<span class="string">"float"</span>, name = <span class="string">"y"</span>)</span><br></pre></td></tr></table></figure>
<p>然后我们定义模型函数，在本例中是个简单的线性函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(x, w, b)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.multiply(x, w) + b</span><br></pre></td></tr></table></figure>
<p>接下来我们创建模型的变量，Tensorflow变量的声明函数是tf.Variable，tf.Variable的作用是保存和更新参数，变量的初始值可以是随机数、常数或是通过其他变量的初始值计算得到。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建线性函数的斜率，变量w</span></span><br><span class="line">w = tf.Variable(<span class="number">1.0</span>, name=<span class="string">"w0"</span>)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 构建线性函数的截距，变量b</span></span><br><span class="line">b = tf.Variable(<span class="number">0.0</span>, name=<span class="string">"b0"</span>)   </span><br><span class="line">​</span><br><span class="line"><span class="comment"># pred是预测值，前向计算</span></span><br><span class="line">pred = model(x, w, b)</span><br></pre></td></tr></table></figure>
<p>定义一些超参数，包括训练的轮数和学习率。其中如果学习率设置过大，可能导致参数在极值附近来回摇摆，无法保证收敛。如果学习率设置过小，虽然能保证收敛，但优化速度会大大降低，我们需要更多迭代次数才能达到较理想的优化效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 迭代次数（训练轮数）</span></span><br><span class="line">train_epochs = <span class="number">10</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># 学习率</span></span><br><span class="line">learning_rate = <span class="number">0.05</span> </span><br><span class="line">​</span><br><span class="line"><span class="comment"># 控制显示loss值的粒度</span></span><br><span class="line">display_step = <span class="number">10</span></span><br></pre></td></tr></table></figure>
<p>定义损失函数和优化器。损失函数用于描述预测值与真实值之间的误差，从而指导模型收敛方向。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 采用均方差作为损失函数</span></span><br><span class="line">loss_function = tf.reduce_mean(tf.square(y-pred))  </span><br><span class="line">​</span><br><span class="line"><span class="comment"># 梯度下降优化器</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_function)</span><br></pre></td></tr></table></figure>
<p>声明会话及初始化。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">​</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">​</span><br><span class="line">sess.run(init)</span><br></pre></td></tr></table></figure></p>
<p>模型训练阶段，设置迭代轮次，每次通过将样本逐个输入模型，进行梯度下降优化操作，每轮迭代后，绘制出模型曲线。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开始训练，轮数为 epoch，采用SGD随机梯度下降优化方法</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(train_epochs):</span><br><span class="line">    <span class="keyword">for</span> xs,ys <span class="keyword">in</span> zip(x_data, y_data):</span><br><span class="line">        _, loss=sess.run([optimizer,loss_function], feed_dict=&#123;x: xs, y: ys&#125;) </span><br><span class="line">                </span><br><span class="line">    b0temp=b.eval(session=sess)</span><br><span class="line">    w0temp=w.eval(session=sess)</span><br><span class="line">    plt.plot (x_data, w0temp * x_data + b0temp  )<span class="comment"># 画图</span></span><br></pre></td></tr></table></figure>
<p>从上图可以看出，本案例所拟合的模型较简单，训练3次之后已经接近收敛 对于复杂模型，需要更多次训练才能收敛。</p>
<p>预测：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x_test = <span class="number">3.21</span></span><br><span class="line">​</span><br><span class="line">predict = sess.run(pred, feed_dict=&#123;x: x_test&#125;)</span><br><span class="line">print(<span class="string">"预测值：%f"</span> % predict)</span><br><span class="line">​</span><br><span class="line">target = <span class="number">2</span> * x_test + <span class="number">1.0</span></span><br><span class="line">print(<span class="string">"目标值：%f"</span> % target)</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/08/tensorflow1-0中conv2的细节/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/03/08/tensorflow1-0中conv2的细节/" class="post-title-link" itemprop="url">tensorflow1.0中conv2的细节</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-03-08 01:46:44 / 修改时间：01:47:25" itemprop="dateCreated datePublished" datetime="2020-03-08T01:46:44+08:00">2020-03-08</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>对比2.0可以知道tensorflow1.0对于网络的搭建更复杂一些，因此细节上的容易出现差错，在此总结一下使用conv2d的一些小问题。</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.conv2d（input, filter, strides, padding, use_cudnn_on_gpu=<span class="keyword">None</span>,</span><br><span class="line">                data_format=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>先来看一下各个参数的意义。</p>
<ul>
<li><strong>input</strong>：需要做卷积的输入数据。注意：这是一个4维的张量（[batch, in_height, in_width,in_channels]）。对于图像数据来说，batch是这一批样本的个数，in_height和in_width是图像的尺寸，in_channels是图像的通道数，而且要求图像的类型为float32或float64。因此，我们在对图像进行处理的时候，首先要把图像转换成这种特定的类型。</li>
<li><strong>filter</strong>：卷积核。这也是一个4维的张量（[filter_height, filter_width, in_channels,out_channels]）。filter_height,和filter_width是图像的尺寸，in_channels,是输入的通道数，out_channels是输出的通道数。</li>
<li><strong>strides</strong>：图像每一维的步长。是一个一维向量，长度为4。</li>
<li><strong>padding</strong>：定义元素边框与元素内容之间的空间。这里只能选择”SAME”或”VALID”，这个值决定了不同的卷积方式。当它为”SAME”时，表示边缘填充，适用于全尺寸操作；当它为”VALID”时，表示边缘不填充。</li>
<li><strong>use_cudnn_on_gpu</strong>：bool类型，是否使用cudnn加速。</li>
<li><strong>name</strong>：该操作的名称。</li>
<li><p>返回值：返回一个张量（tensor），即特征图（feature map）。</p>
<p>需要额外注意一点就是dataformat，关系到网络的output的排列方式，以及下一层的对接工作能否正确完成,它有两个选项，<strong>NHWC</strong>以及<strong>NCHW</strong>，前者为默认值。设置为 “NHWC” 时，排列顺序为 [batch, height, width, channels]。N是说这批图片有几张，H和W描述图像size，C是通道数（黑白图C=1，RBG图C=3）。以RGB为例，直观来说如下：<br><img src="https://img-blog.csdnimg.cn/20200308010449539.png" alt="在这里插入图片描述"><br>这里以灰度计算为例，说明各自的优劣。<br>对NCHW进行计算的时候，对将分成三个独立通道分别计算，，即全红一组，全绿一组这样。而NHWC得排列方式，是单个的三个相邻通道为一组计算。两者计算成本相同。我们可以知道，这样的话NHWC的局部访问存储性能更好（每三个输入像素即可得到一个输出像素）。NCHW 则必须等所有通道输入准备好才能得到最终输出结果，需要占用较大的临时空间。简单来说，就是想得到某个或某些独立像素像素的灰度计算结果，NCHW需要将全部图片计算出来，再取出特定的计算结果，而NHWC可以直接一个像素一个像素的得到结果。</p>
<p>在 CNN 中常常见到 1x1 卷积（例如：用于移动和嵌入式视觉应用的 MobileNets），也是每个输入 channel 乘一个权值，然后将所有 channel 结果累加得到一个输出 channel。如果使用 NHWC 数据格式，可以将卷积计算简化为矩阵乘计算，即 1x1 卷积核实现了每个输入像素组到每个输出像素组的线性变换。</p>
</li>
</ul>
<p>TensorFlow 为什么选择 NHWC 格式作为默认格式？因为早期开发都是基于 CPU，使用 NHWC 比 NCHW 稍快一些（不难理解，NHWC 局部性更好，cache 利用率高）。而NCHW 则是 Nvidia cuDNN 默认格式，使用 GPU 加速时用 NCHW 格式速度会更快（个别情况例外）。<br>所以设计网络的时候，需要根据具体的实践环境进行切换。</p>
<p><em>再回过头来看看conv2d的使用例子</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># 实践基于1.0</span></span><br><span class="line"> <span class="comment"># 2.0想实现请使用这段替换第一行：</span></span><br><span class="line"> <span class="comment"># import tensorflow.compat.v1 as tf</span></span><br><span class="line"> <span class="comment"># tf.disable_eager_execution()</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">input_data = tf.Varible(np.random.rand(<span class="number">10</span>,<span class="number">9</span>,<span class="number">9</span>,<span class="number">4</span>),dtype=np.float32)</span><br><span class="line">filter_data = tf.Varible(np.random.rand(<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">2</span>),dtype=np.float32)</span><br><span class="line">y = tf.nn.conv2d(input_data,filter_data,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">'VALID'</span>)</span><br><span class="line"></span><br><span class="line">print(input_data)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Tensor(“Variable/read:0”,shape=(10,9,9,4),dtype=float32)<br>Tensor(“Conv2D:0”,shape=(10,7,7,2),dtype=float32)</p>
</blockquote>
<p>导入所需要的库，然后我们定义需要做卷积的输入以及卷积核，这里的步长为1，padding为”VALID”。<br>我们可以看到，原本输入的shape是（10，9，9，4），由于padding为”VALID”,不对图像的边缘进行填充，所以在进行卷积之后，图像的尺寸发生了改变。<br>如果将padding改为”SAME”，图像的尺寸不变。</p>
<hr>
<p>2.0中keras的封装十分完善，对于使用来说比较友好，但是我们也更应该关注被封装一场隐藏起来的环节究竟有哪些细节在发生，在学习1.0的过程中我们会有不少收获。顺便<a href="https://minghuiwu.gitbook.io/tfbook/" target="_blank" rel="noopener">这里有一本1.0的开源书籍，对初学者十分友好，大家可以去看看</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/29/keras-layers-核心网络层摘要/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/02/29/keras-layers-核心网络层摘要/" class="post-title-link" itemprop="url">keras.layers--核心网络层摘要</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-02-29 23:20:58 / 修改时间：23:21:35" itemprop="dateCreated datePublished" datetime="2020-02-29T23:20:58+08:00">2020-02-29</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>完成一定实践后仔细阅读keras文档，做了关于核心网络层的一些摘要，主要汇总一些常用的网络层及其使用指南，大都在实践中使用过。</p>
<hr>
<p><strong>Dense</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Dense(units, activation=<span class="keyword">None</span>, use_bias=<span class="keyword">True</span>,</span><br><span class="line">                 kernel_initializer=<span class="string">'glorot_uniform'</span>, </span><br><span class="line">                 bias_initializer=<span class="string">'zeros'</span>, kernel_regularizer=<span class="keyword">None</span>, </span><br><span class="line">                 bias_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>, </span><br><span class="line">                 kernel_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>简要</li>
</ul>
<blockquote>
<p>全连接层。</p>
<p><em>output = activation(dot(input, kernel) + bias)</em>  其中 <em>activation</em> 是按逐个元素计算的激活函数，<em>kernel</em> 是由网络层创建的权值矩阵，以及 <em>bias</em> 是其创建的偏置向量 (只在 <em>use_bias</em><br>为 <em>True</em> 时才有用)。</p>
<p>如果该层的输入的秩大于2，那么它首先被展平然后 再计算与 kernel 的点乘。</p>
</blockquote>
<ul>
<li>重要参数</li>
</ul>
<blockquote>
<p>units: 正整数，输出空间维度。<br>activation: 激活函数。若不指定，则不使用激活函数 (即「线性」激活: a(x)=x。<br>use_bias: 布尔值，该层是否使用偏置向量。</p>
</blockquote>
<ul>
<li>输入输出</li>
</ul>
<blockquote>
<p>输入尺寸：<br>nD 张量，尺寸: (batch_size, …, input_dim)。 最常见的情况是一个尺寸为 (batch_size, input_dim) 的 2D 输入。<br>输出尺寸：<br>nD 张量，尺寸: (batch_size, …, units)。 例如，对于尺寸为 (batch_size, input_dim) 的 2D 输入， 输出的尺寸为 (batch_size, units)。</p>
</blockquote>
<hr>
<p><strong>Activation</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Activation(activation)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>摘要</p>
<blockquote>
<p>激活函数可单独作为一层，也可以嵌入在Dense层中，发挥作用。</p>
</blockquote>
</li>
<li><p>输入输出</p>
<blockquote>
<p>输入尺寸：<br>任意尺寸。 当使用此层作为模型中的第一层时， 使用参数 input_shape （整数元组，不包括样本数的轴）。<br>输出尺寸：<br>与输入相同。</p>
</blockquote>
</li>
</ul>
<hr>
<p><strong>Dropout</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Dropout(rate, noise_shape=<span class="keyword">None</span>, seed=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>摘要<blockquote>
<p>Dropout 包括在训练中每次更新时， 将输入单元的按比率随机设置为 0， 这有助于防止过拟合。随机舍去一定的神经元，来降低过拟合程度。</p>
</blockquote>
</li>
<li>重要参数<blockquote>
<p>rate: 在 0 和 1 之间浮动。需要丢弃的输入比例。<br>noise_shape: 1D 整数张量， 表示将与输入相乘的二进制 dropout 掩层的形状。 例如，如果你的输入尺寸为 (batch_size, timesteps, features)，然后 你希望 dropout 掩层在所有时间步都是一样的， 你可以使用 noise_shape=(batch_size, 1, features)。<br>seed: 一个作为随机种子的 Python 整数。</p>
</blockquote>
</li>
</ul>
<hr>
<p><strong>Flatten</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Flatten(data_format=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>摘要<blockquote>
<p>将输入展平。不影响批量大小。</p>
</blockquote>
</li>
<li>重要参数<blockquote>
<p>data_format：一个字符串，其值为 channels_last（默认值）或者 channels_first。它表明输入的维度的顺序。此参数的目的是当模型从一种数据格式切换到另一种数据格式时保留权重顺序。channels_last 对应着尺寸为 (batch, …, channels) 的输入，而 channels_first 对应着尺寸为 (batch, channels, …) 的输入。默认为 image_data_format 的值，你可以在 Keras 的配置文件 ~/.keras/keras.json 中找到它。如果你从未设置过它，那么它将是 channels_last</p>
</blockquote>
</li>
</ul>
<hr>
<p><strong>Lambda</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Lambda(function, output_shape=<span class="keyword">None</span>, mask=<span class="keyword">None</span>, arguments=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure></p>
<ul>
<li>摘要<blockquote>
<p>将任意表达式封装为 Layer 对象。</p>
</blockquote>
</li>
<li>重要参数<blockquote>
<p>function: 需要封装的函数。 将输入张量作为第一个参数。<br>output_shape: 预期的函数输出尺寸。 只在使用 Theano 时有意义。 可以是元组或者函数。 如果是元组，它只指定第一个维度； 样本维度假设与输入相同： output_shape = (input_shape[0], ) + output_shape 或者，输入是 None 且样本维度也是 None： output_shape = (None, ) + output_shape 如果是函数，它指定整个尺寸为输入尺寸的一个函数： output_shape = f(input_shape)<br>arguments: 可选的需要传递给函数的关键字参数。</p>
</blockquote>
</li>
<li>输入输出<blockquote>
<p>输入尺寸：<br>任意。当使用此层作为模型中的第一层时， 使用参数 input_shape （整数元组，不包括样本数的轴）。<br>输出尺寸：<br>由 output_shape 参数指定 (或者在使用 TensorFlow 时，自动推理得到)。</p>
</blockquote>
</li>
</ul>
<hr>
<p><strong>Conv2D</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Conv2D(filters, kernel_size, strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">                    padding=<span class="string">'valid'</span>, data_format=<span class="keyword">None</span>,</span><br><span class="line">                    dilation_rate=(<span class="number">1</span>, <span class="number">1</span>), activation=<span class="keyword">None</span>,</span><br><span class="line">                    use_bias=<span class="keyword">True</span>, kernel_initializer=<span class="string">'glorot_uniform'</span>,</span><br><span class="line">                    bias_initializer=<span class="string">'zeros'</span>, kernel_regularizer=<span class="keyword">None</span>,</span><br><span class="line">                    bias_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>,</span><br><span class="line">                    kernel_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>摘要<blockquote>
<p>2D 卷积层 (例如对图像的空间卷积)。<br>该层创建了一个卷积核， 该卷积核对层输入进行卷积， 以生成输出张量。 如果 use_bias 为 True， 则会创建一个偏置向量并将其添加到输出中。 最后，如果 activation 不是 None，它也会应用于输出。<br>当使用该层作为模型第一层时，需要提供 input_shape 参数 （整数元组，不包含样本表示的轴），例如， input_shape=(128, 128, 3) 表示 128x128 RGB 图像， 在 data_format=”channels_last” 时。</p>
</blockquote>
</li>
<li><p>重要参数</p>
<blockquote>
<p>filters: 整数，输出空间的维度 （即卷积中滤波器的输出数量）。<br>kernel_size: 一个整数，或者 2 个整数表示的元组或列表， 指明 2D 卷积窗口的宽度和高度。 可以是一个整数，为所有空间维度指定相同的值。<br>strides: 一个整数，或者 2 个整数表示的元组或列表， 指明卷积沿宽度和高度方向的步长。 可以是一个整数，为所有空间维度指定相同的值。 指定任何 stride 值 != 1 与指定 dilation_rate 值 != 1 两者不兼容。<br>padding: “valid” 或 “same” (大小写敏感)。</p>
</blockquote>
</li>
<li><p>输入输出</p>
<blockquote>
<p>输入尺寸：<br>如果 data_format=’channels_first’， 输入 4D 张量，尺寸为 (samples, channels, rows, cols)。<br>如果 data_format=’channels_last’， 输入 4D 张量，尺寸为 (samples, rows, cols, channels)。<br>输出尺寸：<br>如果 data_format=’channels_first’， 输出 4D 张量，尺寸为 (samples, filters, new_rows, new_cols)。<br>如果 data_format=’channels_last’， 输出 4D 张量，尺寸为 (samples, new_rows, new_cols, filters)。</p>
</blockquote>
</li>
</ul>
<hr>
<p><strong>MaxPooling2D</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">                          strides=<span class="keyword">None</span>, padding=<span class="string">'valid'</span>,</span><br><span class="line">                          data_format=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>摘要<blockquote>
<p>对于空间数据的最大池化。</p>
</blockquote>
</li>
<li>重要参数<blockquote>
<p>pool_size: 整数，或者 2 个整数表示的元组， 沿（垂直，水平）方向缩小比例的因数。 （2，2）会把输入张量的两个维度都缩小一半。 如果只使用一个整数，那么两个维度都会使用同样的窗口长度。</p>
</blockquote>
</li>
</ul>
<p><strong>*AveragePooling2D</strong>与之相似不过完成的是平均池化操作。*</p>
<hr>
<p><strong>SimpleRNN</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.SimpleRNN(units, activation=<span class="string">'tanh'</span>, use_bias=<span class="keyword">True</span>,</span><br><span class="line">                       kernel_initializer=<span class="string">'glorot_uniform'</span>,</span><br><span class="line">                       recurrent_initializer=<span class="string">'orthogonal'</span>, </span><br><span class="line">                       bias_initializer=<span class="string">'zeros'</span>, kernel_regularizer=<span class="keyword">None</span>, </span><br><span class="line">                       recurrent_regularizer=<span class="keyword">None</span>, bias_regularizer=<span class="keyword">None</span>,</span><br><span class="line">                       activity_regularizer=<span class="keyword">None</span>, kernel_constraint=<span class="keyword">None</span>, </span><br><span class="line">                       recurrent_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>, </span><br><span class="line">                       dropout=<span class="number">0.0</span>, recurrent_dropout=<span class="number">0.0</span>, </span><br><span class="line">                       return_sequences=<span class="keyword">False</span>, return_state=<span class="keyword">False</span>, </span><br><span class="line">                       go_backwards=<span class="keyword">False</span>, stateful=<span class="keyword">False</span>, unroll=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>摘要</li>
</ul>
<blockquote>
<p>全连接的 RNN，其输出将被反馈到输入。</p>
<ul>
<li>重要参数<br>input_dim: 输入的维度（整数）。 将此层用作模型中的第一层时，此参数（或者，关键字参数 input_shape）是必需的。</li>
<li>输入输出<br>输入尺寸：<br>3D 张量，尺寸为 (batch_size, timesteps, input_dim)。<br>输出尺寸：</li>
<li>如果 return_state：返回张量列表。 第一个张量为输出。剩余的张量为最后的状态， 每个张量的尺寸为 (batch_size, units)。</li>
<li>如果 return_sequences：返回 3D 张量， 尺寸为 (batch_size, timesteps, units)。</li>
<li>否则，返回尺寸为 (batch_size, units) 的 2D 张量。</li>
</ul>
<hr>
<p><strong>LSTM</strong></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.LSTM(units, activation=<span class="string">'tanh'</span>, </span><br><span class="line">                  recurrent_activation=<span class="string">'hard_sigmoid'</span>, use_bias=<span class="keyword">True</span>, </span><br><span class="line">                  kernel_initializer=<span class="string">'glorot_uniform'</span>, </span><br><span class="line">                  recurrent_initializer=<span class="string">'orthogonal'</span>, </span><br><span class="line">                  bias_initializer=<span class="string">'zeros'</span>, unit_forget_bias=<span class="keyword">True</span>, </span><br><span class="line">                  kernel_regularizer=<span class="keyword">None</span>, recurrent_regularizer=<span class="keyword">None</span>, </span><br><span class="line">                  bias_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>, </span><br><span class="line">                  kernel_constraint=<span class="keyword">None</span>, recurrent_constraint=<span class="keyword">None</span>, </span><br><span class="line">                  bias_constraint=<span class="keyword">None</span>, dropout=<span class="number">0.0</span>, recurrent_dropout=<span class="number">0.0</span>, </span><br><span class="line">                  implementation=<span class="number">1</span>, return_sequences=<span class="keyword">False</span>, </span><br><span class="line">                  return_state=<span class="keyword">False</span>, go_backwards=<span class="keyword">False</span>, stateful=<span class="keyword">False</span>, unroll=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>摘要<blockquote>
<p>长短期记忆网络层</p>
</blockquote>
</li>
</ul>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/19/卷积知识小结/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/02/19/卷积知识小结/" class="post-title-link" itemprop="url">卷积知识小结</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-02-19 23:59:04 / 修改时间：23:59:26" itemprop="dateCreated datePublished" datetime="2020-02-19T23:59:04+08:00">2020-02-19</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>简单接触了一些卷积神经网络的知识，在此小结一下。由于它的一般应用于图像分类领域，所以在此不便详细图文并茂的解释，只是浅谈一些自己积累到的小知识点。关于卷积神经网络的入门级知识，我推荐<a href="https://zhuanlan.zhihu.com/p/27908027" target="_blank" rel="noopener">前往这里了解</a>。</p>
<hr>
<p>我们知道cnn其实就是在做一个特征提取器的工作。计算过程就是一个累乘累加的过程。将卷积核的</p>
<h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><ul>
<li>卷积神经网络：（卷积层+（可选）池化层）<em>N+全连接层</em>M</li>
<li>全卷积神经网络：（卷积层+（可选）池化层）<em>N+反卷积层</em>M</li>
</ul>
<p>由于卷积层和池化层一般情况下会使输出的尺寸不断变小提取出抽象的特征，由此可以用来处理分类问题。而全卷积神经网络将最后的全连接层全部换成反卷积层，使得我们可以得到和输入图片尺寸相同的输出，从而完成图片的物体分割工作。</p>
<h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>卷积解决了以下问题：</p>
<p>比如在图像分类问题中，网络由全连接层组成就会导致<strong>参数过多</strong>。会浪费我们的计算资源，并且能承载很大信息量的神经网络会轻易模拟出数据中的规律，而产生过拟合，降低模型的泛化能力。</p>
<h3 id="卷积核"><a href="#卷积核" class="headerlink" title="卷积核"></a>卷积核</h3><p>卷积如何解决问题涉及到卷积核的一部分内容，首先不得不提到我们必须了解的卷积神经网络的两个重要性质：</p>
<ul>
<li><p>[ ] 局部链接</p>
<p>输出单元单元通过卷积核与原图进行连接操作，而卷积核的大小一般不会太大所以减少了连接数目，一定程度上缓解了问题。</p>
</li>
<li>[ ] 参数共享</li>
</ul>
<p>卷积核一次只能和原图相同尺寸的区域产生来连接，而其余的地方采用滑动窗口的方式依次连接，也就是说卷积核前后是不变的，只是按规定的步长进行移动来生成最终的feature map。</p>
<p>经过以上两部分的操作可以大大降低参数过多的问题。</p>
<p>我们使用卷积其实是提取到特征，那这两种操作虽然解决了一些问题，但是否会对我们的特征提取产生影响呢？继续看图像处理的例子，比如人脸识别中脸颊部分的像素相近，嘴唇部分的像素相近，也就是说这样的图片是有信息冗余的，由此可见图像是有一定的区域性，所以在经过局部连接之后，仍然能够保留提取特征的能力。而特征应该还与其所在的位置无关，也就是说，假如人脸在图片的右下角或左上角，对于脸部的特征不论出现在图片的什么位置，都应该被识别出来，而参数共享就恰好对应了这一特点，使得无论这一特征出现在什么位置都会得到好的匹配结果。只能匹配到固定位置的特征毫无疑问就是一种过拟合的表现，而恰恰参数共享避免了这种情况。</p>
<h3 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h3><p>卷积后的数据量减小了，但仍然过于庞大，池化操作就是在减少数据量。池化分为最大值池化和平均池化。最常用的是最大值池化，我们主要介绍这种。最大池化保留了每一个小块内的最大值，所以它相当于保留了这一块最佳匹配结果。这也就意味着它不会具体关注窗口内到底是哪一个地方匹配了，而只关注是不是有某个地方匹配上了。这也就能够看出，CNN能够发现图像中是否具有某种特征，而不用在意到底在哪里具有这种特征（似乎和参数共享有类似的作用）。</p>
<h3 id="非线性激活"><a href="#非线性激活" class="headerlink" title="非线性激活"></a>非线性激活</h3><p>激活函数比如Relu，它的公式：$f(x)=max(0,x)$即，保留大于等于0的值，其余所有小于0的数值直接改写为0。卷积后的图中的值，越小则越不相关，我们进行特征提取时，为了使得数据更少，操作更方便，就直接舍弃掉那些不相关联的数据（直接取零）。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/12/HDF5-python-使用简介/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/02/12/HDF5-python-使用简介/" class="post-title-link" itemprop="url">HDF5--python 使用简介</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-02-12 23:52:23 / 修改时间：23:56:28" itemprop="dateCreated datePublished" datetime="2020-02-12T23:52:23+08:00">2020-02-12</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>引：在使用TensorFlow，学习回调函数时，使用了ModelCheckpoint()，产生了.h5的文件。为了了解回调函数产生的信息，需要了解HDF5的相关内容，以及在python中的使用与相关问题解决。</em><br>文中一些叙述为了方便初次接触者理解，表述并不严谨，仅供简单参考。</p>
<hr>
<h2 id="初次见面"><a href="#初次见面" class="headerlink" title="初次见面"></a>初次见面</h2><p>HDF5（Hierarchical Data Formal）是用于存储大规模数值数据的较为理想的存储格式，文件后缀名为h5，存储读取速度非常快，且可在文件内部按照明确的层次存储数据，同一个HDF5可以看做一个高度整合的文件夹，其内部可存放不同类型的数据。</p>
<p>在Python中操纵HDF5文件的方式主要有两种</p>
<ul>
<li>是利用<strong>pandas</strong>中内建的一系列HDF5文件操作相关的方法，来完成相关操作。</li>
<li>二是利用<strong>h5py</strong>模块来完成Python原生数据结构与HDF5格式的转化</li>
</ul>
<p>本篇主要介绍hdf5的基础内容和对应模块使用的快速入门。</p>
<h2 id="初遇时的差池"><a href="#初遇时的差池" class="headerlink" title="初遇时的差池"></a>初遇时的差池</h2><p><del>一段小插曲</del><br>HDF是HDF(Hierarchical Data File)是美国国家高级计算应用中心(National Center for Supercomputing Application,NCSA)为了满足各种领域研究需求而研制的一种能高效存储和分发科学数据的新型数据格式 。阅读的文档中提到了到国家卫星气象中心（NSMC）曾经发布过一份《HDF5.0 使用简介》,抱着些许迷信权威的心态阅读后发现它涉及的信息就我目前来说价值不大，其中主要有讲HDF5文件的组织，API，创建，数据集数据空间，组群，属性等等，内容大而全，但似乎这篇教材发布的相对较早，所以产生了一定的局限性并且内部的相关URL都失效了，它本身也在国家卫星气象中心的官网上没有什么存在的痕迹。相关的API只涉及了C和FORTRAN的外壳包装函数。对于像我这样在使用python且第一次接触HDF5的使用者并不友好，索性只读了开头的基础内容并建立了更详细的认知后就放弃了继续阅读的打算。</p>
<h2 id="相识"><a href="#相识" class="headerlink" title="相识"></a>相识</h2><p>首先从hdf5文件讲起。</p>
<p>HDF5文件具有两类存储对象，dataset和group。dataset是类似于数组的数据集，而group是类似文件夹一样的容器，存放dataset和其他group。</p>
<p>HDF本意即是<strong>层次数据格式</strong>，所以就其存储结构来说是类似与POSIX风格的。其实现的方式就是group。每层都用’/‘分隔。我们创建的file object其实也可以看作一个group，是一个root group，其余的groups可以称为subgroups。</p>
<p>dataset与numpy中的array相似，比如都具有shape、dtype、以及一些切片操作等。虽然与Numpy的数组在接口上很相近，但是支持更多对外透明的存储特征，如数据压缩，误差检测，分块传输。</p>
<p>HDF5的一个很好的features就是可以在数据旁边存储元数据<sup><a href="#fn_1" id="reffn_1">1</a></sup>。所有的group和dataset都支持叫做<strong>属性</strong>的数据形式。</p>
<h4 id="h5py"><a href="#h5py" class="headerlink" title="h5py"></a>h5py</h4><p>想到python一定有对应的文件解析库，于是我开始了h5py的“快速”入门。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">import h5py</span><br><span class="line"># 读取</span><br><span class="line">file = h5py.File(&apos;test.hdf5&apos;, &apos;r&apos;)# .hdf5与.h5意义相同</span><br><span class="line"># 一下也可以完成读取</span><br><span class="line">with h5py.File(&quot;mytestfile.hdf5&quot;, &quot;w&quot;) as f:</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"># 我们需要注意的是group（包括File对象）与python中的字典相似</span><br><span class="line"># 通过该方法可以获得对应group下的subgroups或datasets，返回包含字符串的列表</span><br><span class="line">f.keys()</span><br><span class="line"></span><br><span class="line"># 此时我们假设存在一个名为DataSet的dataset对象</span><br><span class="line"># 利用对应键来索引值的方法可以获取该对象</span><br><span class="line">dest = f[&apos;DataSet&apos;]</span><br><span class="line"></span><br><span class="line"># dataset对象满足我们平时使用的numpy数组的一些操作，如下：</span><br><span class="line">dest.shape</span><br><span class="line">dest.dtype</span><br><span class="line">dest[:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 创建HDF5文件</span><br><span class="line">f = h5py.File(&apos;test.h5&apos;, &apos;w&apos;)</span><br><span class="line"></span><br><span class="line"># 使用create_dataset创建给定形状和数据类型的空dataset</span><br><span class="line">dataset = f.create_dataset(&apos;DS&apos;,(100,), dtype=&apos;i&apos;)</span><br><span class="line"># 也可以使用numpy中数组来初始化</span><br><span class="line">array = np.arange(100)</span><br><span class="line">dataset = f.create_dataset(&apos;init&apos;, data=array)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 分块存储</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">在缺省设置下，HDF5数据集在内存中是连续布局的，也就是按照传统的C序。</span><br><span class="line">Dataset也可以在HDF5的分块存储布局下创建。</span><br><span class="line">也就是dataset被分为大小相同的若干块随意地分布在磁盘上，并使用B树建立索引。</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"># 为了进行分块存储，将关键字设为一个元组来指示块的形状。</span><br><span class="line">dset = f.create_dataset(&quot;chunked&quot;, (1000, 1000), chunks=(100, 100))</span><br><span class="line"># 也可以自动分块，不必指定块的形状。</span><br><span class="line">dset = f.create_dataset(&quot;autochunk&quot;, (1000, 1000), chunks=True)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 分层结构</span><br><span class="line"># 遍历subgroups</span><br><span class="line">for name in f:</span><br><span class="line">    print(name)</span><br><span class="line"># 递归遍历所有subgroups</span><br><span class="line">def print_name(name):</span><br><span class="line">    print(name)</span><br><span class="line"></span><br><span class="line">f.visit(print_name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 属性通过attrs成员访问，类似于python中词典格式。</span><br><span class="line"> dataset.attrs[&apos;bias&apos;] = 60</span><br><span class="line"> &apos;bias&apos; in dataset.attrs</span><br></pre></td></tr></table></figure>
<p>一些其他的特性</p>
<ol>
<li>滤波器组<br>HDF5的滤波器组能够对分块数组进行变换。最常用的变换是高保真压缩。使用一个特定的压缩滤波器创建dataset之后，读写都可以向平常一样，不必添加额外的步骤。<br>用关键词compression来指定压缩滤波器，而滤波器的可选参数使用关键词compression_opt来指定：</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dset = f.create_dataset(&quot;zipped&quot;, (100, 100), compression=&quot;gzip&quot;)</span><br></pre></td></tr></table></figure>
<ol>
<li>HDF5文件的限制</li>
</ol>
<ul>
<li>HDF5文件本身大小没有限制，但是HDF5的一个dataset最高允许32个维，每个维度最多可有2^64个值，每个值大小理论上可以任意大</li>
<li>目前一个chunk允许的最大容量为2^32-1 byte (4GB). 大小固定的dataset的块的大小不能超过dataset的大小。</li>
</ul>
<p><a href="http://docs.h5py.org/en/latest/index.html" title="HDF5 for Python -- h5py 2.10.0 doc" target="_blank" rel="noopener">更多信息</a></p>
<h4 id="pandas"><a href="#pandas" class="headerlink" title="pandas"></a>pandas</h4><p><strong> 写出 </strong></p>
<p>　　pandas中的HDFStore()用于生成管理HDF5文件IO操作的对象，其主要参数如下：</p>
<p>　　path：字符型输入，用于指定h5文件的名称（不在当前工作目录时需要带上完整路径信息）</p>
<p>　　mode：用于指定IO操作的模式，与Python内建的open()中的参数一致，默认为’a’，即当指定文件已存在时不影响原有数据写入，指定文件不存在时则新建文件；’r’，只读模式；’w’，创建新文件（会覆盖同名旧文件）；’r+’，与’a’作用相似，但要求文件必须已经存在；</p>
<p>　　complevel：int型，用于控制h5文件的压缩水平，取值范围在0-9之间，越大则文件的压缩程度越大，占用的空间越小，但相对应的在读取文件时需要付出更多解压缩的时间成本，默认为0，代表不压缩</p>
<p>　　下面我们创建一个HDF5 IO对象store：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">store = pd.HDFStore(<span class="string">'demo.h5'</span>)</span><br><span class="line"><span class="string">'''查看store类型'''</span></span><br><span class="line">print(store)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">pandas</span>.<span class="title">io</span>.<span class="title">pytables</span>.<span class="title">HDFStore</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">File</span> <span class="title">path</span>:</span> demo.h5</span><br></pre></td></tr></table></figure>
<p>可以看到store对象属于pandas的io类，通过上面的语句我们已经成功的初始化名为demo.h5的的文件，本地也相应的出现了对应文件。</p>
<p>接下来我们创建pandas中不同的两种对象，并将它们共同保存到store中，首先创建series对象：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建一个series对象</span></span><br><span class="line">s = pd.Series(np.random.randn(<span class="number">5</span>), index=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>, <span class="string">'e'</span>])</span><br><span class="line"><span class="comment">#创建一个dataframe对象</span></span><br><span class="line">df = pd.DataFrame(np.random.randn(<span class="number">8</span>, <span class="number">3</span>),</span><br><span class="line">                 columns=[<span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'C'</span>])</span><br></pre></td></tr></table></figure>
<p>第一种方式利用键值对将不同的数据存入store对象中，这里为了代码简洁使用了元组赋值法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">store[<span class="string">'s'</span>],store[<span class="string">'df'</span>] = s,df</span><br></pre></td></tr></table></figure>
<p>第二种方式利用store对象的put()方法，其主要参数如下：</p>
<p>　　key：指定h5文件中待写入数据的key</p>
<p>　　value：指定与key对应的待写入的数据</p>
<p>　　format：字符型输入，用于指定写出的模式，’fixed’对应的模式速度快，但是不支持追加也不支持检索；’table’对应的模式以表格的模式写出，速度稍慢，但是支持直接通过store对象进行追加和表格查询操作</p>
<p>使用put()方法将数据存入store对象中：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">store.put(key=<span class="string">'s'</span>,value=s);store.put(key=<span class="string">'df'</span>,value=df)</span><br></pre></td></tr></table></figure></p>
<p>既然是键值对的格式，那么可以查看store的items属性（注意这里store对象只有items和keys属性，没有values属性）：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">store.items</span><br></pre></td></tr></table></figure></p>
<p>调用store对象中的数据直接用对应的键名来索引即可：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">store[<span class="string">'df'</span>]</span><br></pre></td></tr></table></figure></p>
<p>删除store对象中指定数据的方法有两种，一是使用remove()方法，传入要删除数据对应的键：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">store.remove(<span class="string">'s'</span>)</span><br><span class="line">print(store.keys())</span><br></pre></td></tr></table></figure>
<p>　　二是使用Python中的关键词del来删除指定数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span> store[<span class="string">'s'</span>]</span><br><span class="line">print(store.keys())</span><br></pre></td></tr></table></figure>
<p>这时若想将当前的store对象持久化到本地，只需要利用close()方法关闭store对象即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">store.close()</span><br><span class="line"><span class="string">'''查看store连接状况，False则代表已关闭'''</span></span><br><span class="line">store.is_open</span><br><span class="line"><span class="comment"># 这时本地的h5文件也相应的存储进store对象关闭前包含的文件</span></span><br></pre></td></tr></table></figure>
<p>　除了通过定义一个确切的store对象的方式，还可以从pandas中的数据结构直接导出到本地h5文件中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建新的数据框</span></span><br><span class="line">df_ = pd.DataFrame(np.random.randn(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line"><span class="comment">#导出到已存在的h5文件中，这里需要指定key</span></span><br><span class="line">df_.to_hdf(path_or_buf=<span class="string">'demo.h5'</span>,key=<span class="string">'df_'</span>)</span><br><span class="line"><span class="comment">#创建于本地demo.h5进行IO连接的store对象</span></span><br><span class="line">store = pd.HDFStore(<span class="string">'demo.h5'</span>)</span><br><span class="line"><span class="comment">#查看指定h5对象中的所有键</span></span><br><span class="line">print(store.keys())</span><br></pre></td></tr></table></figure>
<p><strong>读入</strong><br>在pandas中读入HDF5文件的方式主要有两种，一是通过上一节中类似的方式创建与本地h5文件连接的IO对象，接着使用键索引或者store对象的get()方法传入要提取数据的key来读入指定数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">store = pd.HDFStore(<span class="string">'demo.h5'</span>)</span><br><span class="line"><span class="string">'''方式1'''</span></span><br><span class="line">df1 = store[<span class="string">'df'</span>]</span><br><span class="line"><span class="string">'''方式2'''</span></span><br><span class="line">df2 = store.get(<span class="string">'df'</span>)</span><br><span class="line">df1 == df2</span><br></pre></td></tr></table></figure>
<p>可以看出这两种方式都能顺利读取键对应的数据。</p>
<p>　　第二种读入h5格式文件中数据的方法是pandas中的read_hdf()，其主要参数如下：</p>
<p>　　path_or_buf：传入指定h5文件的名称</p>
<p>　　key：要提取数据的键</p>
<p>　　需要注意的是利用read_hdf()读取h5文件时对应文件不可以同时存在其他未关闭的IO对象，否则会报错，如下例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(store.is_open)</span><br><span class="line">df = pd.read_hdf(<span class="string">'demo.h5'</span>,key=<span class="string">'df'</span>)</span><br></pre></td></tr></table></figure>
<p>　把IO对象关闭后再次提取：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">store.close()</span><br><span class="line">print(store.is_open)</span><br><span class="line">df = pd.read_hdf(<span class="string">'demo.h5'</span>,key=<span class="string">'df'</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>
<hr>
<p>参考：<br><a href="https://blog.csdn.net/yudf2010/article/details/50353292" target="_blank" rel="noopener">https://blog.csdn.net/yudf2010/article/details/50353292</a><br><a href="https://segmentfault.com/a/119000001667088" target="_blank" rel="noopener">https://segmentfault.com/a/119000001667088</a><br><a href="https://www.cnblogs.com/feffery/p/11135082.html" target="_blank" rel="noopener">https://www.cnblogs.com/feffery/p/11135082.html</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/06/激活函数/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/02/06/激活函数/" class="post-title-link" itemprop="url">激活函数</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-02-06 02:32:07 / 修改时间：12:38:03" itemprop="dateCreated datePublished" datetime="2020-02-06T02:32:07+08:00">2020-02-06</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>借此机会把深度学习中常用的激活函数做一些总结。</p>
<h3 id="激活函数概念"><a href="#激活函数概念" class="headerlink" title="激活函数概念"></a>激活函数概念</h3><p>所谓激活函数（Activation Function），就是在人工神经网络的神经元上运行的函数，负责将神经元的输入映射到输出端。</p>
<h3 id="什么是激活函数"><a href="#什么是激活函数" class="headerlink" title="什么是激活函数"></a>什么是激活函数</h3><p>激活函数（Activation functions）对于人工神经网络模型去学习、理解非常复杂和非线性的函数来说具有十分重要的作用。它们将非线性特性引入到我们的网络中。神经元中，输入的 inputs 通过加权，求和后，还被作用了一个函数，这个函数就是激活函数。引入激活函数是为了增加神经网络模型的非线性。没有激活函数的每层都相当于矩阵相乘。就算你叠加了若干层之后，无非还是个矩阵相乘罢了。</p>
<h3 id="为什么使用激活函数"><a href="#为什么使用激活函数" class="headerlink" title="为什么使用激活函数"></a>为什么使用激活函数</h3><p>如果不用激活函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合，这种情况就是最原始的感知机（Perceptron）。<br>如果使用的话，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。</p>
<h3 id="个人认知与理解"><a href="#个人认知与理解" class="headerlink" title="个人认知与理解"></a>个人认知与理解</h3><p>我觉得机器学习的传统算法与深度学习算法的比较重要的区别是：</p>
<ol>
<li>从广义上来讲，机器学习的传统算法一般只能使用一种目标模型函数，比如逻辑回归使用logistic函数、只能解决单维度问题；而深度学习可以在不同神经层使用不同或者多种激活函数、因此拥有多种或者不同函数的特性，所以解决问题具有多维度、线性、非线性等处理能力</li>
<li>深度学习的激活函数使得深度学习算法既能解决简单的线性问题、也能处理复杂的非线性问题</li>
<li>数据中的特征往往具有不同的特性、特征与不同模型之间也有较大的辨识差异，机器学习的传统算法的单一模型可能只能对部分特征产生重要作用，而深度学习的多种激活函数则比较全面、多维度对特征进行学习</li>
</ol>
<h3 id="常用的激活函数"><a href="#常用的激活函数" class="headerlink" title="常用的激活函数"></a>常用的激活函数</h3><ol>
<li>sigmoid 函数</li>
<li>tanh 函数</li>
<li>relu 函数</li>
<li>leaky relu 函数</li>
<li>elu 函数</li>
<li>softmax 函数</li>
</ol>
<h3 id="饱和激活函数与非饱和激活函数"><a href="#饱和激活函数与非饱和激活函数" class="headerlink" title="饱和激活函数与非饱和激活函数"></a>饱和激活函数与非饱和激活函数</h3><ol>
<li>饱和函数是指当自变量 x 达到某个值(或者说趋于无穷小、无穷大)的时候，因变量 y 就不再发生变化，而是趋于某一个固定的值</li>
</ol>
<ul>
<li>sigmoid 函数就是一个饱和激活函数，当自变量 z 趋于无穷小时，因变量 y 趋于 0；当自变量 z 趋于无穷大时，因变量 y 趋于 1</li>
<li>tanh 函数就是一个饱和激活函数，当自变量 z 趋于无穷小时，因变量 y 趋于 -1；当自变量 z 趋于无穷大时，因变量 y 趋于 1</li>
</ul>
<ol>
<li>饱和函数是指当自变量 x 达到某个值(或者说趋于无穷小、无穷大)的时候，因变量 y 就依然发生变化，并不是趋于某一个固定的值</li>
</ol>
<ul>
<li>relu 函数就是一个非饱和激活函数，当自变量 z 小于 0 时，因变量 y 等于 0；但当自变量 z 大于 0 时，因变量 y 是一个 z 的变化值</li>
<li>relu 的变种激活函数</li>
</ul>
<ol>
<li>非饱和激活函数的优势</li>
</ol>
<ul>
<li>首先，“非饱和激活函数”能解决所谓的“梯度消失”问题</li>
<li>其次，它能加快收敛速度<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">-10</span>, <span class="number">10</span>, <span class="number">100</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="1-sigmoid激活函数-饱和函数"><a href="#1-sigmoid激活函数-饱和函数" class="headerlink" title="1.sigmoid激活函数(饱和函数)"></a>1.sigmoid激活函数(饱和函数)</h3><p>sigmoid函数计算公式<br>$<br>\sigma(z) =\frac{1}{1+e^{-z}},e\approx 2.7183<br>$<br>sigmoid函数的导数公式<br>$<br>{\sigma(z)}’ =\sigma(z)(1-\sigma(z)),e\approx 2.7183<br>$<br>sigmoid函数评价</p>
<ul>
<li>优点：sigmoid函数有效地将实数域的线性问题映射到[0,1]区间的类别概率问题，实现分类；</li>
<li>缺点：然而在深度学习算法中使用sigmoid函数有时候在反向求导传播时会导致梯度消失的现象：<br>当z很大时，$\sigma(z)$趋近于1，当z很小时，$\sigma(z)$趋近于0<br>其导数${\sigma(z)}’ =\sigma(z)(1-\sigma(z))$在z很大或很小时都会趋近于0，造成梯度消失的现象<h1 id="手写计算函数"><a href="#手写计算函数" class="headerlink" title="手写计算函数"></a>手写计算函数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z, mode=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> mode:  <span class="comment"># 手写公式</span></span><br><span class="line">        L = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(z)):</span><br><span class="line">            sigmoid_value = <span class="number">1</span>/(<span class="number">1</span>+(<span class="number">2.7183</span>)**(-z[i]))</span><br><span class="line">            L.append(sigmoid_value)</span><br><span class="line">        <span class="keyword">return</span> L</span><br><span class="line">    <span class="keyword">else</span>:  <span class="comment"># 使用numpy</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br><span class="line">plt.plot(x, sigmoid(z=x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用 Tensorflow 的 sigmoid 函数</span></span><br><span class="line">y_sigmoid = tf.nn.sigmoid(x)</span><br><span class="line">plt.plot(x, y_sigmoid)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-tanh激活函数-饱和函数"><a href="#2-tanh激活函数-饱和函数" class="headerlink" title="2.tanh激活函数(饱和函数)"></a>2.tanh激活函数(饱和函数)</h3><p>tanh函数计算公式<br>$<br>tanh(z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}},e\approx 2.7183<br>$<br>tanh函数的导数公式<br>$<br>{tanh(z)}’=1-(tanh(z))^{2},e\approx 2.7183<br>$<br>tanh函数评价</p>
<ul>
<li>优点：sigmoid函数有效地将实数域的线性问题映射到[-1,1]区间的类别概率问题，实现分类；</li>
<li>缺点：然而在深度学习算法中使用tanh函数有时候在反向求导传播时会导致梯度消失的现象：<br>当z很大时，tanh(z)\tanh(z)tanh(z)趋近于1，当z很小时，tanh(z)tanh(z)tanh(z)趋近于-1<br>其导数 ${tanh(z)}’=1-(tanh(z))^{2}$在z很大或很小时都会趋近于0，造成梯度消失的现象<br>tanh与sigmoid的关系式：<br>$tanh(z)=2sigmoid(2z)-1$<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tanh</span><span class="params">(z, mode=True)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> mode:  <span class="comment"># 手写公式</span></span><br><span class="line">        L = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(z)):</span><br><span class="line">            exp1 = <span class="number">2.7183</span>**(z[i])</span><br><span class="line">            exp2 = <span class="number">2.7183</span>**(-z[i])</span><br><span class="line">            tanh_value = (exp1-exp2)/(exp1+exp2)</span><br><span class="line">            L.append(tanh_value)</span><br><span class="line">        <span class="keyword">return</span> L</span><br><span class="line">    <span class="keyword">else</span>:  <span class="comment"># 使用numpy</span></span><br><span class="line">        <span class="keyword">return</span> (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))</span><br><span class="line">plt.plot(x,tanh(z=x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用 Tensorflow 的 tanh 函数</span></span><br><span class="line">y_tanh = tf.nn.tanh(x)</span><br><span class="line">plt.plot(x, y_tanh)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="3-relu激活函数-非饱和函数"><a href="#3-relu激活函数-非饱和函数" class="headerlink" title="3.relu激活函数(非饱和函数)"></a>3.relu激活函数(非饱和函数)</h3><p>修正线性单元（Rectified linear unit，ReLU）</p>
<p>relu函数计算公式<br>$relu(z)=max(0,z)$<br>relu函数的导数公式<br>$<br>{relu(z)}’=\left\{\begin{matrix}1 &amp; z&gt; 0\ 0 &amp; z\leq 0\end{matrix}\right.<br>$<br>relu函数评价</p>
<ul>
<li>优点：<br>relu函数的非饱和性可以有效地解决梯度消失的问题，提供相对较宽的激活边界<br>relu函数是阈值函数，运算简单且快速，只需通过阈值判断就可以得到激活值，而sigmoid、tanh函数都需要计算指数，运算复杂<br>ReLU的单侧抑制(负梯度都为 0)提供了网络的稀疏表达能力</li>
<li>缺点：<br>relu(z)=max(0,z)relu(z)=max(0,z)relu(z)=max(0,z)会导致负梯度的神经元产生不可逆的死亡，也称为死亡神经元，由于负梯度值都为 0，在往后的运算中都将以 0 传播<br>如果学习率（Learning Rate）设置较大，会导致超过一定比例的神经元不可逆死亡，进而参数梯度无法更新，整个训练过程失败<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(x)</span>:</span>  <span class="comment"># 手写公式</span></span><br><span class="line">    relu_list = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x)):</span><br><span class="line">        relu_list.append(max(<span class="number">0</span>, x[i]))</span><br><span class="line">    <span class="keyword">return</span> relu_list</span><br><span class="line">plt.ylim(<span class="number">-1</span>,<span class="number">6</span>)</span><br><span class="line">plt.plot(x, relu(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用 Tensorflow 的 relu 函数</span></span><br><span class="line">y_relu = tf.nn.relu(x)</span><br><span class="line">plt.ylim(<span class="number">-1</span>,<span class="number">6</span>)</span><br><span class="line">plt.plot(x, y_relu)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="4-leaky-relu"><a href="#4-leaky-relu" class="headerlink" title="4.leaky relu"></a>4.leaky relu</h3><p>带泄露修正线性单元（Leaky ReLU）函数是经典（以及广泛使用的）的ReLu激活函数的变体，该函数输出对负值输入有很小的坡度，它旨在解决负梯度神经元死亡的问题。由于负梯度神经元导数总是不为零，这能减少静默神经元的出现，允许基于梯度的学习（虽然会很慢），解决了Relu函数进入负区间后神经元不学习的问题。</p>
<p>leaky relu 函数计算公式<br>$Lrelu(z)=max(0.1z,z),a&lt;1(a=0.1)$,其中a可以自定义</p>
<p>leaky relu 函数的导数公式<br>$<br>{Lrelu(z)}’=\left\{\begin{matrix}1 &amp; z&gt;0\ 0.1 &amp;z\leq 0 \end{matrix}\right.<br>$<br>leaky relu 函数评价</p>
<ul>
<li>优点：<br>旨在解决 relu 负梯度神经元死亡的问题，函数输出对负值输入有很小的坡度<br>解决了Relu函数进入负区间后神经元不学习的问题</li>
<li>缺点：<br>负梯度的 a 难以寻求合适的系数值，a 通常小于 1 且通常使用 a = 0.1<br>负梯度的 a 的选择和确定需要一定的经验或者做实验验证，相对麻烦一些<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">leaky_rule</span><span class="params">(z)</span>:</span>  <span class="comment"># 手写公式</span></span><br><span class="line">    L = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(z)):</span><br><span class="line">        lrule_value = max(<span class="number">0.1</span>*z[i], z[i])</span><br><span class="line">        L.append(lrule_value)</span><br><span class="line">    <span class="keyword">return</span> L</span><br><span class="line">plt.plot(x, leaky_rule(z=x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用 Tensorflow 的 leaky_relu 函数</span></span><br><span class="line">y_lrelu = tf.nn.leaky_relu(x)</span><br><span class="line">plt.plot(x, y_lrelu)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="5-elu"><a href="#5-elu" class="headerlink" title="5.elu"></a>5.elu</h3><p>指数线性单元（Exponential Linear Units）</p>
<p>elu 函数计算公式<br>$elu(z)={za(ez−1)z≥0z&lt;0$,tensorflow中,a默认为1</p>
<p>elu 函数的导数公式<br>$<br>{elu(z)}’=\left\{\begin{matrix}1 &amp; z\leq 0\ ae^{z} &amp; z&lt;0\end{matrix}\right.<br>$<br>elu 函数评价</p>
<p>融合了sigmoid和ReLU，左侧具有软饱和性，右侧无饱和性。<br>右侧线性部分使得ELU能够缓解梯度消失，而左侧软饱能够让ELU对输入变化或噪声更鲁棒。<br>ELU的输出均值接近于零，所以收敛速度更快。<br>在 ImageNet上，不加 Batch Normalization 30 层以上的 ReLU 网络会无法收敛，PReLU网络在MSRA的Fan-in （caffe ）初始化下会发散，而 ELU 网络在Fan-in/Fan-out下都能收敛。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">elu</span><span class="params">(z, a=<span class="number">0.1</span>)</span>:</span>  <span class="comment"># 手写公式</span></span><br><span class="line">    L = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(z)):</span><br><span class="line">        <span class="keyword">if</span> z[i] &gt;= <span class="number">0</span>:</span><br><span class="line">            elu_value = z[i]</span><br><span class="line">            L.append(elu_value)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            elu_value = a*(<span class="number">2.7183</span>**(z[i]) - <span class="number">1</span>)</span><br><span class="line">            L.append(elu_value)</span><br><span class="line">    <span class="keyword">return</span> L</span><br><span class="line">plt.plot(x, elu(z=x, a=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用 Tensorflow 的 elu 函数</span></span><br><span class="line">y_elu = tf.nn.elu(x)</span><br><span class="line">plt.plot(x, y_elu)</span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/03/tensorflow基础/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/02/03/tensorflow基础/" class="post-title-link" itemprop="url">tensorflow基础</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-02-03 12:11:06" itemprop="dateCreated datePublished" datetime="2020-02-03T12:11:06+08:00">2020-02-03</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2020-02-06 12:11:56" itemprop="dateModified" datetime="2020-02-06T12:11:56+08:00">2020-02-06</time>
              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="第一部分：Tensorflow基础"><a href="#第一部分：Tensorflow基础" class="headerlink" title="第一部分：Tensorflow基础"></a>第一部分：Tensorflow基础</h2><h3 id="Tensors张量"><a href="#Tensors张量" class="headerlink" title="Tensors张量"></a>Tensors张量</h3><h4 id="常量constant"><a href="#常量constant" class="headerlink" title="常量constant:"></a>常量constant:</h4><p>‘’’python<br>x = tf.constant([[4,2],[9,5]])<br>print(x)</p>
<p>tf.Tensor(<br>[[4 2]<br> [9 5]], shape=(2, 2), dtype=int32)<br>‘’’</p>
<p>可以通过“.numpy()”来得到numpy array类型</p>
<p>‘’’python<br>x.numpy()</p>
<p>array([[4, 2],<br>       [9, 5]], dtype=int32)<br>‘’</p>
<p>像numpy一样，有shape和dtype属性</p>
<p>‘’’python<br>print(‘shape:’,x.shape)<br>print(‘dx.dtype)</p>
<p>(2, 2)</p>
<p><dtype: 'int32'=""><br>‘’’</dtype:></p>
<p>常用的产生常量的方法是tf.ones和tf.zeros就像numpy的np.ones``np.zeros</p>
<p>‘’’python<br>print(tf.ones(shape=(2,3)))<br>print(tf.zeros(shape=(3,2)))</p>
<p>tf.Tensor(<br>[[1. 1. 1.]<br> [1. 1. 1.]], shape=(2, 3), dtype=float32)<br>tf.Tensor(<br>[[0. 0.]<br> [0. 0.]<br> [0. 0.]], shape=(3, 2), dtype=float32)<br>‘’’</p>
<h3 id="随机数常量random-constant正态分布"><a href="#随机数常量random-constant正态分布" class="headerlink" title="随机数常量random constant正态分布"></a>随机数常量random constant正态分布</h3><p>‘’’python<br>tf.random.normal(shape=(2,2),mean=0,stddev=1.0)</p>
<tf.tensor: id="12," shape="(2," 2),="" dtype="float32," numpy="array([[-0.05229542," 0.64488363],="" [="" 0.37966082,="" 1.0098479="" ]],="">

<h1 id="整数均匀分布"><a href="#整数均匀分布" class="headerlink" title="整数均匀分布"></a>整数均匀分布</h1><p>tf.random.uniform(shape=(2,2),minval=0,maxval=10,dtype=tf.int32)</p>
<p><tf.tensor: id="16," shape="(2," 2),="" dtype="int32," numpy="array([[6," 3],="" [8,="" 7]],=""><br>‘’’</tf.tensor:></p>
<h3 id="Variables变量"><a href="#Variables变量" class="headerlink" title="Variables变量"></a>Variables变量</h3><p>变量是一种特别的张量，用来存储可变数值，需要用一些值来初始化<br>‘’’pyhon<br>initial_value = tf.random.normal(shape=(2,2))<br>a = tf.Variable(initial_value)<br>print(a)</p>
<p><tf.variable 'variable:0'="" shape="(2," 2)="" dtype="float32," numpy="array([[" 0.07630513,="" -0.39769924],="" [-0.9712114="" ,="" -0.62955064]],=""><br>‘’’</tf.variable></p>
<p>可以通过assign(value)来赋值“=”，或assign_add(value)“+=”，或assign_sub(value)“-=”<br>‘’’python<br>new_value = tf.random.normal(shape=(2, 2))<br>a.assign(new_value)<br>for i in range(2):<br>    for j in range(2):<br>        assert a[i, j] == new_value[i, j]</p>
<p>added_value = tf.random.normal(shape=(2,2))<br>a.assign_add(added_value)<br>for i in range(2):<br>    for j in range(2):<br>        assert a[i,j] == new_value[i,j]+added_value[i,j]<br>‘’’</p>
<h3 id="Tensorflow数学运算"><a href="#Tensorflow数学运算" class="headerlink" title="Tensorflow数学运算"></a>Tensorflow数学运算</h3><p>可以像numpy那样做作运算，Tensorflow的不同时这些运算可以放到GPU或TPU上执行<br>‘’’python<br>a = tf.random.normal(shape=(2,2))<br>b = tf.random.normal(shape=(2,2))<br>c = a+b<br>d = tf.square(c)<br>e = tf.exp(c)<br>print(a)<br>print(b)<br>print(c)<br>print(d)<br>print(e)</p>
<p>tf.Tensor(<br>[[ 1.6862711 -1.4246397]<br> [-1.0287055 -1.3188182]], shape=(2, 2), dtype=float32)<br>tf.Tensor(<br>[[ 1.4519434  0.7635907]<br> [ 1.1213834 -1.4559215]], shape=(2, 2), dtype=float32)<br>tf.Tensor(<br>[[ 3.1382146  -0.661049  ]<br> [ 0.09267795 -2.7747397 ]], shape=(2, 2), dtype=float32)<br>tf.Tensor(<br>[[9.8483906e+00 4.3698579e-01]<br> [8.5892025e-03 7.6991806e+00]], shape=(2, 2), dtype=float32)<br>tf.Tensor(<br>[[23.062654    0.51630944]<br> [ 1.0971084   0.0623657 ]], shape=(2, 2), dtype=float32)<br>‘’’</p>
<h2 id="GradientTape计算梯度"><a href="#GradientTape计算梯度" class="headerlink" title="GradientTape计算梯度"></a>GradientTape计算梯度</h2><p>和numpy的另一个不同是，可以自动跟踪任何变量的梯度。<br>打开一个GradientTape,然后通过tape.watch()来跟踪变量<br>‘’’python<br>a = tf.random.normal(shape=(2,2))<br>b = tf.random.normal(shape=(2,2))<br>with tf.GradientTape() as tape:<br>    tape.watch(a)#开始记录所有有关a参与过的运算<br>    c = tf.sqrt(tf.square(a)+tf.square(b)) #变量a做一些运算</p>
<pre><code>#计算c对于a的梯度
dc_da = tape.gradient(c,a)
print(dc_da)
</code></pre><p>tf.Tensor(<br>[[-0.53557533  0.87920487]<br> [ 0.24663754  0.4680054 ]], shape=(2, 2), dtype=float32)<br>‘’’</p>
<p>对于所有变量，默认状态下会跟踪计算并用来求梯度，所以不用使用tape.watch()<br>‘’’python<br>a = tf.Variable(a)<br>with tf.GradientTape() as tape:<br>    c = tf.sqrt(tf.square(a)+tf.square(b))<br>    dc_da = tape.gradient(c,a)<br>    print(dc_da)</p>
<p>tf.Tensor(<br>[[-0.53557533  0.87920487]<br> [ 0.24663754  0.4680054 ]], shape=(2, 2), dtype=float32)<br>‘’’<br>可以通过多开几个GradientTape来求高阶导数：<br>‘’’python<br>with tf.GradientTape() as outer_tape:<br>    with tf.GradientTape() as tape:<br>        c = tf.sqrt(tf.square(a)+tf.square(b))<br>        dc_da = tape.gradient(c,a)<br>    d2c_d2a = outer_tape.gradient(dc_da,a)<br>    print(d2c_d2a)</p>
<p>tf.Tensor(<br>[[0.54411626 0.33872807]<br> [1.5284648  0.5024241 ]], shape=(2, 2), dtype=float32)<br>‘’’</p>
</tf.tensor:>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/01/二分法/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/01/01/二分法/" class="post-title-link" itemprop="url">二分法</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-01-01 00:12:16 / 修改时间：00:16:14" itemprop="dateCreated datePublished" datetime="2020-01-01T00:12:16+08:00">2020-01-01</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>2019年终记事：一年的经历那么多，回忆起来却短的不可想象。2019年的最后一篇博客，回想第一篇到现在，虽然一路上走走停停，但学习的脚步还在前进。短短二十年放弃的东西比一声叹息多多了，回头看来人生得要有一件能坚持下去的东西……新年的钟声带不走2019的遗憾，希望它能带来2020的希望。</em></p>
<hr>
<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><blockquote>
<p>在计算机科学中，二分搜索，也称为半间隔搜索，对数搜索，是一种搜索算法，用于查找排序数组中目标值的位置。二分搜索将目标值与数组的中间元素进行比较。<br>如果它们不相等，则消除目标不能位于其中的那一半，并在剩余的一半上继续搜索，再次将中间元素与目标值进行比较，并重复此过程直到找到目标值。<br>如果搜索以其余一半为空结束，则目标不在数组中。<br>在最坏的情况下，二分搜索会以对数时间运行，进行$O(\log_{}n)$比较，其中${n}$是数组中元素的数量，${O}$是Big O表示法， 而${\log}$是对数。 除了小数组以外，二分搜索比线性搜索快。 但是，必须首先对数组进行排序才能应用二进制搜索。有专门为快速搜索而设计的专用数据结构，例如哈希表，可以比二进制搜索更有效地进行搜索。 但是，二分搜索可用于解决更广泛的问题，例如，即使数组中不存在目标，也要在数组中找到相对于目标而言第二小的元素。<br>二进制搜索树和B树数据结构基于二分搜索。<br>——wiki</p>
</blockquote>
<p>在leetcode刷题时我们总能遇到可以利用二分搜索解决的问题，但往往我们写得出来的二分搜索代码并不能work或者潜藏有bug。造成这种现象的原因是纷杂的情况导致算法的细节处理不同而容易忽视边界的细节问题。我们需要一个简洁明了的思路来将问题一般化。矛盾的特殊性应该包含在矛盾的一般性当中，用高度抽象化的过程对具体问题降维打击。</p>
<p>编写博客时翻阅了不少leetcode中的二分题解，在此借鉴并总结一下。在完成的过程中翻阅了《计算机程序设计艺术》的相关章节，受益颇多，一部分内容会穿插在文中讲述。</p>
<h2 id="入题"><a href="#入题" class="headerlink" title="入题"></a>入题</h2><p><strong>一些背景——</strong></p>
<p>《计算机程序设计艺术》的作者 Donald Knuth：</p>
<blockquote>
<p>Although the basic idea of binary search is comparatively straightforward, the details can be surprisingly tricky …</p>
</blockquote>
<p>译：“虽然二分搜索的基本思想很直白，但细节出奇的难以应对…”</p>
<p>他在《计算机程序设计艺术（第三卷）》中也指出了（大意）:也许是第一部出版的关于非数值程序设计方法的书（1946）中，首先提出了二分查找。再到后来的许多人对二分算法的改进直至60年代往后，所有工作才算是基本完成。</p>
<p><strong>一些待解决的问题——</strong></p>
<ol>
<li>中位数索引值的获取</li>
<li>循环执行条件的设置与返回值的选择</li>
<li>分支语句的选择</li>
</ol>
<p><strong>关于模板——</strong></p>
<p>这里借助Thomas H. Cormen在《算法基础—打开算法之门》中的一段伪代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">程序   BINARY-SEARCH(A,n,x)</span><br><span class="line"></span><br><span class="line">输入：</span><br><span class="line">·A：一个数组；</span><br><span class="line">·n：要查找的数组A中元素的个数；</span><br><span class="line">·x：要查找的值；</span><br><span class="line"></span><br><span class="line">输出：要么是满足A[i]=x的索引i，要么是一个特殊值 NOT-FOUND（可取相对数组A的任何无效索引值，例如0或任意负整数）。</span><br><span class="line"></span><br><span class="line">//左右索性记为p,r；中间位置记为q</span><br><span class="line"></span><br><span class="line">1. 将p赋值为1，将r赋值为n。</span><br><span class="line">2. 只要p≤r，执行如下操作：</span><br><span class="line">        A. 将q赋值为⌊(p+r)/2⌋。</span><br><span class="line">        B. 如果A[q]=x，那么返回q。</span><br><span class="line">        C. 否则（A[q]≠x），如果A[q]&gt;x，那么将r赋值为q-1。</span><br><span class="line">        D. 否则（A[q]&lt;x），那么将p赋值为q+1。</span><br><span class="line">3. 返回 NOT-FOUND。</span><br></pre></td></tr></table></figure></p>
<p>这段算法描述是我们常规的理解和处理方法，这次打算讨论的是在它的基础上进行一定的改变，并简单谈谈优劣。</p>
<p><strong>基本思想——</strong></p>
<p>二分的基本思想其实就是每次可以将当前的数中将近一般的不满足要求的数全部除掉，所以大O时间复杂度是$O(\log_{2}n)$,可以达到对数级的复杂度。</p>
<p>为了真正了解二分搜索算法中所发生的事情，最好把它想象成一棵二叉搜索树。</p>
<p>关于平均比较次数暂且不论。</p>
<h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><p><strong>1. 中位数索引值</strong></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> mid=(left+right)/<span class="number">2</span>;</span><br></pre></td></tr></table></figure>
<p>上面的代码是我们经常写的，但它确实存在有bug，在于当left与right都很大的时候，left+right很有可能超过int类型能表示的最大值（32位机下为$2^{31}-1$，即2147483647），此时会产生整形溢出产生负值，为了避免此问题的发生一般会写作下面的形式：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> mid=left+(right-left)/<span class="number">2</span>;</span><br></pre></td></tr></table></figure></p>
<p>可以有效避免绝大多数会产生溢出的情况。但它并不是最完美的写法。我们知道对于二进制计算机来说，除法的计算并不容易完成，二进制的位运算应该是最简单快捷的。当面对需要以二作为除数的时候，选择&gt;&gt;1，会带来更好的效果。但仅仅只是修改这一点，mid的计算仍旧存在冗余的部分。</p>
<p>我们从原码的角度思考，其实不难发现，因为第一位用来表示正负符号的缘故，当发4个字节下可以表示的最大正整数比int最大值两倍还多。当发生整形溢出时，实际存储的二进制只不过因为最高位从0变为1，导致整形正数被认为是整形负数，无法正确表示我们目标的数值，但其实内部存储的数据如果按无符号整形（即最高位具有二进制权值，而非表示符号）来输出其实是我们需要的值，此时再对这个数进行<font color="#FF0000">无符号的&gt;&gt;1</font>操作得到我们需要的除二结果。</p>
<p>关于无符号右移，在java中使用&gt;&gt;&gt;，所以java代码如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> mid = (right + left) &gt;&gt;&gt; <span class="number">1</span> ;</span><br></pre></td></tr></table></figure></p>
<p>而在c语言中，仅有的&gt;&gt;运算符却是在有符号位时产生的是有符号的右移，所以运算前需要将运算数值强制类型转换为无符号型，再右移。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> mid=((<span class="keyword">unsigned</span> <span class="keyword">int</span>)(left+right))&gt;&gt;<span class="number">1</span>;</span><br></pre></td></tr></table></figure></p>
<p>不进行强制类型转换的话，会把溢出产生的负数值除二。具体大家看下面的例子：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> a=<span class="number">2147483647</span>,b=<span class="number">2147483645</span>;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"%d"</span>,(a+b)&gt;&gt;<span class="number">1</span>);<span class="comment">//结果输出-2</span></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"%u"</span>,((<span class="keyword">unsigned</span> <span class="keyword">int</span>)(a+b))&gt;&gt;<span class="number">1</span>);<span class="comment">//结果输出2147483646</span></span><br></pre></td></tr></table></figure></p>
<p><strong>2. 算法设计</strong></p>
<p>这里将之前提出的关于“循环执行条件的设置与返回值的选择”以及“分支选择”问题归结在一起，我们其实是从算法设计的角度来考虑的。</p>
<p>首先如上面的伪代码，我们循环的条件设置的为left≤right，出循环时的情况是left已经越过right，表示全部的元素都被搜索完。我们的修改是将循环条件改为left &lt; right,此时出循环的情况为left与right相等，表示整个数组只有现在唯一一个元素没有被搜索，也就是下标为left（right）的元素。根据不同的情形，对最后一个元素进行判断。比如说，我们知道这个数组一定会有我们搜索的值时，此时不用判断就可以知道当其他元素都被排除时，最后这个就是我们要找的元素。如果我们不确定是否一定存在，那么在循环外部额外判断此时最后的元素是否满足条件即可。</p>
<p>问题的关键在于这样的设计意义是什么，因为当我们用之前的方法，我们在每一次都需要判断是否中值是我们需要的值，但是根据统计规律要查找的数据一般情况下出现在中间的情况并不多。而且出循环时表示没有找到，举个例子：某些情况下我们会想把没有找到的元素添加进我们所查找的表中时，我们想先返回插入的位置，此时我们需要考虑返回left或者right。而每次遇到不同的问题我们需要返回的值都需要根据不同的情况选择，这样的设计并不完美。我们对于任意情况下都可以统一处理是理想状态，这样这段代码的复用性也会变高。</p>
<p>关于死循环，死循环容易发生在只有 2 个元素时，我需要慎重选择中位数，一定要确保：</p>
<ol>
<li>如果分支的逻辑，在选择左边界的时候，不能排除中位数，那么中位数就选“右中位数”，只有这样区间才会收缩，否则进入死循环。</li>
<li>同理，如果分支的逻辑，在选择右边界的时候，不能排除中位数，那么中位数就选“左中位数”，只有这样区间才会收缩，否则进入死循环。</li>
</ol>
<p>在区间中的元素只剩下 2 个时候，例如：left = 3，right = 4。此时左中位数就是左边界，如果你的逻辑执行到 left = mid 这个分支，且你选择的中位数是左中位数，此时左边界就不会得到更新，区间就不会再收缩（理解这句话是关键），从而进入死循环；<br>为了避免出现死循环，你需要选择中位数是右中位数，当逻辑执行到 left = mid 这个分支的时候，因为你选择了右中位数，让逻辑可以转而执行到 right = mid - 1 让区间收缩，最终成为 1 个数，退出 while 循环。</p>
<h1 id="总结与延申"><a href="#总结与延申" class="headerlink" title="总结与延申"></a>总结与延申</h1><p>其实Donald Knuth在著作中也指出了</p>
<blockquote>
<p>H.Bottenbruch迈出了二分算法的第二步，介绍了有趣的变形，避免了在最后结束之前单作一次相等判断：在步骤B2中利用$i⬅⌈(l+u)/2⌉$代替$⌊(l+u)/2⌋$，每当 $K≥K_i$时置$l⬅i$；然后在每一步中$u-l$都减值。最后，当$l=u$时，我们有$K_l≤K≤K_{l+1}$，而且再做一次比较即可判断这个查找是否成功（假定开始时$K≥K_l$）。这个思想子啊许多计算机上稍微加速了内循环，而且同样的原理可以在这一节讨论的所有算法使用，但由于此前推导出的一次成功的平均查找次数于一次不成功的平均查找次数之间的关系一次成功的查找平均将要求大约再做一次迭代。由于内循环仅执行$lgN$次，因此在这一次额外的迭代与一次更快的循环之间的折衷并不节省时间，除非N非常大。在含有重复码值时，获取该算法给定码值最右出现，这一性质有时很重要。<br><em>大师的思想与见识比我们深远的多，多多读书无疑是一场对于世界和自我的再发掘。</em></p>
</blockquote>
<p>在上述描述中，我们不难看出来其实整个算法的核心就是在靠左右边界夹逼，而我们做出的改变其实再更深刻的讨论中，在不涉及大数量的情况下，对时间的影响并不明显。但是这种算法仍然有可用性，就如我在之前分析的一样。</p>
<p>最后的一点延申，我们的二分使用了left mid right三个指针，而仅使用如下两个量，记录当前位置i,和它的变化速度δ；每次不相等的比较之后，我们可以置i⬅1±δ和δ⬅δ/2（近似地）。这是可以的，但要对细节极端小心才行，简化的解决方法注定引起失误！这个算法实现后，产生的一颗“均匀的”二分查找树，我们可以观察它得到更好的算法。</p>
<p>而且另外一点我们在整个过程的观察中，其实可以发现斐波那契数列可以起到和2的乘方相似的作用。斐波那契搜索就是在二分查找的基础上根据斐波那契数列进行分割的。在斐波那契数列找一个等于略大于查找表中元素个数的数F[n]，将原查找表扩展为长度为F[n]<br>(如果要补充元素，则补充重复最后一个元素，直到满足F[n]个元素)，完成后进行斐波那契分割，即F[n]个元素分割为前半部分F[n-1]个元素，后半部分F[n-2]个元素，找出要查找的元素在那一部分并递归，直到找到。斐波那契查找，是区间中单峰函数的搜索技术。乍看之下，似乎有些神秘，就如Donald Knuth所说，如果我们简单地把程序拿出来并试图看看它在干什么活，它似乎在变魔术，但只要把查找树画出来，神秘感就消失了。<br>这次地介绍就到此结束了。</p>
<p><em><font color="#FF0000" size="0">关于平均比较数和均匀二分查找等详细内容，本文暂时未涉及，后续可能补充，有兴趣的读者可以自行了解。</font></em></p>
<hr>
<p align="right"><font face="Segoe Script">Attempt the end;and never stand to doubt;<br>
Nothing's so hard,but search will find it out.</font></p>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/06/02/Excel模块简析/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/02/Excel模块简析/" class="post-title-link" itemprop="url">Excel模块简析</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-02 17:30:47" itemprop="dateCreated datePublished" datetime="2019-06-02T17:30:47+08:00">2019-06-02</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-07 15:08:09" itemprop="dateModified" datetime="2019-08-07T15:08:09+08:00">2019-08-07</time>
              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<p>Python中处理Excel文件主要依靠xlrd xlwt和pandas库的模块进行数据处理。这里主要介绍xlrd和xlwt扩展包。</p>
<hr>
<p>xlrd 是一个库, 用于读取 Excel 文件中的数据和格式信息, 无论它们是. xls 还是. xlsx 文件。<br>对于包含文本的数据处理我们需要注意的是扩展包所支持的编码方案，此包将所有文本字符串显示为 Python unicode 对象。<br>我们通常会接触到的Exel文件中日期的存储方式与一般数据有格式上的区别。但是在Excel文件中，日期不存储为单独的数据类型;它们存储为浮点数。这个浮点数代表从1990年1月0日，或者说1899年12月31日开始经过的日期数加上一个24小时的小数部分。（注：在Mac上日期是从1994年1月1日开始）这对我们处理这些数据产生了预期之外的困难，原因在于我们不容易分辨一个浮点数是否表示日期数据。对于本模块，它通过检查已应用于每个数字单元格的格式来帮助我们，它可以检测到单元格数据的格式，从而判定单元格中的值是否是日期。但我们处理这些数据时，仍要格式化进行。往往我们的处理方式如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> worksheet.cell_type(row_index, col_index) == <span class="number">3</span>:</span><br><span class="line"></span><br><span class="line">	date_cell = xldate_as_tuple(worksheet.cell_value(row_index,col_index),workbook.datemode)</span><br><span class="line">		date_cell = date(*date_cell[<span class="number">0</span>:<span class="number">3</span>]).strftime(<span class="string">'%m/%d/%Y'</span>)</span><br></pre></td></tr></table></figure></p>
<p>简单解释一下，在xlrd的文档中我们可以知道，日期型数据的单元格类型为3。这里使用了worksheet对象的cell_value函数和行列索引来获得单元格中的值。这个值作为xldate_as_tuple()函数的第一个参数，参数workbook.datemode，可以使函数确定日期是基于1990年还是1994年，返回一个元组。strftime将data对象转化为格式化字符串。<br>基础部分我就直接列在下方：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xlrd.open_workbook(filename=<span class="keyword">None</span>, logfile=&lt;_io.TextIOWrapper name=<span class="string">'&lt;stdout&gt;'</span> mode=<span class="string">'w'</span> encoding=<span class="string">'UTF-8'</span>&gt;, verbosity=<span class="number">0</span>, use_mmap=<span class="number">1</span>, file_contents=<span class="keyword">None</span>, encoding_override=<span class="keyword">None</span>, formatting_info=<span class="keyword">False</span>, on_demand=<span class="keyword">False</span>, ragged_rows=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure></p>
<p><em>filename– 要打开的电子表格文件的路径。
</em>logfile – 一个打开的文件, 将消息和诊断写入其中。<br><em>verbosity – 增加写入日志文件的跟踪材料的数量。
</em>use_mmap – 是否使用 mmap 模块是试探性确定的。使用此参数覆盖结果。<br>目前的试探: mmap (如果存在) 将使用它。<br>file_contents – 一个字符串或者一个mmap.mmap对象或者其他类似行为的对象。 如果file_contents提供, filename将不会被使用, except (possibly)但 (可能) 在消息中除外。<br>encoding_override – 用于克服旧版本文件中丢失或错误的代码页信息。<br>formatting_info –默认值为False, 它节省内存。 在这种情况下, “空白” 单元格 (具有自己的格式信息但没有数据) 通过忽略文件的BLANK和MULBLANK记录被视为空。 这将切断空单元格或空白单元格行的任何底部或右侧 “边距”。<br>当为True,格式信息将从电子表格文件中读取。这将提供所有单元格, 包括空单元格和空白单元格。格式信息可用于每个单元格。<br>请注意, 当与 xlsx 文件一起使用时，这将引发注意实现错误,。<br>on_demand –控制工作表最初是否全部加载, 或者在调用方要求时加载。<br>ragged_rows –<br>默认值False表示所有行都用空单元格填充, 以便所有行的大小与 ncols中的大小相同。<br>True表示行的末尾没有空单元格。如果行的大小千差万别, 这可能会节省大量内存。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/26/sqlite3-SQLite数据库模块简单认识/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="X">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="X">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/05/26/sqlite3-SQLite数据库模块简单认识/" class="post-title-link" itemprop="url">sqlite3-SQLite数据库模块简单认识</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-26 19:35:07" itemprop="dateCreated datePublished" datetime="2019-05-26T19:35:07+08:00">2019-05-26</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-07 15:08:47" itemprop="dateModified" datetime="2019-08-07T15:08:47+08:00">2019-08-07</time>
              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong><em>SQLite 是一个C语言库，它可以提供一种轻量级的基于磁盘的数据库，这种数据库不需要独立的服务器进程，也允许需要使用一种非标准的 SQL 查询语言来访问它。一些应用程序可以使用 SQLite 作为内部数据存储。可以用它来创建一个应用程序原型，然后再迁移到更大的数据库，比如 PostgreSQL 或 Oracle。</em></strong><br>要使用这个模块，必须先创建一个 Connection 对象，它代表数据库。下面例子中，数据将存储在 example.db 文件中：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import sqlite3</span><br><span class="line">conn = sqlite3.connect(&apos;example.db&apos;)</span><br></pre></td></tr></table></figure></p>
<p>你也可以使用 :memory: 来创建一个内存中的数据库</p>
<p>当有了 Connection 对象后，你可以创建一个 Cursor 游标对象，然后调用它的 execute() 方法来执行 SQL 语句：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">c = conn.cursor()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建表</span></span><br><span class="line">c.execute(<span class="string">'''CREATE TABLE stocks</span></span><br><span class="line"><span class="string">             (date text, trans text, symbol text, qty real, price real)'''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 插入一行数据</span></span><br><span class="line">c.execute(<span class="string">"INSERT INTO stocks VALUES ('2006-01-05','BUY','RHAT',100,35.14)"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存（提交）更改</span></span><br><span class="line">conn.commit()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们也可以在完成任务后，关闭连接</span></span><br><span class="line"><span class="comment"># 只需要保证在关闭之前任何更改都已提交</span></span><br><span class="line">conn.close()</span><br></pre></td></tr></table></figure></p>
<p>这些数据被持久化保存了，而且可以在之后的会话中使用它们：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sqlite3</span><br><span class="line">conn = sqlite3.connect(<span class="string">'example.db'</span>)</span><br><span class="line">c = conn.cursor()</span><br></pre></td></tr></table></figure></p>
<p>通常你的 SQL 操作需要使用一些 Python 变量的值。你不应该使用 Python 的字符串操作来创建你的查询语句，因为那样做不安全；它会使你的程序容易受到 SQL 注入攻击（在 <a href="https://xkcd.com/327/" target="_blank" rel="noopener">https://xkcd.com/327/</a> 上有一个搞笑的例子，看看有什么后果）</p>
<p>推荐另外一种方法：使用 DB-API 的参数替换。在你的 SQL 语句中，使用 ? 占位符来代替值，然后把对应的值组成的元组做为 execute() 方法的第二个参数。（其他数据库可能会使用不同的占位符，比如 %s 或者 :1）例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># Never do this -- insecure!</span><br><span class="line">symbol = &apos;RHAT&apos;</span><br><span class="line">c.execute(&quot;SELECT * FROM stocks WHERE symbol = &apos;%s&apos;&quot; % symbol)</span><br><span class="line"></span><br><span class="line"># Do this instead</span><br><span class="line">t = (&apos;RHAT&apos;,)</span><br><span class="line">c.execute(&apos;SELECT * FROM stocks WHERE symbol=?&apos;, t)</span><br><span class="line">print(c.fetchone())</span><br><span class="line"></span><br><span class="line"># Larger example that inserts many records at a time</span><br><span class="line">purchases = [(&apos;2006-03-28&apos;, &apos;BUY&apos;, &apos;IBM&apos;, 1000, 45.00),</span><br><span class="line">             (&apos;2006-04-05&apos;, &apos;BUY&apos;, &apos;MSFT&apos;, 1000, 72.00),</span><br><span class="line">             (&apos;2006-04-06&apos;, &apos;SELL&apos;, &apos;IBM&apos;, 500, 53.00),</span><br><span class="line">            ]</span><br><span class="line">c.executemany(&apos;INSERT INTO stocks VALUES (?,?,?,?,?)&apos;, purchases)</span><br></pre></td></tr></table></figure></p>
<p>要在执行 SELECT 语句后获取数据，你可以把游标作为 iterator，然后调用它的 fetchone() 方法来获取一条匹配的行，也可以调用 fetchall() 来得到包含多个匹配行的列表。</p>
<p>下面是一个使用迭代器形式的例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; for row in c.execute(&apos;SELECT * FROM stocks ORDER BY price&apos;):</span><br><span class="line">        print(row)</span><br><span class="line"></span><br><span class="line">(&apos;2006-01-05&apos;, &apos;BUY&apos;, &apos;RHAT&apos;, 100, 35.14)</span><br><span class="line">(&apos;2006-03-28&apos;, &apos;BUY&apos;, &apos;IBM&apos;, 1000, 45.0)</span><br><span class="line">(&apos;2006-04-06&apos;, &apos;SELL&apos;, &apos;IBM&apos;, 500, 53.0)</span><br><span class="line">(&apos;2006-04-05&apos;, &apos;BUY&apos;, &apos;MSFT&apos;, 1000, 72.0)</span><br></pre></td></tr></table></figure></p>
<hr>
<p>摘自Python官方文档</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

      <div>
       
      </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="X">
            
              <p class="site-author-name" itemprop="name">X</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">15</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              

              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">X</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v6.6.0</div>



<div class="powered-by">
  <i class="fa fa-child" font="" style="font-size:26px;"></i>
  <span id="busuanzi_container_site_uv">
    本站访客数:<span id="busuanzi_value_site_uv"></span>
  </span>
</div>

        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=6.6.0"></script>

  <script src="/js/src/motion.js?v=6.6.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.6.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.6.0"></script>



  

  


  <script src="/js/src/bootstrap.js?v=6.6.0"></script>



  



  











  





  

  

  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {equationNumbers: { autoNumber: "AMS" }}
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<style>
.MathJax_Display {
  overflow: auto hidden;
}
</style>

    
  


  

  

  

  

  

  

  

  

</body>
</html>
